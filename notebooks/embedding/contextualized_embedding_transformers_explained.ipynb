{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualized embedding with transformer models illustrated\n",
    "\n",
    "In this notebook, we begin to peek under the hood of a BERT transformer model to understand how contextualized embedding work.\n",
    "We then also introduce a couple of potential use cases that leverage contextualized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/haukelicht/advanced_text_analysis/blob/main/notebooks/embedding/contextualized_embedding_transformers_explained.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    # shallow clone of current state of main branch \n",
    "    !git clone --branch main --single-branch --depth 1 --filter=blob:none https://github.com/haukelicht/advanced_text_analysis.git\n",
    "\n",
    "    !pip install -q transformers==4.44.1 matplotlib==3.9.2 umap-learn==0.5.6 bertviz==1.4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import (\n",
    "    BertForMaskedLM, \n",
    "    BertModel, BertTokenizer\n",
    ")\n",
    "\n",
    "from bertviz import head_view\n",
    "from bertviz.transformers_neuron_view import BertModel as BertVizModel \n",
    "from bertviz.transformers_neuron_view import BertTokenizer as BertVizTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to the `transformers` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, the standard library to work with transformer models is `transformers`.\n",
    "It provides access to pre-trained transformers models through its [model hub]().\n",
    "The `transformers` library is developed and maintained by Hugging Face Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-trained models and tokenizers\n",
    "\n",
    "To use a pre-trained model for embedding texts, we need two things:\n",
    "\n",
    "1. the model's tokenizer\n",
    "1. and of course the model itself\n",
    "\n",
    "We use the model to process a text though its **layers** to obtain the text's **embedding**.\n",
    "But to be able to do this, we need to **tokenize** the text to convert it into number â€“ because deep neural network can only process with numbers, not with raw text.\n",
    "\n",
    "Below we load a pre-trained BERT model, specifically \"bert-base-uncased\", which is a smallish version of BERT (hence 'base' instead of 'large') that does not distinguish between upper- and lowercase letters (hence 'uncased'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the model we want to load\n",
    "model_id = 'bert-base-uncased'\n",
    "\n",
    "# load the pre-trained model and tokenizer \n",
    "model = BertModel.from_pretrained(model_id)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "# NOTE: this will trigger downloading the model and tokenizer if you haven't done so before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some information about the model by looking at its configuration attribute (`config`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get some important information about the model\n",
    "print('embedding dimensionality:', model.config.hidden_size)\n",
    "print('number of layers:', model.config.num_hidden_layers)\n",
    "print('vocabulary size:', model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets' have a look at the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the models first component is a `BertEmbeddings` module that contains\n",
    "    1. the initial word embedding layer\n",
    "    2. the positional embedding\n",
    "- after this we have the `BertEncoder` module that consists of 12 `BertLayer`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to get the initial word embeddings, we can access them like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.embeddings.word_embeddings)\n",
    "\n",
    "# let's get the first five values of the first embedding\n",
    "model.embeddings.word_embeddings.weight[0][:5].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: \n",
    "\n",
    "- the layers are attributes of the `model` and they are organized and nested as can be seen when calling `print(model)` \n",
    "- we get the actual parameters of the model from a layer's \"weigths\" (weights is just the machine learning term for parameters)\n",
    "- weights are $n$-dimensional arrays (called \"tensors\" in `pytorch` etc.) and we can index them just like numpy arrays\n",
    "- we use `detach()` because the model and its weights (parameters) are tracked by the optimization algorithm, which we dont need when we only want to see the weight values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the main reason we use BERT & Co. is to obtain contextualized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualized embedding\n",
    "\n",
    "To illustrate how contextualized embedding works in transformers, we will first look at how embeddings of the same word differ if their context differs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take two sentences what contain the word \"bank\" but use it with different meanings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Today, I will hike along the bank of a river.\",\n",
    "    \"Today, I will open a new account at my bank and deposit some money.\",\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the transformer embedding of the word \"bank\" in these two sentences, we need to follow three steps:\n",
    "\n",
    "1. tokenizer thge texts and convert tokens into tokens IDs (to look-up their input embeddings)\n",
    "2. process these inputs through the model\n",
    "3. locate the embedding of the focal word in the two sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer converts the text into tokens and maps the tokens to token IDs\n",
    "\n",
    "Token IDs indicate tokens' locations in the tokenizers vocabulary and hence the model's input embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"decode\" these token IDs into their tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "- the `[CLS]` token is a special token used to summarize the information in a sequence (e.g., for classification tasks)\n",
    "- the `[SEP]` token is the special \"separator\" token that indicates sequence boundaries\n",
    "- the `[PAD]` token is the special \"padding\" token that is appended to sequences that are shorter than the other sequences in a batch to make the input rectengular (e.g., all rows have an equal number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the tokenizer to get the token ID of the focal word \n",
    "focal_word_id = tokenizer.convert_tokens_to_ids('bank')\n",
    "focal_word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create maks that is true where input ID == focal word ID\n",
    "mask = inputs['input_ids'] == focal_word_id\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) embed (process through model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intial emebdding of the focal word (\"bank\")\n",
    "model.embeddings.word_embeddings.weight[focal_word_id].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** We use `torch.no_grad()` to disable gradient tracking, which is used for \"back propagation\" â€“ the method used to optimize deep neural networks' parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(outputs))\n",
    "# list the object's attributes\n",
    "list(dict(outputs).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiden states are the embeddings after each layer\n",
    "len(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final embedding can be accessed like this: \n",
    "outputs.hidden_states[-1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let'S look at the shape:\n",
    "outputs.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) get the words' contextualized embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final transformer embeddings of bank in different contexts\n",
    "embeddings = outputs.last_hidden_state[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity between the two embeddings\n",
    "cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see that the similarity of \"bank\"'s transformer embedding deepends on the model layer we look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all layers\n",
    "for i, layer in enumerate(outputs.hidden_states):\n",
    "    # skip input embeddings\n",
    "    if i == 0:\n",
    "        continue\n",
    "    embeddings = layer[mask]\n",
    "    similarity = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))\n",
    "    print(f'layer {i}: {similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Competition ðŸ”¥\n",
    "\n",
    "**Try it your self!** \n",
    "\n",
    "- Define a pair of sentences that use a word with different meanings.\n",
    "- Whoever gets the **lowest similar score** (at the final layer of `bert-base-uncase`) for their example pair wins!\n",
    "\n",
    "*Bonus:* Can you think of a word from your research domain or area of interest that has multiple meanins. If so, does BERT seem to distinguish these meanings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention &mdash; peeking under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the amazing `bertviz` library to have a deeper look into the workings of transformers.\n",
    "Let's define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Today, I will hike along the bank of a river.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Exercise ðŸ”¥\n",
    "\n",
    "Use the interactive attention head and neuron views below to answer the following questions\n",
    "\n",
    "1. In what layers does BERT attend to the \"bank\"'s context token \"river\"? And which head focuses the most on this context word?\n",
    "1. What other context tokens of \"bank\" does BERT attend to across layers?\n",
    "1. How does that change across layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head View\n",
    "\n",
    "<b>The head view visualizes attention in one or more heads from a single Transformer layer.</b> Each line shows the attention from one token (left) to another (right). Line weight reflects the attention value (ranges from 0 to 1), while line color identifies the attention head. When multiple heads are selected (indicated by the colored tiles at the top), the corresponding  visualizations are overlaid onto one another.  For a more detailed explanation of attention in Transformer models, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = BertModel.from_pretrained(model_id, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve attention weights\n",
    "inputs = tokenizer.encode_plus(sentence, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention = model(input_ids)[-1]\n",
    "input_id_list = input_ids[0].tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Usage*\n",
    "\n",
    "- **Hover** over any **token** on the left/right side of the visualization to filter attention from/to that token. <br/>\n",
    "- **Double-click** on any of the **colored tiles** at the top to filter to the corresponding attention head.<br/>\n",
    "- **Single-click** on any of the **colored tiles** to toggle selection of the corresponding attention head. <br/>\n",
    "- **Click** on the **Layer** drop-down to change the model layer (zero-indexed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron View\n",
    "<b>The neuron view visualizes the intermediate representations (e.g. query and key vectors) that are used to compute attention.</b> In the collapsed view (initial state), the lines show the attention from each token (left) to every other token (right). In the expanded view, the tool traces the chain of computations that produce these attention weights. For a detailed explanation of the attention mechanism, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertviz_model = BertVizModel.from_pretrained(model_id, output_attentions=True)\n",
    "bertviz_tokenizer = BertVizTokenizer.from_pretrained(model_id)\n",
    "show(bertviz_model, 'bert', bertviz_tokenizer, sentence, layer=4, head=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Usage*\n",
    "\n",
    "- **Hover** over any of the tokens on the left side of the visualization to filter attention from that token.<br/>\n",
    "- Then **click** on the **plus** icon that is revealed when hovering. This exposes the query vectors, key vectors, and other intermediate representations used to compute the attention weights. Each color band represents a single neuron value, where color intensity indicates the magnitude and hue the sign (blue=positive, orange=negative).<br/>\n",
    "- Once in the expanded view, **hover** over any other **token** on the left to see the associated attention computations.<br/>\n",
    "- **Click** on the **Layer** or **Head** drop-downs to change the model layer or head (zero-indexed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Brainstorming session ðŸ”¥\n",
    "\n",
    "Assume you have a BERT model that has only been trained on a corpus of texts specific to your research domain.\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "- Can you think of any uses of the attention-level information we inspected above to understand language use and discourse in this text copus?\n",
    "- Do you think you could as well us the pretrained `bert-base-uncased` model loaded above? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example application: Using BERT for word sense disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How can we use transformers to categorize in what sense a word is used in its context?\n",
    "\n",
    "**_Idea:_**\n",
    "\n",
    "1. a words' context clarifies its meaning\n",
    "2. contextualized embeddings capture this by shifting embeddings to their context\n",
    "3. this means that contextualized embeddings of a word different senses occupy different \"locations\" in the embedding space\n",
    "4. given that the embeddings are high-dimensional numeric vectors, we can cluster them to disambiguate senses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have asked OpenAI's GPT-4o to generate a list of sentences that use the word \"bank\" in different senses.\n",
    "Below, we'll use this data to see how well BERT's ability to generate contextualized embeddings allows us to disambiguate between this word's contextual meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained model for masked language modeling\n",
    "model = BertModel.from_pretrained(model_id)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_path = '/content/advanced_text_analysis/' if COLAB else '../../'\n",
    "base_path = Path(base_path)\n",
    "data_path = base_path / 'data' / 'misc'\n",
    "\n",
    "# load the file\n",
    "fp = data_path / 'bank_sentences_with_senses.csv'\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all sentences contain the word \"bank\"\n",
    "df.text.str.contains('bank').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sense').sample(1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('sense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer the sentences\n",
    "inputs = tokenizer(df.text.to_list(), return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create maks that is true where input ID == focal word ID\n",
    "focal_word_id = tokenizer.convert_tokens_to_ids('bank')\n",
    "mask = inputs['input_ids'] == focal_word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the inputs through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mask to the last hidden layer output to get the focal words embeddings\n",
    "focal_word_embeddings = outputs.last_hidden_state[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have the 768-dimensional embeddings of the focal word \"bank\" in each sentence\n",
    "focal_word_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:** How can we *see* whether or not and, if so, how the embeddings of words used in similar senses occupy similar locations in the embedding space?\n",
    "\n",
    "**Answer:** dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the embeddings to 2D\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_jobs=1)\n",
    "embeddings_2d = reducer.fit_transform(focal_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 2D embeddings by sense, using different colors and a legend indicating the sense\n",
    "for sense in df.sense.unique():\n",
    "    idxs = df.sense == sense\n",
    "    plt.scatter(embeddings_2d[idxs, 0], embeddings_2d[idxs, 1], label=sense, s=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we eyeballed the data to find clusters.\n",
    "\n",
    "**Question:** (How reliably) Can we automate this disambiguation approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster in 2D using k-means with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(focal_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get category indicator of the sense\n",
    "# cross tabulate the cluster labels with the sense labels\n",
    "pd.crosstab(df.cluster, df.sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use majority to label induced clusters\n",
    "cluster_to_sense = {0: 'geographical', 1: 'motion', 2: 'financial'}\n",
    "df['cluster_label'] = df.cluster.map(cluster_to_sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the clustering\n",
    "accuracy_score(df.sense, df.cluster_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get examples where cluster label disagrees with label\n",
    "for row in df[df.sense != df.cluster_label].itertuples():\n",
    "    print(f'in cluster \\'{row.cluster_label}\\' instead of \\'{row.sense}\\': \"{row.text}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Brainstorming session ðŸ”¥\n",
    "\n",
    "- Can you think of any potential uses BERT's ability to contextualize words' embeddings in your research, for example to study differences in word use across actors or domains?\n",
    "- Do you think you could as well us the pretrained `bert-base-uncased` model loaded above? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting masked-out words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for masked language modeling\n",
    "model = BertForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "# Define the text with a masked token\n",
    "text = \"He was walking along the [MASK] of the river.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the index of the masked token\n",
    "masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "\n",
    "# Predict the masked token\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0, masked_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the log probabilities of the 10 best fitting words\n",
    "log_probs = torch.log_softmax(predictions[0, masked_index], dim=-1)\n",
    "top_10_log_probs, top_10_indices = torch.topk(log_probs, 10)\n",
    "\n",
    "# Convert indices to tokens\n",
    "top_10_tokens = tokenizer.convert_ids_to_tokens(top_10_indices.tolist())\n",
    "\n",
    "# Print the results\n",
    "for token, log_prob in zip(top_10_tokens, top_10_log_probs):\n",
    "    print(f\"{token}: {log_prob.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example application: Using masked token prediction to study gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for masked language modeling\n",
    "model = BertForMaskedLM.from_pretrained(model_id)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def get_topk_words(text):\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the index of the masked token\n",
    "    masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "\n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Get the log probabilities of the 10 best fitting words\n",
    "    log_probs = torch.log_softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_10_log_probs, top_10_indices = torch.topk(log_probs, 10)\n",
    "\n",
    "    # Convert indices to tokens\n",
    "    top_10_tokens = tokenizer.convert_ids_to_tokens(top_10_indices.tolist())\n",
    "\n",
    "    # Print the results\n",
    "    out = pd.DataFrame({'token': top_10_tokens, 'log_prob': top_10_log_probs})\n",
    "    out['prob'] = np.exp(out.log_prob.to_numpy())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topk_words('He was very [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topk_words('She was very [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'Homosexuals'\\n\", get_topk_words('Homosexuals are making our country [MASK].'))\n",
    "print(\"'Straights'\\n\", get_topk_words('Straights are making our country [MASK].'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'Muslims'\\n\", get_topk_words('Muslims are making our country [MASK].'))\n",
    "print(\"'Christians'\\n\", get_topk_words('Christians are making our country [MASK].'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'Kids'\\n\", get_topk_words('Kids are very [MASK].'))\n",
    "print(\"'Teens'\\n\", get_topk_words('Teens are very [MASK].'))\n",
    "print(\"'Adults'\\n\", get_topk_words('Adults are very [MASK].'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
