{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing with word embeddings: Exercises\n",
    "\n",
    "| Author | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2023-09-24 |\n",
    "\n",
    "This notebook illustrates how to use `gensim` to compute with word vectors (e.g., word2vec) to, for example\n",
    "\n",
    "- compute two words similarity\n",
    "- find the most similar words for a focal word\n",
    "- solve analogy tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file in- and export\n",
    "import os\n",
    "\n",
    "# for working with word embeddings\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "# for using arrays and data frames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pre-trained word embedding model with `gensim`'s model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and name it's instance in our notebook environment 'word2vec'\n",
    "word2vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note_:** You can also load another model if you want. It will still be a `KeyedVectors` object. So you can directly apply what you learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_records': 3000000,\n",
       " 'file_size': 1743563840,\n",
       " 'base_dataset': 'Google News (about 100 billion words)',\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'parameters': {'dimension': 300},\n",
       " 'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
       " 'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
       "  'https://arxiv.org/abs/1301.3781',\n",
       "  'https://arxiv.org/abs/1310.4546',\n",
       "  'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
       " 'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
       " 'file_name': 'word2vec-google-news-300.gz',\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list available models\n",
    "print(list(api.info()['models'].keys()))\n",
    "\n",
    "# get detailed info for a specific model\n",
    "api.info(name='word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word vector similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Use the `wordnet` python package to find synonyms and antonyms of your choice of focus word(s), and compute their similarities.\n",
    "What do you observe?\n",
    "\n",
    "**_Hint:_** You can also define your own lists of synonyms and antonyms.\n",
    "\n",
    "**_Note:_** *WordNet* is a lexical database and semantic network of words and their relationships. \n",
    "It was developed to assist natural language processing and computational linguistics applications by providing a structured and comprehensive way to represent the English language vocabulary. \n",
    "WordNet was created at Princeton University and has been widely used in various text analysis tasks, including machine learning, information retrieval, and natural language understanding.\n",
    "([source](https://chat.openai.com/share/1de49018-f487-4789-82cb-98ccf8d47ccb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('nice.n.01') a city in southeastern France on the Mediterranean; the leading resort on the French Riviera\n",
      "Synset('nice.a.01') pleasant or pleasing or agreeable in nature or appearance; - George Meredith\n",
      "Synset('decent.s.01') socially or conventionally correct; refined or virtuous\n",
      "Synset('nice.s.03') done with delicacy and skill\n",
      "Synset('dainty.s.04') excessively fastidious and easily disgusted\n",
      "Synset('courteous.s.01') exhibiting courtesy and politeness\n"
     ]
    }
   ],
   "source": [
    "# import wordnet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# TODO: insert your focus word here\n",
    "focus_word = 'nice' # <== change!\n",
    "\n",
    "# find the sysnset for the focus word of your choice\n",
    "synsets = wordnet.synsets(focus_word)\n",
    "for synset in synsets:\n",
    "    print(synset, synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for: nice\n",
      "{'prissy', 'squeamish', 'Nice', 'gracious', 'courteous', 'decent', 'skillful', 'dainty'}\n",
      "Antonyms for: nice\n",
      "{'nasty'}\n"
     ]
    }
   ],
   "source": [
    "# find the synonyms and antonyms for <YOUR WORD>\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for synset in wordnet.synsets(focus_word):\n",
    "    for lemma in synset.lemmas():\n",
    "        if focus_word not in lemma.name():\n",
    "            synonyms.append(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            for a in lemma.antonyms():\n",
    "                antonyms.append(a.name())\n",
    "\n",
    "# print the results\n",
    "print('Synonyms for: ' + focus_word)\n",
    "print(set(synonyms))\n",
    "print('Antonyms for: ' + focus_word)\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Nice\": 0.46746230125427246\n",
      "\"decent\": 0.5993331670761108\n",
      "\"skillful\": 0.20048925280570984\n",
      "\"dainty\": 0.2785818874835968\n",
      "\"prissy\": 0.25895628333091736\n",
      "\"squeamish\": 0.19743596017360687\n",
      "\"courteous\": 0.28391408920288086\n",
      "\"gracious\": 0.4229739308357239\n"
     ]
    }
   ],
   "source": [
    "# TODO: iterate over synonyms and antonyms and print the similarity between focus word and synonym/antonym pairs\n",
    "for synonym in synonyms:\n",
    "    if synonym in word2vec.index_to_key:\n",
    "        print(f'\"{synonym}\": {word2vec.similarity(focus_word, synonym)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nasty\": 0.3650338053703308\n"
     ]
    }
   ],
   "source": [
    "for antonym in antonyms:\n",
    "    print(f'\"{antonym}\": {word2vec.similarity(focus_word, antonym)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2\n",
    "\n",
    "Let's implement a classic approach to evaluate how word embeddings capture cultural biases in their training copora.\n",
    "Here, we'll focus on **_gender bias_** &mdash; the differential association of traits and attributes with women and men (my lose definition).\n",
    "\n",
    "Compile a list `comparison_words` with occupations, character traits, and other words that might exhibit gender bias.\n",
    "Then compute how similar each word is with terms like 'man' and 'women', that indicate the male and female genders.\n",
    "\n",
    "Which words exhibit gender bias?\n",
    "And in which direction? \n",
    "Do you spot a pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_words = [\n",
    "    'programmer',\n",
    "    'scientist',\n",
    "    'smart',\n",
    "    'emotional',\n",
    "    'caring',\n",
    "    'CEO',\n",
    "    'annoying',\n",
    "    'football',\n",
    "    'soccer',\n",
    "    'politician',\n",
    "    'populist',\n",
    "    'power',\n",
    "    'nurse',\n",
    "    'doctor'\n",
    "    # add more interesting words here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarities to male and female terms\n",
    "male_terms = ['man', 'men', 'male', 'he', 'him', 'his']\n",
    "female_terms = ['woman', 'women', 'female', 'she', 'her', 'hers']\n",
    "\n",
    "s_male = dict()\n",
    "s_female = dict()\n",
    "\n",
    "for word in comparison_words:\n",
    "    s_male[word] = []\n",
    "    s_female[word] = []\n",
    "    for w in male_terms:\n",
    "        s = word2vec.similarity(word, w)\n",
    "        s_male[word].append(s)\n",
    "    for w in female_terms:\n",
    "        s = word2vec.similarity(word, w)\n",
    "        s_female[word].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'programmer': 0.029597761,\n",
       " 'scientist': 0.06876839,\n",
       " 'smart': 0.041580766,\n",
       " 'emotional': 0.18292125,\n",
       " 'caring': 0.20474696,\n",
       " 'CEO': -0.106728755,\n",
       " 'annoying': 0.097587876,\n",
       " 'football': 0.063607715,\n",
       " 'soccer': 0.13928266,\n",
       " 'politician': 0.22477663,\n",
       " 'populist': 0.08099014,\n",
       " 'power': 0.042475563,\n",
       " 'nurse': 0.3203691,\n",
       " 'doctor': 0.23384072}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{w: np.mean(s) for w, s in s_female.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'programmer': 0.054541085,\n",
       " 'scientist': 0.10331989,\n",
       " 'smart': 0.06168256,\n",
       " 'emotional': 0.15299931,\n",
       " 'caring': 0.17566599,\n",
       " 'CEO': -0.0061667506,\n",
       " 'annoying': 0.07397268,\n",
       " 'football': 0.17734624,\n",
       " 'soccer': 0.11919821,\n",
       " 'politician': 0.22752604,\n",
       " 'populist': 0.09036921,\n",
       " 'power': 0.07486963,\n",
       " 'nurse': 0.19163398,\n",
       " 'doctor': 0.27489495}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{w: np.mean(s) for w, s in s_male.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: summarize the results in a table or figure\n",
    "# hint: if you have more than one term per gender, you might want to compute the average of comparison term--gender word similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "\n",
    "Implement the same logic but now use the gender bisa-related word and attribute lists used in Caliskan *et al.*'s paper [\"Semantics derived automatically from language corpora contain human-like biases\"](https://www.science.org/doi/10.1126/science.aal4230).\n",
    "\n",
    "You find the word and attribute lists in the folder `./../data/replications/caliskan_semantics_2017/wordlists/` (e.g., the file 'science_arts_male_female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nearest neihbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Let's use nearest neighbors search to find conceptually equivalent terms for a \"seed\" word.\n",
    "\n",
    "**_Note:_** This is a typical task in expanding keyword lists for dictionaries.\n",
    "\n",
    "You can choose which seed word you want to start with (see example below for a suggestion).\n",
    "But while going through nearest neighbors, keep track of how many of the candidate terms in the top-20 or top-50 terms (or so) you would inlcude in your dictionary, and how many you would discard!\n",
    "\n",
    "**_Example_**: \n",
    "Say you want to compile a dictionary that contains typical words used to express *positive emotions*.\n",
    "In this case, you could start with the seed word 'happy.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w, s in word2vec.most_similar('happy', topn=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: I made decisions\n",
    "'glad'  # approved\n",
    "'pleased', # approved\n",
    "'ecstatic', # approved\n",
    "'overjoyed', # approved\n",
    "'thrilled', # approved\n",
    "'satisfied', # approved\n",
    "'proud', # approved\n",
    "'delighted', # approved\n",
    "'disappointed', # not approved <== !!!\n",
    "'excited', # approved\n",
    "'happier', # approved\n",
    "'Said_Hirschbeck', # not approved <== !!!\n",
    "# ... and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Discuss with your neighbor how one could improve this nearest neighbor search-based dictionary expansion strategy?\n",
    "Do you ideas for automated quality checks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Can you come up with analogy problems involving terms from your discipline or research area?\n",
    "Can the word embedding model solve these specialized problems?\n",
    "\n",
    "**_Example:_** In politics \"Democrat is to progressive what Republican is to ___?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_x1 = word2vec['Democrat']\n",
    "v_y1 = word2vec['progressive']\n",
    "v_x2 = word2vec['Republican']\n",
    "\n",
    "v_q = v_y1 - v_x1 + v_x2\n",
    "\n",
    "word2vec.similar_by_vector(v_q, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Take examples from one of the word lists in the folder `./../data/benchmarks/bats/3_encyclopedic_semantics/` to construct analgoy tests.\n",
    "How well does the word2vec model perform on average?\n",
    "\n",
    "**_Hint:_** Think about possible ways of defining performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "fp = './../data/benchmarks/bats/3_encyclopedic_semantics/E01 [country - capital].txt'\n",
    "\n",
    "with open(fp, 'r') as f:\n",
    "    wordlist = [tuple(line.strip().split('\\t')) for line in f]\n",
    "\n",
    "wordlist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: depending on your evaluation strategy, you might need to change this function\n",
    "def analogy(x1='man', y1='king', x2='woman', verbose=True):\n",
    "    \"\"\"Computes return to query 'y1 is to x1 what WORD is to x2?'\"\"\"\n",
    "    result = word2vec.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    if verbose:\n",
    "        print(f\"'{x1}' : '{y1}' :: '{x2}' : ?? ==> '{result[0][0]}'\")\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogy(x1='man', y1='king', x2='women') # :)\n",
    "analogy(x1='abuja', y1='nigeria', x2='ankara') # :/\n",
    "analogy(x1='nigeria', y1='abuja', x2='turkey') # x/\n",
    "analogy(x1='athens', y1='greece', x2='baghdad') # :/\n",
    "analogy(x1='berlin', y1='germany', x2='paris') # x/\n",
    "analogy(x1='germany', y1='berlin', x2='france') # x/\n",
    "analogy(x1='germany', y1='berlin', x2='france') # x/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
