{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune a transformer model for sentence classification\n",
        "\n",
        "| Authors | Last update |\n",
        "|:------ |:----------- |\n",
        "| Hauke Licht (https://github.com/haukelicht) | 2023-09-29 |\n",
        "\n",
        "This notebook shows how to use the hugging face ü§ó `transformers` library to train a Transformer-based sentence classifier with transfer learning (i.e., \"fine tuning\").\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/haukelicht/advanced_text_analysis/blob/main/notebooks/04_transformers_sentence_classification.ipynb)\n",
        "\n",
        "**_Source:_** The notebook is adapted from the one distributed with this tutorial: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
        "\n",
        "**Table of contents**\n",
        "\n",
        "- Setup\n",
        "- Supervised text classification\n",
        "- Preparing the data\n",
        "    - loading the dataset\n",
        "    - preprocessing\n",
        "- How to evaluate model performance\n",
        "- Train\n",
        "    - setting up the GPU (if available)\n",
        "    - preparing the trainer\n",
        "    - training\n",
        "    - test set evaluation\n",
        "    - storing the model\n",
        "- Inference\n",
        "- Outlook\n",
        "- Appendix\n",
        "    - reproducibility\n",
        "    - data loading\n",
        "    - data splitting\n",
        "    - multi-class classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you run this notebook on Google Colab or you have not yet installed the `transformers` and `datasets` python libraries, you need to do so first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t6lIsnGIwC5"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install accelerate transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# dataset loading\n",
        "from datasets import load_dataset, DatasetDict \n",
        "\n",
        "# used to tokenize text\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# used to load the pre-trained model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# used to finetune the pre-trained model\n",
        "from transformers import Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge7uLpvVIwC6"
      },
      "source": [
        "## Supervised text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJDLJIWIIwC8"
      },
      "source": [
        "Text classification means to assigns a label or class to each text in a corpus.\n",
        "It is a common NLP and computational text analysis task.\n",
        "\n",
        "A common and popular text classification task is **_sentiment analysis_**.\n",
        "Sentiment analysis assigns a label like üôÇ 'positive', üôÅ 'negative', or üòê 'neutral' to a sequence of text, for example a sentence of paragraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ingredients\n",
        "\n",
        "Here is what you need for training a supervised sequence classifier through finetuning (i.e. transfer learning):\n",
        "\n",
        "- a **pre-trained model** you can fine-tune for sequence classification\n",
        "    - the tokenizer comes with this too\n",
        "- a pre-defined set of **label classes** (e.g., 'positive', 'neutral', 'negative')\n",
        "- a **label dataset**, i.e., a corpus of texts (e.g., sentences) in which each document has been assigned to a single label label class\n",
        "    - this we split into train, development, and test set\n",
        "- some **metric to quantify classification performance** so that we know how well our classifier is doing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This notebook\n",
        "\n",
        "In this notebook, we will use\n",
        "\n",
        "1. the [IMDb](https://huggingface.co/datasets/imdb) corpus that records movie review that have been classified as positive or negative, and\n",
        "2. finetune a [DistilBERT](https://huggingface.co/distilbert-base-uncased) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejVMVM5hIwC9"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el12ezRDIwC9"
      },
      "source": [
        "We'll use the IMDb dataset from the ü§ó `datasets` library.\n",
        "However, the train and test splits ([?](https://chat.openai.com/share/d71207ff-d374-4540-927a-83ac5370cd8f)) of this dataset each contain 25,000 documents.\n",
        "Using them all will result in very slow training and we'll thus just use subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3000, 1000, 1000)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_train = 3000\n",
        "n_dev = 1000\n",
        "n_test = 1000\n",
        "\n",
        "# sample without replacement\n",
        "idxs = np.random.choice(25_000, n_train + n_dev + n_test, replace=False)\n",
        "\n",
        "# split the indices into train, dev, and test\n",
        "train_idxs = idxs[:n_train]\n",
        "dev_idxs = idxs[n_train:(n_train+n_dev)]\n",
        "test_idxs = idxs[-n_test:]\n",
        "\n",
        "# show the number of examples in each split\n",
        "len(train_idxs), len(dev_idxs), len(test_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Ec1YkX5OIwC9"
      },
      "outputs": [],
      "source": [
        "imdb = DatasetDict({\n",
        "    \"train\": load_dataset(\"imdb\", split='train').select(train_idxs),\n",
        "    \"dev\": load_dataset(\"imdb\", split='test').select(dev_idxs),\n",
        "    \"test\": load_dataset(\"imdb\", split='test').select(test_idxs),\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `imdb` is an instance of the `datasets` `DatasetDict` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.dataset_dict.DatasetDict"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(imdb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This class is there to gather several pre-defined splits of a dataset.\n",
        "\n",
        "Among these splits, one is usually named \"train\" and another on \"test\" (see next cell).\n",
        "\n",
        "**_Note:_** It'll become clearer further below why we need these splits. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'dev', 'test'])"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3000, 1000, 1000)"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(imdb['train']), len(imdb['dev']), len(imdb['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-d-KOvzIwC9"
      },
      "source": [
        "Here is how you can access one \"example\" (i.e., observation) in the the \"test\" split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(imdb[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "kQEm_8XyIwC9",
        "outputId": "0219cd92-b7a1-4550-b6ab-408b29f400ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': \"It's hard to believe that with a cast as strong as this one has, that this movie can be such a dud. It's such an incredibly horrible film. How was it ever made? How did so many good actors wind up in such a terrible film? Don't waste your life. Don't watch even one moment of this film.\",\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZBe68pIwC-"
      },
      "source": [
        "This shows in the test split, there are two fields for *each* example :\n",
        "\n",
        "- `text`: the movie review text.\n",
        "- `label`: a value that is either `0` for a negative review or `1` for a positive review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**_Important:_** check that the splits have about equal label class distributions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% \"pos\" in train: 0.5076666666666667\n",
            "% \"pos\" in dev: 0.499\n",
            "% \"pos\" in test: 0.498\n"
          ]
        }
      ],
      "source": [
        "print('% \"pos\" in train:', np.mean([ex['label'] for ex in imdb[\"train\"]]))\n",
        "print('% \"pos\" in dev:', np.mean([ex['label'] for ex in imdb[\"dev\"]]))\n",
        "print('% \"pos\" in test:', np.mean([ex['label'] for ex in imdb[\"test\"]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vw5OFMUIwC-"
      },
      "source": [
        "### Preprocessing texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLhZbEdaIwC-"
      },
      "source": [
        "The next step is to load a DistilBERT tokenizer to preprocess the `text` field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "FXLRVYJ6IwC-"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# read about \"byte-pair encoding\" and \"sentence-piece\" algorithms if interested in how tokeeizers work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tokenizer is a so-called *callable* and can thus be used like a function:\n",
        "If you input a text string, it return a dictionary with the tokenized text and additional information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        }
      ],
      "source": [
        "toks = tokenizer(\"Hello, this one sentence! [SEP] And this is another one.\")\n",
        "print(toks.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The field 'input_ids' indicates the numbers used to represent the tokens in the example sentence.\n",
        "- The 'attention_mask' is there to help the model to know to which tokens in of a bunch of sentences it should pay attention when fine-tuning, and which it can ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 2003, 2178, 2028, 1012, 0, 102]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'hello',\n",
              " ',',\n",
              " 'this',\n",
              " 'one',\n",
              " 'sentence',\n",
              " '!',\n",
              " '[SEP]',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'another',\n",
              " 'one',\n",
              " '.',\n",
              " '[PAD]',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "toks['input_ids']\n",
        "print(toks['input_ids'])\n",
        "tokenizer.convert_ids_to_tokens(toks['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "print(toks['attention_mask'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdrDTeZYIwC-"
      },
      "source": [
        "Let's create a helper function that tokenizes the `text` value of an input called example.\n",
        "This will allow us to iterate over examples in our dataset splits (e.g., `imdb[\"test\"]`) and pre-process them one by one.\n",
        "\n",
        "**_Note:_** Setting `truncate=True` we ensure that none of the text sequences we'll use for fine-tuning is too longer for DistilBERT to handle it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "veclR-QmIwC-"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA74NYovIwC-"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ü§ó Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "kbNKLzNbIwC-"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ceefdaabf7348ab8f8d995b4b7c9c21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e970af88513d49869aa03b2d965006e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f099d6384174b9db9a8545c5aa15f2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# need to do this beause you want to add the input IDs and \n",
        "#  attention mask values to each example in each of the data splits\n",
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'As far as the movie goes, it\\'s an OK science fiction movie. It has a lot of cool stuff in it, and some quality scenes. That said, it\\'s not that good, and some of the stuff is pretty far fetched...<br /><br />As for calling this another cube-movie is utter and complete bullsh!t. This is the very definition of milking a great and inventive original movie... The whole feel to it can be somewhat translated into the core of the first, but the introduction of people/androids as part of the \"team\" behind the cube itself is somewhat a stretch...<br /><br />I gave this a 3*** because of the backstabbing of the original. This one should have been kept sterile in so many parts of the movie that there is no place or time to mention them all...<br /><br />Watchable for those who have not seen Cube & Hypercube, but not recommendable for fans of the series...',\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(imdb)\n",
        "imdb.keys()\n",
        "imdb['train']\n",
        "imdb['train'][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'As far as the movie goes, it\\'s an OK science fiction movie. It has a lot of cool stuff in it, and some quality scenes. That said, it\\'s not that good, and some of the stuff is pretty far fetched...<br /><br />As for calling this another cube-movie is utter and complete bullsh!t. This is the very definition of milking a great and inventive original movie... The whole feel to it can be somewhat translated into the core of the first, but the introduction of people/androids as part of the \"team\" behind the cube itself is somewhat a stretch...<br /><br />I gave this a 3*** because of the backstabbing of the original. This one should have been kept sterile in so many parts of the movie that there is no place or time to mention them all...<br /><br />Watchable for those who have not seen Cube & Hypercube, but not recommendable for fans of the series...',\n",
              " 'label': 0,\n",
              " 'input_ids': [101,\n",
              "  2004,\n",
              "  2521,\n",
              "  2004,\n",
              "  1996,\n",
              "  3185,\n",
              "  3632,\n",
              "  1010,\n",
              "  2009,\n",
              "  1005,\n",
              "  1055,\n",
              "  2019,\n",
              "  7929,\n",
              "  2671,\n",
              "  4349,\n",
              "  3185,\n",
              "  1012,\n",
              "  2009,\n",
              "  2038,\n",
              "  1037,\n",
              "  2843,\n",
              "  1997,\n",
              "  4658,\n",
              "  4933,\n",
              "  1999,\n",
              "  2009,\n",
              "  1010,\n",
              "  1998,\n",
              "  2070,\n",
              "  3737,\n",
              "  5019,\n",
              "  1012,\n",
              "  2008,\n",
              "  2056,\n",
              "  1010,\n",
              "  2009,\n",
              "  1005,\n",
              "  1055,\n",
              "  2025,\n",
              "  2008,\n",
              "  2204,\n",
              "  1010,\n",
              "  1998,\n",
              "  2070,\n",
              "  1997,\n",
              "  1996,\n",
              "  4933,\n",
              "  2003,\n",
              "  3492,\n",
              "  2521,\n",
              "  18584,\n",
              "  2098,\n",
              "  1012,\n",
              "  1012,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  2004,\n",
              "  2005,\n",
              "  4214,\n",
              "  2023,\n",
              "  2178,\n",
              "  14291,\n",
              "  1011,\n",
              "  3185,\n",
              "  2003,\n",
              "  14395,\n",
              "  1998,\n",
              "  3143,\n",
              "  12065,\n",
              "  2232,\n",
              "  999,\n",
              "  1056,\n",
              "  1012,\n",
              "  2023,\n",
              "  2003,\n",
              "  1996,\n",
              "  2200,\n",
              "  6210,\n",
              "  1997,\n",
              "  6501,\n",
              "  2075,\n",
              "  1037,\n",
              "  2307,\n",
              "  1998,\n",
              "  1999,\n",
              "  15338,\n",
              "  3512,\n",
              "  2434,\n",
              "  3185,\n",
              "  1012,\n",
              "  1012,\n",
              "  1012,\n",
              "  1996,\n",
              "  2878,\n",
              "  2514,\n",
              "  2000,\n",
              "  2009,\n",
              "  2064,\n",
              "  2022,\n",
              "  5399,\n",
              "  5421,\n",
              "  2046,\n",
              "  1996,\n",
              "  4563,\n",
              "  1997,\n",
              "  1996,\n",
              "  2034,\n",
              "  1010,\n",
              "  2021,\n",
              "  1996,\n",
              "  4955,\n",
              "  1997,\n",
              "  2111,\n",
              "  1013,\n",
              "  11924,\n",
              "  2015,\n",
              "  2004,\n",
              "  2112,\n",
              "  1997,\n",
              "  1996,\n",
              "  1000,\n",
              "  2136,\n",
              "  1000,\n",
              "  2369,\n",
              "  1996,\n",
              "  14291,\n",
              "  2993,\n",
              "  2003,\n",
              "  5399,\n",
              "  1037,\n",
              "  7683,\n",
              "  1012,\n",
              "  1012,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1045,\n",
              "  2435,\n",
              "  2023,\n",
              "  1037,\n",
              "  1017,\n",
              "  1008,\n",
              "  1008,\n",
              "  1008,\n",
              "  2138,\n",
              "  1997,\n",
              "  1996,\n",
              "  10457,\n",
              "  2696,\n",
              "  23200,\n",
              "  1997,\n",
              "  1996,\n",
              "  2434,\n",
              "  1012,\n",
              "  2023,\n",
              "  2028,\n",
              "  2323,\n",
              "  2031,\n",
              "  2042,\n",
              "  2921,\n",
              "  25403,\n",
              "  1999,\n",
              "  2061,\n",
              "  2116,\n",
              "  3033,\n",
              "  1997,\n",
              "  1996,\n",
              "  3185,\n",
              "  2008,\n",
              "  2045,\n",
              "  2003,\n",
              "  2053,\n",
              "  2173,\n",
              "  2030,\n",
              "  2051,\n",
              "  2000,\n",
              "  5254,\n",
              "  2068,\n",
              "  2035,\n",
              "  1012,\n",
              "  1012,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  3422,\n",
              "  3085,\n",
              "  2005,\n",
              "  2216,\n",
              "  2040,\n",
              "  2031,\n",
              "  2025,\n",
              "  2464,\n",
              "  14291,\n",
              "  1004,\n",
              "  23760,\n",
              "  10841,\n",
              "  4783,\n",
              "  1010,\n",
              "  2021,\n",
              "  2025,\n",
              "  16755,\n",
              "  3085,\n",
              "  2005,\n",
              "  4599,\n",
              "  1997,\n",
              "  1996,\n",
              "  2186,\n",
              "  1012,\n",
              "  1012,\n",
              "  1012,\n",
              "  102],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(tokenized_imdb)\n",
        "tokenized_imdb.keys()\n",
        "tokenized_imdb['train'][0]#.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsVftz9cIwC-"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). \n",
        "\n",
        "Our `data_collator` instance of this class will handle preprocessing and a thing called \"padding\" when sampling batches of examples during finetuning to iteratively update our classifier's parameters.\n",
        "\n",
        "*Padding* means that you make all text sequences in a set of sequences the same length.\n",
        "To do this, we just append the `<PAD>` special token to shorter text sequences in the set.\n",
        "For example, the (tokenized) sequences in the following set \n",
        "\n",
        "```json\n",
        "[\n",
        "    ['Hello', 'world', '!'               ],\n",
        "    ['Have',  'a',     'nice', 'day', '!']\n",
        "]\n",
        "```\n",
        "\n",
        "will be \"padded\" to \n",
        "\n",
        "```json\n",
        "[\n",
        "    ['Hello', 'world', '!',    '<PAD>', '<PAD>'],\n",
        "    ['Have',  'a',     'nice', 'day',   '!'    ]\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "nOP2H-qPIwC-"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G61kFtuQIwC-"
      },
      "source": [
        "## How to evaluate model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZfQHOjuIwC-"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance.\n",
        "\n",
        "**_Note_** You could also just load a evaluation method with the ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) library (see the ü§ó Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib-IdxbhIwC-"
      },
      "source": [
        "Let's create a function that passes your predictions and labels to calculate some central metrics (explanations below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "w0FFpYUJIwC-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true=labels, y_pred=predictions, average='binary', zero_division=0)\n",
        "    metrics = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": p,\n",
        "        \"recall\": r,\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compute the following metrics:\n",
        "\n",
        "- precision: the share of examples a classifier as correctly assigned into a class\n",
        "- recall: the share of positive examples a classifier labels correctly\n",
        "- F1: a measure combining recall and precision\n",
        "- balanced accuary: an accurarcy metric adjusting for class imbalance\n",
        "\n",
        "<p><a href=\"https://commons.wikimedia.org/wiki/File:Precisionrecall.svg#/media/File:Precisionrecall.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precisionrecall.svg\" height=\"800\" width=\"440\"></a><br>By &lt;a href=\"//commons.wikimedia.org/wiki/User:Walber\" title=\"User:Walber\"&gt;Walber&lt;/a&gt; - &lt;span class=\"int-own-work\" lang=\"en\"&gt;Own work&lt;/span&gt;, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=36926283\">Link</a></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 0.5, 0.6666666666666666)"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p, r, f1, _ = precision_recall_fscore_support(y_true=[0, 1, 1], y_pred=[0, 0, 1], average='binary', zero_division=0)\n",
        "p, r, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "2*(p*r / (p+r))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfqRQ3_8IwC_"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ryvTj2IwC_"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODl7cT-SIwC_"
      },
      "source": [
        "Before you start training your model, create two dictionaries that map labels' numeric IDS their character values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d9UyZaDvIwC_"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "douQqh3YIwC_"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "EWRzhpclIwC_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, # <== the name of the pre-trained model (downloaded from huggingface hub)\n",
        "    num_labels=2, # number of label classes (adapt this if you have, e.g., 4 label classes)\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspect the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the GPU (if available)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make training as fast as possible, you want to utilize GPU computing.\n",
        "When you run notebooks on Colab, you can enable GPU computing by \n",
        "\n",
        "1. clicking on \"Runtime\" in the menu,\n",
        "2. selecting \"Change runtime type\", and\n",
        "3. choose \"GPU\" in the \"Hardware accelerator\" section of the pop-up\n",
        "\n",
        "If you are running this notebook elsewhere, you want to determine to what kind of device you have access\n",
        "\n",
        "- with a GPU &rarr; \"cuda\"\n",
        "- with MacOS's M1/M2 chip &rarr; \"mps\"\n",
        "- else \"cpu\"\n",
        "\n",
        "We do so like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='mps')"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "# check if GPU or MPS is available, else use CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "device = torch.device(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we've figured this out, we put our model on that device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTANT: put this thing to the respective device (e.g., GPU)\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g_U2wEeIwC_"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
        "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "kStUdSXmIwC_"
      },
      "outputs": [],
      "source": [
        "# define the path where you want to save the fine tuned model\n",
        "model_path = os.path.join('..', 'data', 'models', 'distillbert_ibmd_sentiment')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    # leave the following unchanged ;)\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16, # <== reduce only if you get a \"CUDA out of memory\" error\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    # increase this value to train for longer\n",
        "    num_train_epochs=2,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # how to save and determine (\"best\") model\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"f1\", # <== needs to match one of the names of the dictionary returned by `compute_metrics()` function\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model, # the model instance you loaded two cells above\n",
        "    args=training_args, # the training args you created one cells above\n",
        "    train_dataset=tokenized_imdb[\"train\"], # the training data split\n",
        "    eval_dataset=tokenized_imdb[\"dev\"], # the testing data split\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can finetune the model!\n",
        "\n",
        "**_Warning:_** This will take long if you are using only your CPU ü•π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hlicht/miniforge3/envs/advanced_text_analysis_gesis_2023/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fb351dca4e4428c9970b7dacc0ddb43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/376 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "553ee113de4446978794e8277c548d07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2353682667016983, 'eval_f1': 0.9089068825910932, 'eval_precision': 0.918200408997955, 'eval_recall': 0.8997995991983968, 'eval_runtime': 26.9237, 'eval_samples_per_second': 37.142, 'eval_steps_per_second': 2.34, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bc1c1a97a034273a8dc9568f3953363",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.25140172243118286, 'eval_f1': 0.9069767441860467, 'eval_precision': 0.8780487804878049, 'eval_recall': 0.9378757515030061, 'eval_runtime': 26.6467, 'eval_samples_per_second': 37.528, 'eval_steps_per_second': 2.364, 'epoch': 2.0}\n",
            "{'train_runtime': 609.4697, 'train_samples_per_second': 9.845, 'train_steps_per_second': 0.617, 'train_loss': 0.2808544077771775, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=376, training_loss=0.2808544077771775, metrics={'train_runtime': 609.4697, 'train_samples_per_second': 9.845, 'train_steps_per_second': 0.617, 'train_loss': 0.2808544077771775, 'epoch': 2.0})"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f435a68d0f41438c8dc5b73ff5727f51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.2442506104707718,\n",
              " 'eval_f1': 0.8967611336032388,\n",
              " 'eval_precision': 0.9040816326530612,\n",
              " 'eval_recall': 0.8895582329317269,\n",
              " 'eval_runtime': 27.5243,\n",
              " 'eval_samples_per_second': 36.332,\n",
              " 'eval_steps_per_second': 2.289,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 231,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate the final model on the held-out tetst set\n",
        "trainer.evaluate(tokenized_imdb[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation**\n",
        "\n",
        "- the precision of 0.85 tells us that the classifier is correct about 17 out of 20 times when it says a text has \"positive\" sentiment (our positive label class)\n",
        "- the recall of 0.85 tells us that the classifier correctly classifies about 17 in every 20 \"true\" positive-sentiment examples\n",
        "- the F1 score just summarizes thes values in one score\n",
        "\n",
        "Overall our classifier performs pretty well even with only 3000 traning examples. ü•≥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the model and tokenizer for re-use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean-up all other checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.DS_Store',\n",
              " 'tokenizer_config.json',\n",
              " 'special_tokens_map.json',\n",
              " 'config.json',\n",
              " 'tokenizer.json',\n",
              " 'training_args.bin',\n",
              " 'vocab.txt',\n",
              " 'pytorch_model.bin',\n",
              " 'checkpoint-188',\n",
              " 'checkpoint-376']"
            ]
          },
          "execution_count": 233,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['checkpoint-188', 'checkpoint-376']"
            ]
          },
          "execution_count": 234,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoints = [fn for fn in os.listdir(model_path) if fn.startswith('checkpoint-')]\n",
        "checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# remove the checkpoint folders\n",
        "for checkpoint in checkpoints:\n",
        "    dir_path = os.path.join(model_path, checkpoint)\n",
        "    shutil.rmtree(dir_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed look at the classifiers output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create predictions for the first three examples in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82d7aa943d594b4babf6c6b3a0d643b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "preds = trainer.predict(tokenized_imdb[\"test\"].select([0, 1, 2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "transformers.trainer_utils.PredictionOutput"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(preds.predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds.predictions.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prediction array has two dimensions:\n",
        "\n",
        "- the first axis ('rows') corresponds to the *number of examples* for which we generated predictions\n",
        "- the second axis ('columns') corresponds to the *number of label classes* we generate probability-like scores for when predicting\n",
        "\n",
        "Let's look at the scores for the first example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1.7259933948516846, -1.9657995700836182]"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds.predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first score is larger than the second one.\n",
        "This means that given example is more similar to examples from the first label class: documents with negative sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NEGATIVE'"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2label[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To convert those scores in something probability-like, we apply the so-called [softmax transformation](), which rescales values such that they each range between 0 and 1 and sum to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.975679  , 0.02432101], dtype=float32)"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "softmax(preds.predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also call this function on all examples' prediction scores in our current batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.975679  , 0.02432101],\n",
              "       [0.97882915, 0.02117081],\n",
              "       [0.05613839, 0.9438616 ]], dtype=float32)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_probs = softmax(preds.predictions, axis=1)\n",
        "pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"It's hard to believe that with a cast as strong as this one has, that this movie can be such a dud. It's such an incredibly horrible film. How was it ever made? How did so many good actors wind up in such a terrible film? Don't waste your life. Don't watch even one moment of this film.\",\n",
              " \"One of the worst Arnold movies I've seen. Special effects were terrible. Script was horrible. Hopefully his next movie will be much better like T2, Total Recall, True Lies and Eraser(not as good as the rest). Watch Stigmata if you want to see an apocalyptic future movie. It's much better.\",\n",
              " \"I saw this on DVD with subtitles, which made it a little frustrating to get through, because of the film's length. But I was riveted throughout all of it. That I was fascinated by the characters and always engrossed in the story, despite the subtitles, is a testament to the film's power. It's an amazing piece of work. I have it on my list of ten favorite films of all time. It's easily the best foreign film I've seen in the last twenty years or so. I would like to know the full story behind the making of this film. It must have taken a very long time and required the use of hundreds of locations. Its use of some hardcore scenes (on the TV in the motel room) may unfortunately make some people choose not to see it, but if you don't mind those, you'll be deeply moved by all the stories in this one!\"]"
            ]
          },
          "execution_count": 236,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[ex['text'] for ex in tokenized_imdb[\"test\"].select([0, 1, 2])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now if you want to know for each row in which cell the value is the largest, you can call the `argmax()` method on the numpy array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_probs.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This turns prediction scores into predicted labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['NEGATIVE', 'NEGATIVE', 'POSITIVE']"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[id2label[pp] for pp in preds.predictions.argmax(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cmBaAB-IwC_"
      },
      "source": [
        "## Inference\n",
        "\n",
        "\"Inference\" = \"Using the model for labeling texts\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOM0j0yZIwC_"
      },
      "source": [
        "When you have saved your finetuned model, you can always re-load it to label texts.\n",
        "In machine learning this is called \"inference\" &mdash; which is unfortunate given the meaning of the term in positive social science methodology.\n",
        "\n",
        "So let's just call it **prediction**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_classifier = pipeline(task=\"text-classification\", model=model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZBBmqrvIwDC"
      },
      "source": [
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for sentiment analysis with your model, and pass your text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "wF0rTEXuIwDC",
        "outputId": "c3986ab0-b2d6-42c1-8c00-8745c628e5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.945286750793457}]\n"
          ]
        }
      ],
      "source": [
        "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
        "print(sentiment_classifier(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9645718932151794}]"
            ]
          },
          "execution_count": 248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = imdb[\"test\"][0][\"text\"]\n",
        "sentiment_classifier(text)\n",
        "# TODO: figure out if you can make this deterministic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0y4lRm7IwDD"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like:\n",
        "\n",
        "Tokenize the text and return PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo3PR9lEIwDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq65AznnIwDD"
      },
      "source": [
        "Pass your inputs to the model and return the `logits`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "njKLoOP-IwDD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'POSITIVE'"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id] # <== use the 'id2label' we've added to the model we saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DistillBERT is not the only pre-trained model you can fine tune for sequence classification.\n",
        "The huggingface `transformers` library supports it also for the following many models, for example\n",
        "\n",
        "- [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert)\n",
        "- [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert)\n",
        "- [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2)\n",
        "- [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert)\n",
        "- [OpenAI GPT-2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt2)\n",
        "- [LLaMA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/llama)\n",
        "- [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer)\n",
        "- [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart)\n",
        "- [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta)\n",
        "- [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In programming, randomness is governed by *Random Number Generator* (RNG) algorithms.\n",
        "You can control randomness by setting a so called [\"seed\"](https://towardsdatascience.com/random-seeds-and-reproducibility-933da79446e3) that determines an RNG's initial state.\n",
        "By setting a seed at the beginning of your script, each random value you generate (e.g., like [this](https://realpython.com/numpy-random-number-generator/)) will be the same at each run of your script &mdash; as long as you run all the code in the script in the same order (e.g., cell by cell, from bottom to top).\n",
        "\n",
        "**_Important:_** If you set the seed but programm interactively, the order in which you call individual code chunks will vary from interactive use to interactive use.\n",
        "So setting a seed at the beginning of the script will make it's execution only reproducible for runs from top to bottom (without user interaction in between).\n",
        "\n",
        "The packages you will use for randomizing computations have notes on the topic of reproducibility you should read:\n",
        "\n",
        "- `random`: https://docs.python.org/3/library/random.html#notes-on-reproducibility\n",
        "- `numpy`: https://numpy.org/doc/stable/reference/random/generator.html (but read also [here](https://builtin.com/data-science/numpy-random-seed), [here](https://albertcthomas.github.io/good-practices-random-number-generators/), and [here](https://stackoverflow.com/a/5837352))\n",
        "- `pandas`: random number generation inside `pandas` code uses `numpy` under the hood.\n",
        "- `torch` and all pacakges using it (e.g., `transformers`): https://pytorch.org/docs/stable/notes/randomness.html\n",
        "- `transformers`: introduced [here](https://github.com/huggingface/transformers/pull/16907)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import set_seed\n",
        "\n",
        "SEED = 42 # you can choose any integer value\n",
        "\n",
        "# set seed for random package\n",
        "random.seed(SEED) # see https://docs.python.org/3/library/random.html\n",
        "\n",
        "# set numpy seed for reproducibility\n",
        "np.random.seed(SEED)\n",
        "# or better: use a RNG in all your custom functions and classes\n",
        "rng = np.random.default_rng(SEED) \n",
        "\n",
        "# set seed for torch\n",
        "torch.manual_seed(SEED)\n",
        "# make CUDA (i.e., GPU) operations deterministic\n",
        "torch.use_deterministic_algorithms(True) # see https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms\n",
        "\n",
        "# in transformers\n",
        "set_seed(SEED) # does all of the above, see https://github.com/hasansalimkanmaz/transformers/blob/87ac401e776b865e92b1c5f31b0e1f67c76404c4/src/transformers/trainer_utils.py#L69"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**_Important:_** When fine-tuning with `transformers`'s `Trainer` (like above), you also need to set the following arguments in your call to `TrainingArguments()`:\n",
        "\n",
        "```python\n",
        "TrainingArguments(\n",
        "    ...\n",
        "    # ensure reproducibility\n",
        "    full_determinism = True,\n",
        "    seed = SEED,\n",
        "    data_seed = SEED,\n",
        "    ...\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using your own datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the example above, we downloaded our labeled data and directly converted it into a `DatasetDict`(a dictionary of `Dataset` instaces).\n",
        "\n",
        "But in your research, you'll have your own labeled data.\n",
        "So you'll need to create `Dataset` instances from python objects.\n",
        "\n",
        "Below, I show how to do this for lists of dictionaries and pandas data frames, respectively.\n",
        "More options are shown [here](https://huggingface.co/docs/datasets/loading#inmemory-data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "labeled_data = [\n",
        "    {'text': 'I am happy', 'label': 1},\n",
        "    {'text': 'I am sad', 'label': 0}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "{'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}\n",
            "{'text': 'I am happy', 'label': 1}\n",
            "['I am happy', 'I am sad']\n",
            "[1, 0]\n"
          ]
        }
      ],
      "source": [
        "# from list (of dictionaries)\n",
        "dataset = Dataset.from_list(labeled_data)\n",
        "print(type(dataset))\n",
        "print(dataset.features)\n",
        "print(dataset[0])\n",
        "print(dataset['text'])\n",
        "print(dataset['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "{'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}\n",
            "{'text': 'I am happy', 'label': 1}\n",
            "['I am happy', 'I am sad']\n",
            "[1, 0]\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(labeled_data)\n",
        "\n",
        "# from pandas data frame\n",
        "dataset = Dataset.from_pandas(df)\n",
        "print(type(dataset))\n",
        "print(dataset.features)\n",
        "print(dataset[0])\n",
        "print(dataset['text'])\n",
        "print(dataset['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To split your dataset into train, dev, and test sets, you should\n",
        "\n",
        "- rely on scikit-learns pre-defined functions,\n",
        "- and always set the seed\n",
        "\n",
        "This helps to avoid that you accidentally have the same examples in different sets, and that your data splitting is reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example data (see https://chat.openai.com/share/2d5af33c-acb9-4b0f-a260-7c38651fa7b0)\n",
        "labeled_data = [\n",
        "    {\"text\": \"I'm over the moon with joy!\", \"label\": \"happy\"},\n",
        "    {\"text\": \"Happiness radiates from every fiber of my being.\", \"label\": \"happy\"},\n",
        "    {\"text\": \"I can't stop smiling because life is beautiful.\", \"label\": \"happy\"},\n",
        "    {\"text\": \"Tears stream down my face; I'm so heartbroken.\", \"label\": \"sad\"},\n",
        "    {\"text\": \"I feel so lonely and despondent right now.\", \"label\": \"sad\"},\n",
        "    {\"text\": \"It's a gloomy day, and my spirits are low.\", \"label\": \"sad\"},\n",
        "    {\"text\": \"Laughter fills the air, and my heart is light.\", \"label\": \"happy\"},\n",
        "    {\"text\": \"I'm ecstatic about the news! Pure bliss!\", \"label\": \"happy\"},\n",
        "    {\"text\": \"The weight of sadness bears down on me like a ton of bricks.\", \"label\": \"sad\"},\n",
        "    {\"text\": \"Every moment without them feels like an eternity of sorrow.\", \"label\": \"sad\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We usually specify the set sizes in percentages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_size = 0.20\n",
        "dev_size = 0.20\n",
        "\n",
        "# compute N\n",
        "import math\n",
        "n = len(labeled_data)\n",
        "n_test = math.ceil(n*test_size)\n",
        "n_dev = math.ceil(n*dev_size)\n",
        "n_train = n-n_test-n_dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Simple random splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The simplest splitting strategy is to assign examples randomly to the three sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6 2 2\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# use train_test_split from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "SEED = 1234\n",
        "\n",
        "tmp, test_idxs = train_test_split(range(n), test_size=n_test, random_state=SEED)\n",
        "train_idxs, dev_idxs = train_test_split(tmp, test_size=n_dev, random_state=SEED)\n",
        "del tmp\n",
        "\n",
        "print(len(train_idxs), len(dev_idxs), len(test_idxs)) # should be approx 60%, 20%, 20%\n",
        "\n",
        "print(len(set(train_idxs).intersection(set(dev_idxs)))) # should be 0\n",
        "print(len(set(train_idxs).intersection(set(test_idxs)))) # should be 0\n",
        "print(len(set(dev_idxs).intersection(set(test_idxs)))) # should be 0\n",
        "\n",
        "train_data = [labeled_data[i] for i in train_idxs]\n",
        "dev_data = [labeled_data[i] for i in dev_idxs]\n",
        "test_data = [labeled_data[i] for i in test_idxs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With random sampling-based splitting, you might end up with different label proportions, though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.16666666666666666\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "print(np.mean([d['label'] == 'happy' for d in train_data]))\n",
        "print(np.mean([d['label'] == 'happy' for d in dev_data]))\n",
        "print(np.mean([d['label'] == 'happy' for d in test_data]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**_Note:_** In a larger dataset, the differences wouldn't be as dramatic though. So the problem is not too bad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stratify by label class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to ensure that the label proportions in your train, dev, and test splits are identical,\n",
        "you want to stratify by examples' true labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6 2 2\n",
            "0\n",
            "0\n",
            "0\n",
            "0.5\n",
            "0.5\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "# use train_test_split from sklearn\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "SEED = 1234\n",
        "\n",
        "labels = np.array([d['label'] for d in labeled_data])\n",
        "\n",
        "tmp, test_idxs = train_test_split(range(n), test_size=n_test, random_state=SEED, stratify=labels)\n",
        "train_idxs, dev_idxs = train_test_split(tmp, test_size=n_dev, random_state=SEED, stratify=labels[tmp])\n",
        "del tmp\n",
        "\n",
        "print(len(train_idxs), len(dev_idxs), len(test_idxs)) # should be approx 60%, 20%, 20%\n",
        "\n",
        "print(len(set(train_idxs).intersection(set(dev_idxs)))) # should be 0\n",
        "print(len(set(train_idxs).intersection(set(test_idxs)))) # should be 0\n",
        "print(len(set(dev_idxs).intersection(set(test_idxs)))) # should be 0\n",
        "\n",
        "train_data = [labeled_data[i] for i in train_idxs]\n",
        "dev_data = [labeled_data[i] for i in dev_idxs]\n",
        "test_data = [labeled_data[i] for i in test_idxs]\n",
        "\n",
        "# the label proportions are now (approx.) the same in all splits\n",
        "print(np.mean([d['label'] == 'happy' for d in train_data]))\n",
        "print(np.mean([d['label'] == 'happy' for d in dev_data]))\n",
        "print(np.mean([d['label'] == 'happy' for d in test_data]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**_Note:_** \n",
        "You could also stratify by other indicators, such as document authors' IDs.\n",
        "In this case, you'd have similar proportions of author's documents in the different splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Grouped sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes you want to develop a classifier that is able to predict text from held-out documents.\n",
        "For example, if you have collected annotations for sentences sampled from parties' elections manifestos, you might not have sampled sentences from some manifestos.\n",
        "In this case, at prediction time (i.e., when applying your final model to the entire corpus of election manifestos), you'd need to make \"out-of-document\" classifications.\n",
        "\n",
        "\n",
        "Achieving good \"out-of-document\" classification performance requires **generalization**.\n",
        "But it can be difficult because language use within documents tends to be more similar than across documents.\n",
        "\n",
        "What you want to do to asses the ability of your classifier to predict reliably \"out-of-document\" is to mirror this setup in your splitting strategy.\n",
        "This is done by assigning sentences to the train, dev, and test, splits based on their document membership.\n",
        "\n",
        "So for example, below we have a simple illustration of how we assign all sentences from document 1 to the test set, and all sentences from documents 2 and 3 to the train set: \n",
        "\n",
        "| sentence ID | doc ID | set |\n",
        "|:----------- |:------ |:--- |\n",
        "| 1 | 1 | &rarr; 'test' |\n",
        "| 2 | 1 | &rarr; 'test' |\n",
        "| 1 | 2 | &rarr; 'train' |\n",
        "| 2 | 2 | &rarr; 'train' |\n",
        "| 3 | 2 | &rarr; 'train' |\n",
        "| 1 | 3 | &rarr; 'train' |\n",
        "| 2 | 3 | &rarr; 'train' |\n",
        "| 1 | 3 | &rarr; 'train' |\n",
        "| ... | ... | ... |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "labeled_data = [\n",
        "    {\"doc_id\": 0, \"text\": \"I'm over the moon with joy!\", \"label\": \"happy\"},\n",
        "    {\"doc_id\": 0,\"text\": \"Happiness radiates from every fiber of my being.\", \"label\": \"happy\"},\n",
        "    {\"doc_id\": 1,\"text\": \"I can't stop smiling because life is beautiful.\", \"label\": \"happy\"},\n",
        "    {\"doc_id\": 1,\"text\": \"Tears stream down my face; I'm so heartbroken.\", \"label\": \"sad\"},\n",
        "    {\"doc_id\": 2,\"text\": \"I feel so lonely and despondent right now.\", \"label\": \"sad\"},\n",
        "    {\"doc_id\": 2,\"text\": \"It's a gloomy day, and my spirits are low.\", \"label\": \"sad\"},\n",
        "    {\"doc_id\": 3,\"text\": \"Laughter fills the air, and my heart is light.\", \"label\": \"happy\"},\n",
        "    {\"doc_id\": 3,\"text\": \"I'm ecstatic about the news! Pure bliss!\", \"label\": \"happy\"},\n",
        "    {\"doc_id\": 4,\"text\": \"The weight of sadness bears down on me like a ton of bricks.\", \"label\": \"sad\"},\n",
        "    {\"doc_id\": 4,\"text\": \"Every moment without them feels like an eternity of sorrow.\", \"label\": \"sad\"}\n",
        "]\n",
        "# note: in reality, the number of sentences per document might vary. \n",
        "#       But this is not a problem for running the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6 2 2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# use GroupSplit strategy from sklearn\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "doc_ids = np.array([d[\"doc_id\"] for d in labeled_data])\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=2, test_size=test_size, random_state=SEED)\n",
        "tmp, test_idxs = next(gss.split(range(n), groups=doc_ids))\n",
        "train_idxs, dev_idxs = next(gss.split(range(len(tmp)), groups=doc_ids[tmp]))\n",
        "del tmp\n",
        "\n",
        "print(len(train_idxs), len(dev_idxs), len(test_idxs)) # should be approx 60%, 20%, 20%\n",
        "\n",
        "print(len(set(train_idxs).intersection(set(dev_idxs)))) # should be 0\n",
        "print(len(set(train_idxs).intersection(set(test_idxs)))) # should be 0\n",
        "print(len(set(dev_idxs).intersection(set(test_idxs)))) # should be 0\n",
        "\n",
        "train_data = [labeled_data[i] for i in train_idxs]\n",
        "dev_data = [labeled_data[i] for i in dev_idxs]\n",
        "test_data = [labeled_data[i] for i in test_idxs]\n",
        "\n",
        "train_doc_ids = [d[\"doc_id\"] for d in train_data]\n",
        "dev_doc_ids = [d[\"doc_id\"] for d in dev_data]\n",
        "test_doc_ids = [d[\"doc_id\"] for d in test_data]\n",
        "\n",
        "# the label proportions are now (approx.) the same in all splits\n",
        "print(len(set(train_doc_ids).intersection(set(dev_doc_ids)))) # should be 0\n",
        "print(len(set(train_doc_ids).intersection(set(test_doc_ids)))) # should be 0\n",
        "print(len(set(dev_doc_ids).intersection(set(test_doc_ids)))) # should be 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-class classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In many use cases, you have more than two label classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model setup\n",
        "\n",
        "`transformers`' `AutoModelForSequenceClassification` can handle this well.\n",
        "You just need to \n",
        "\n",
        "1. adapt your `id2label` and `label2id` dictionaries accordingly. So if you have three label classes \"positive\", \"neutral\", and \"negative,\" \n",
        "\n",
        "```python\n",
        "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "```\n",
        "\n",
        "2. adpat the `num_labels` argument accordingly:\n",
        "\n",
        "```python\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation metrics\n",
        "\n",
        "If you have more than two classes, you will need to adapt your evaluation metrics.\n",
        "This is because the *precision* and *recall* metrics, for example, only differentiate between correct and false classifications (\"hits\" and \"misses\" ) of \"positive\" vs. \"negative\" examples.\n",
        "So for each label class, you will convert (\"dichotomize\") your multi-class labels and predicted classifications into so-called \"one vs. rest\" indicators.\n",
        "\n",
        "**_Example:_** If you are interested in the performance of your model to correctly classify \"neutral\" samples and the other two label classes are \"positive\" and \"negative\", you will redifine the label categories as follows\n",
        "\n",
        "- \"positive\" &rarr; 0\n",
        "- \"neutral\" &rarr; 1\n",
        "- \"negative\" &rarr; 0\n",
        "\n",
        "In this way, you can compute a **_\"neutral\"-specific_ recall, precision, and F1 scores**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation functions in the `sklearn.metrics` module like `f1_score()` support multi-class classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.5       , 0.4       , 0.66666667])"
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "true_labels = [0, 0, 1, 1, 2, 2]\n",
        "pred_labels = [0, 1, 0, 1, 2, 1]\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# get one F1 score per label class 0, 1, and 2 (in ascending order)\n",
        "f1_score(y_true=true_labels, y_pred=pred_labels, average=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Put for finding the \"best\" model, you'll still need a single performance score.\n",
        "\n",
        "So what we do is **average** class-specific scores into one performacne estimate.\n",
        "The most common averaging strategy is the so-called **_macro_ average**.\n",
        "It just computes the average between class-specific scores.\n",
        "\n",
        "So given the example above, the \"macro F1 score\" is `(0.5+0.4+0.666)/3 = 0.5222`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5222222222222223"
            ]
          },
          "execution_count": 252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(y_true=true_labels, y_pred=pred_labels, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The alternative strategy is the **_micro_ average**.\n",
        "In this strategy, we just summarize the which labels we got right, and which we got wrong. So the \"micro F1 scores\" is just the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 262,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(y_true=true_labels, y_pred=pred_labels, average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is also a function that summarizes everything:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      0.50      0.50         2\n",
            "     neutral       0.33      0.50      0.40         2\n",
            "    positive       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.61      0.50      0.52         6\n",
            "weighted avg       0.61      0.50      0.52         6\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "rep = classification_report(\n",
        "    y_true=true_labels, \n",
        "    y_pred=pred_labels, \n",
        "    labels=list(id2label.keys()),\n",
        "    target_names=list(id2label.values()),\n",
        ")\n",
        "print(rep)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "advanced_text_analysis_gesis_2023",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
