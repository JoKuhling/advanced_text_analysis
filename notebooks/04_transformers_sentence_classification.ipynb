{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune a transformer model for sentence classification\n",
        "\n",
        "| Authors | Last update |\n",
        "|:------ |:----------- |\n",
        "| Hauke Licht (https://github.com/haukelicht) | 2023-09-28 |\n",
        "\n",
        "This notebook shows how to use the hugging face ü§ó `transformers` library to train a Transformer-based sentence classifier with transfer learning (i.e., \"fine tuning\").\n",
        "\n",
        "**_Source:_** The notebook is adapted from the one distributed with this tutorial: https://huggingface.co/docs/transformers/tasks/sequence_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you run this notebook on Google Colab or you have not yet installed the `transformers` and `datasets` python libraries, you need to do so first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t6lIsnGIwC5"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# dataset loading\n",
        "from datasets import load_dataset, DatasetDict \n",
        "\n",
        "# used to tokenize text\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# used to load the pre-trained model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# used to finetune the pre-trained model\n",
        "from transformers import Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge7uLpvVIwC6"
      },
      "source": [
        "## Supervised text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJDLJIWIIwC8"
      },
      "source": [
        "Text classification means to assigns a label or class to each text in a corpus.\n",
        "It is a common NLP and computational text analysis task.\n",
        "\n",
        "A common and popular text classification task is **_sentiment analysis_**.\n",
        "Sentiment analysis assigns a label like üôÇ 'positive', üôÅ 'negative', or üòê 'neutral' to a sequence of text, for example a sentence of paragraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ingredients\n",
        "\n",
        "Here is what you need for training a supervised sequence classifier through finetuning (i.e. transfer learning):\n",
        "\n",
        "- a pre-defined set of **label classes** (e.g., 'positive', 'neutral', 'negative')\n",
        "- a **label dataset**, i.e., a corpus of texts (e.g., sentences) in which each document has been assigned to a single label label class\n",
        "- a **pre-trained model** you can fine-tune for sequence classification\n",
        "- some **metric to quantify classification performance** so that we know how well our classifier is doing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This notebook\n",
        "\n",
        "In this notebook, we will use\n",
        "\n",
        "1. the [IMDb](https://huggingface.co/datasets/imdb) corpus that records movie review that have been classified as positive or negative, and\n",
        "2. finetune a [DistilBERT](https://huggingface.co/distilbert-base-uncased) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejVMVM5hIwC9"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el12ezRDIwC9"
      },
      "source": [
        "We'll use the IMDb dataset from the ü§ó `datasets` library.\n",
        "However, the train and test splits ([?](https://chat.openai.com/share/d71207ff-d374-4540-927a-83ac5370cd8f)) of this dataset each contain 25,000 documents.\n",
        "Using them all will result in very slow training and we'll thus just use subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3000, 1000, 1000)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_train = 3000\n",
        "n_dev = 1000\n",
        "n_test = 1000\n",
        "\n",
        "# sample without replacement\n",
        "idxs = np.random.choice(25_000, n_train + n_dev + n_test, replace=False)\n",
        "\n",
        "# split the indices into train, dev, and test\n",
        "train_idxs = idxs[:n_train]\n",
        "dev_idxs = idxs[n_train:(n_train+n_dev)]\n",
        "test_idxs = idxs[-n_test:]\n",
        "\n",
        "# show the number of examples in each split\n",
        "len(train_idxs), len(dev_idxs), len(test_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Ec1YkX5OIwC9"
      },
      "outputs": [],
      "source": [
        "imdb = DatasetDict({\n",
        "    \"train\": load_dataset(\"imdb\", split='train').select(train_idxs),\n",
        "    \"dev\": load_dataset(\"imdb\", split='test').select(dev_idxs),\n",
        "    \"test\": load_dataset(\"imdb\", split='test').select(test_idxs),\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `imdb` is an instance of the `datasets` `DatasetDict` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.dataset_dict.DatasetDict"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(imdb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This class is there to gather several pre-defined splits of a dataset.\n",
        "\n",
        "Among these splits, one is usually named \"train\" and another on \"test\" (see next cell).\n",
        "\n",
        "**_Note:_** It'll become clearer further below why we need these splits. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'dev', 'test'])"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3000, 1000, 1000)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(imdb['train']), len(imdb['dev']), len(imdb['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-d-KOvzIwC9"
      },
      "source": [
        "Here is how you can access one \"example\" (i.e., observation) in the the \"test\" split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "kQEm_8XyIwC9",
        "outputId": "0219cd92-b7a1-4550-b6ab-408b29f400ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': \"It's hard to believe that with a cast as strong as this one has, that this movie can be such a dud. It's such an incredibly horrible film. How was it ever made? How did so many good actors wind up in such a terrible film? Don't waste your life. Don't watch even one moment of this film.\",\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZBe68pIwC-"
      },
      "source": [
        "This shows in the test split, there are two fields for *each* example :\n",
        "\n",
        "- `text`: the movie review text.\n",
        "- `label`: a value that is either `0` for a negative review or `1` for a positive review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**_Important:_** check that the splits have about equal label class distributions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% \"pos\" in train: 0.5076666666666667\n",
            "% \"pos\" in dev: 0.499\n",
            "% \"pos\" in test: 0.498\n"
          ]
        }
      ],
      "source": [
        "print('% \"pos\" in train:', np.mean([ex['label'] for ex in imdb[\"train\"]]))\n",
        "print('% \"pos\" in dev:', np.mean([ex['label'] for ex in imdb[\"dev\"]]))\n",
        "print('% \"pos\" in test:', np.mean([ex['label'] for ex in imdb[\"test\"]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vw5OFMUIwC-"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLhZbEdaIwC-"
      },
      "source": [
        "The next step is to load a DistilBERT tokenizer to preprocess the `text` field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "FXLRVYJ6IwC-"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tokenizer is a so-called *callable* and can thus be used like a function:\n",
        "If you input a text string, it return a dictionary with the tokenized text and additional information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        }
      ],
      "source": [
        "toks = tokenizer(\"Hello, this one sentence!\")\n",
        "print(toks.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The field 'input_ids' indicates the numbers used to represent the tokens in the example sentence.\n",
        "- The 'attention_mask' is there to help the model to know to which tokens in of a bunch of sentences it should pay attention when fine-tuning, and which it can ignore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdrDTeZYIwC-"
      },
      "source": [
        "Let's create a helper function that tokenizes the `text` value of an input called example.\n",
        "This will allow us to iterate over examples in our dataset splits (e.g., `imdb[\"test\"]`) and pre-process them one by one.\n",
        "\n",
        "**_Note:_** Setting `truncate=True` we ensure that none of the text sequences we'll use for fine-tuning is too longer for DistilBERT to handle it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "veclR-QmIwC-"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA74NYovIwC-"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ü§ó Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "kbNKLzNbIwC-"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ca2c83848c24ef6a07dd7d8a125daa7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "535a0f33ec464c6fa8fcdfd6e1f46ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0440afd2c6646d79437950a44780e88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsVftz9cIwC-"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). \n",
        "\n",
        "Our `data_collator` instance of this class will handle preprocessing and a thing called \"padding\" when sampling batches of examples during finetuning to iteratively update our classifier's parameters.\n",
        "\n",
        "*Padding* means that you make all text sequences in a set of sequences the same length.\n",
        "To do this, we just append the `<PAD>` special token to shorter text sequences in the set.\n",
        "For example, the (tokenized) sequences in the following set \n",
        "\n",
        "```json\n",
        "[\n",
        "    ['Hello', 'world', '!'               ],\n",
        "    ['Have',  'a',     'nice', 'day', '!']\n",
        "]\n",
        "```\n",
        "\n",
        "will be \"padded\" to \n",
        "\n",
        "```json\n",
        "[\n",
        "    ['Hello', 'world', '!',    '<PAD>', '<PAD>'],\n",
        "    ['Have',  'a',     'nice', 'day',   '!'    ]\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "nOP2H-qPIwC-"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G61kFtuQIwC-"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZfQHOjuIwC-"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance.\n",
        "\n",
        "**_Note_** You could also just load a evaluation method with the ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) library (see the ü§ó Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib-IdxbhIwC-"
      },
      "source": [
        "Let's create a function that passes your predictions and labels to calculate some central metrics (explanations below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "w0FFpYUJIwC-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true=labels, y_pred=predictions, average='macro', zero_division=0)\n",
        "    # ba = balanced_accuracy_score(y_true=labels, y_pred=predictions)\n",
        "    metrics = {\n",
        "        \"macro_f1\": f1,\n",
        "        \"macro_precision\": p,\n",
        "        \"macro_recall\": r,\n",
        "        # \"balanced_accuracy\": ba\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compute the following metrics:\n",
        "\n",
        "- precision: the share of examples a classifier as correctly assigned into a class\n",
        "- recall: the share of positive examples a classifier labels correctly\n",
        "- F1: a measure combining recall and precision\n",
        "- balanced accuary: an accurarcy metric adjusting for class imbalance\n",
        "\n",
        "<p><a href=\"https://commons.wikimedia.org/wiki/File:Precisionrecall.svg#/media/File:Precisionrecall.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precisionrecall.svg\" height=\"800\" width=\"440\"></a><br>By &lt;a href=\"//commons.wikimedia.org/wiki/User:Walber\" title=\"User:Walber\"&gt;Walber&lt;/a&gt; - &lt;span class=\"int-own-work\" lang=\"en\"&gt;Own work&lt;/span&gt;, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=36926283\">Link</a></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfqRQ3_8IwC_"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ryvTj2IwC_"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODl7cT-SIwC_"
      },
      "source": [
        "Before you start training your model, create two dictionaries that map labels' numeric IDS their character values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d9UyZaDvIwC_"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "douQqh3YIwC_"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EWRzhpclIwC_"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbc595a4347c443d97ecf5898c968fd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", # <== the name of the pre-trained model (downloaded from huggingface hub)\n",
        "    num_labels=2, # number of label classes\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make training as fast as possible, you want to utilize GPU computing.\n",
        "When you run notebooks on Colab, you can enable GPU computing by \n",
        "\n",
        "1. clicking on \"Runtime\" in the menu,\n",
        "2. selecting \"Change runtime type\", and\n",
        "3. choose \"GPU\" in the \"Hardware accelerator\" section of the pop-up\n",
        "\n",
        "If you are running this notebook elsewhere, you want to determine to what kind of device you have access\n",
        "\n",
        "- with a GPU &rarr; \"cuda\"\n",
        "- with MacOS's M1/M2 chip &rarr; \"mps\"\n",
        "- else \"cpu\"\n",
        "\n",
        "We do so like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='mps')"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "# check if GPU or MPS is available, else use CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "device = torch.device(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we've figured this out, we put our model on that device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g_U2wEeIwC_"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
        "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "kStUdSXmIwC_"
      },
      "outputs": [],
      "source": [
        "# define the path where you want to save the fine tuned model\n",
        "model_path = os.path.join('..', 'data', 'models', 'distillbert_ibmd_sentiment')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    # leave the following unchanged ;)\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1, # <== increase this value to train for longer\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    # metric_for_best_model=\"macro_f1\", # <== needs to match one of the names of the dictionary returned by `compute_metrics()` function\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model, # the model instance you loaded two cells above\n",
        "    args=training_args, # the training args you created one cells above\n",
        "    train_dataset=tokenized_imdb[\"train\"], # the training data split\n",
        "    eval_dataset=tokenized_imdb[\"dev\"], # the testing data split\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can finetune the model!\n",
        "\n",
        "**_Warning:_** This will take long if you are using only your CPU ü•π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hlicht/miniforge3/envs/advanced_text_analysis_gesis_2023/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76fefe76b7164280ac954d71460d29b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/188 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4bbec8ce86a4685a691bfe797016a1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3237026631832123, 'eval_macro_f1': 0.8649987849890648, 'eval_macro_precision': 0.8650313620071685, 'eval_macro_recall': 0.8650094600378402, 'eval_runtime': 23.2899, 'eval_samples_per_second': 42.937, 'eval_steps_per_second': 2.705, 'epoch': 1.0}\n",
            "{'train_runtime': 252.5856, 'train_samples_per_second': 11.877, 'train_steps_per_second': 0.744, 'train_loss': 0.6630656059752119, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=188, training_loss=0.6630656059752119, metrics={'train_runtime': 252.5856, 'train_samples_per_second': 11.877, 'train_steps_per_second': 0.744, 'train_loss': 0.6630656059752119, 'epoch': 1.0})"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b16b4baedaae40d980cd2da6ce289268",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.3573879599571228,\n",
              " 'eval_macro_f1': 0.8508566732630058,\n",
              " 'eval_macro_precision': 0.8526682227784228,\n",
              " 'eval_macro_recall': 0.8511376182018913,\n",
              " 'eval_runtime': 23.3862,\n",
              " 'eval_samples_per_second': 42.76,\n",
              " 'eval_steps_per_second': 2.694,\n",
              " 'epoch': 1.0}"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate the final model on the held-out tetst set\n",
        "+trainer.evaluate(tokenized_imdb[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation**\n",
        "\n",
        "- the precision of 0.85 tells us that the classifier is correct about 17 out of 20 times when it says a text has \"positive\" sentiment (our positive label class)\n",
        "- the recall of 0.85 tells us that the classifier correctly classifies about 17 in every 20 \"true\" positive-sentiment examples\n",
        "- the F1 score just summarizes thes values in one score\n",
        "\n",
        "Overall our classifier performs pretty well even with only 3000 traning examples. ü•≥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the model and tokenizer for re-use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./../models/distillbert_ibmd_sentiment/tokenizer_config.json',\n",
              " './../models/distillbert_ibmd_sentiment/special_tokens_map.json',\n",
              " './../models/distillbert_ibmd_sentiment/vocab.txt',\n",
              " './../models/distillbert_ibmd_sentiment/added_tokens.json',\n",
              " './../models/distillbert_ibmd_sentiment/tokenizer.json')"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean-up all other checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tokenizer_config.json',\n",
              " 'special_tokens_map.json',\n",
              " 'config.json',\n",
              " 'tokenizer.json',\n",
              " 'training_args.bin',\n",
              " 'checkpoint-79',\n",
              " 'vocab.txt',\n",
              " 'pytorch_model.bin',\n",
              " 'checkpoint-188']"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.listdir(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['checkpoint-79', 'checkpoint-188']"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoints = [fn for fn in os.listdir(model_path) if fn.startswith('checkpoint-')]\n",
        "checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# remove the cehckpoint folders\n",
        "for checkpoint in checkpoints:\n",
        "    dir_path = os.path.join(model_path, checkpoint)\n",
        "    shutil.rmtree(dir_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed look at the classifiers output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create predictions for the first three examples in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82d7aa943d594b4babf6c6b3a0d643b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "preds = trainer.predict(tokenized_imdb[\"test\"].select([0, 1, 2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "transformers.trainer_utils.PredictionOutput"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(preds.predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds.predictions.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prediction array has two dimensions:\n",
        "\n",
        "- the first axis ('rows') corresponds to the *number of examples* for which we generated predictions\n",
        "- the second axis ('columns') corresponds to the *number of label classes* we generate probability-like scores for when predicting\n",
        "\n",
        "Let's look at the scores for the first example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1.7259933948516846, -1.9657995700836182]"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds.predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first score is larger than the second one.\n",
        "This means that given example is more similar to examples from the first label class: documents with negative sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NEGATIVE'"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2label[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To convert those scores in something probability-like, we apply the so-called [softmax transformation](), which rescales values such that they each range between 0 and 1 and sum to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.975679  , 0.02432101], dtype=float32)"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "softmax(preds.predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also call this function on all examples' prediction scores in our current batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.975679  , 0.02432101],\n",
              "       [0.97882915, 0.02117081],\n",
              "       [0.05613839, 0.9438616 ]], dtype=float32)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_probs = softmax(preds.predictions, axis=1)\n",
        "pred_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now if you want to know for each row in which cell the value is the largest, you can call the `argmax()` method on the numpy array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_probs.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This turns prediction scores into predicted labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['NEGATIVE', 'NEGATIVE', 'POSITIVE']"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[id2label[pp] for pp in preds.predictions.argmax(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cmBaAB-IwC_"
      },
      "source": [
        "## Using the model for labeling texts "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOM0j0yZIwC_"
      },
      "source": [
        "When you have saved your finetuned model, you can always re-load it to label texts.\n",
        "In machine learning this is called \"inference\" &mdash; which is unfortunate given the meaning of the term in positive social science methodology.\n",
        "\n",
        "So let's just call it **prediction**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "2o_TyvYXIwC_"
      },
      "outputs": [],
      "source": [
        "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZBBmqrvIwDC"
      },
      "source": [
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for sentiment analysis with your model, and pass your text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "wF0rTEXuIwDC",
        "outputId": "c3986ab0-b2d6-42c1-8c00-8745c628e5ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9153976440429688}]"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# set environment variable TOKENIZERS_PARALLELISM=false\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_path)\n",
        "classifier(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0y4lRm7IwDD"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like:\n",
        "\n",
        "Tokenize the text and return PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo3PR9lEIwDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq65AznnIwDD"
      },
      "source": [
        "Pass your inputs to the model and return the `logits`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "njKLoOP-IwDD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'POSITIVE'"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id] # <== use the 'id2label' we've added to the model we saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DistillBERT is not the only pre-trained model you can fine tune for sequence classification.\n",
        "The huggingface `transformers` library supports it also for the following many models, for example\n",
        "\n",
        "- [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert)\n",
        "- [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert)\n",
        "- [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2)\n",
        "- [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert)\n",
        "- [OpenAI GPT-2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt2)\n",
        "- [LLaMA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/llama)\n",
        "- [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer)\n",
        "- [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart)\n",
        "- [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta)\n",
        "- [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "advanced_text_analysis_gesis_2023",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
