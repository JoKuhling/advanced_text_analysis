{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a word embedding model from scratch\n",
    "\n",
    "| Author | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2023-09-26 |\n",
    "\n",
    "This notebook illustrates how to use `gensim` to train a word2vec model from scratch on original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this code if 'swifter' package is not installed\n",
    "# !pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter # <== `pip install swifter` (if not yet done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..', 'data', 'corpora', 'gbr_commons')\n",
    "os.makedirs(data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have prepared a corpus of sentence-splitted speeches from the UK *House of Commons* (lower house chamber of the parliament).\n",
    "The file is too big to be uploaded on Github.\n",
    "So you need to download it if you have not yet done so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(data_path, 'gbr_commons_speech_sentences_tokenized.tsv.gzip')\n",
    "if not os.path.exists(fp):\n",
    "    print('downloading the corpus ... might take 3-10 minutes (depending on your internet connection)')\n",
    "    url = 'https://www.dropbox.com/scl/fi/wkxj7k2uiy0935dmbjp34/gbr_commons_speech_sentences_tokenized.tsv.gzip?rlkey=urjdpz0vgbymllzugzsh0m2ld&dl=1'\n",
    "    corp = pd.read_csv(url, sep='\\t', compression='gzip')\n",
    "    corp.to_csv(fp, sep='\\t', compression='gzip', index=False)\n",
    "    # keep only the first 100K sentences in the corpus\n",
    "    corp = corp.iloc[:100_000]\n",
    "else:\n",
    "    # load only the first 100K sentences in the corpus\n",
    "    corp = pd.read_csv(fp, sep='\\t', compression='gzip', nrows=100_000)\n",
    "\n",
    "corp = corp[~corp.text_tokenized.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello world, I'm <NUM> years old\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.sub('\\d+[.,]\\d', '<NUM>', 'Hello world, I\\'m 33 years old')\n",
    "# note: maybe also add '<OOV>' = out of vocabulary /but then set `min_count=1`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uk.org.publicwhip/debate/1970-01-19a.1.5_0_0</td>\n",
       "      <td>On a point of order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uk.org.publicwhip/debate/1970-01-19a.1.5_0_1</td>\n",
       "      <td>Is it true Mr Speaker that your Private Secret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uk.org.publicwhip/debate/1970-01-19a.1.5_0_2</td>\n",
       "      <td>If that be true then it is a matter of deep re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uk.org.publicwhip/debate/1970-01-19a.1.5_0_3</td>\n",
       "      <td>Will you Mr Speaker be kind enough to convey o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uk.org.publicwhip/debate/1970-01-19a.1.7_0_0</td>\n",
       "      <td>On behalf of hon and right hon Members on this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_id  \\\n",
       "0  uk.org.publicwhip/debate/1970-01-19a.1.5_0_0   \n",
       "1  uk.org.publicwhip/debate/1970-01-19a.1.5_0_1   \n",
       "2  uk.org.publicwhip/debate/1970-01-19a.1.5_0_2   \n",
       "3  uk.org.publicwhip/debate/1970-01-19a.1.5_0_3   \n",
       "4  uk.org.publicwhip/debate/1970-01-19a.1.7_0_0   \n",
       "\n",
       "                                      text_tokenized  \n",
       "0                                On a point of order  \n",
       "1  Is it true Mr Speaker that your Private Secret...  \n",
       "2  If that be true then it is a matter of deep re...  \n",
       "3  Will you Mr Speaker be kind enough to convey o...  \n",
       "4  On behalf of hon and right hon Members on this...  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file maps sentences to sentence IDs.\n",
    "The sentences have been preprocessed and tokenized into words and then concatenated with white spaces.\n",
    "So to get a sentences words, we can split at the white space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is it true Mr Speaker that your Private Secretary Sir Francis Reid died suddenly over the weekend'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.text_tokenized[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train (illustration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a word2vec model on a corpus, we need to\n",
    "\n",
    "1. create a new model instance `model = gensim.models.Word2Vec(...)`. Here we pass all model *hyper-parameters* like the embedding dimension and window size, the algorithm we want to use (Skip-gram or CBOW), etc.\n",
    "2. build the model's vocabulary on our corpus `model.build_vocab(...)`\n",
    "3. train the model by calling `model.train(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a corpus (in memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you need as **input** for these steps is a list of sentences split into tokens (\"words\").\n",
    "That is, you input data should be prepared as follows:\n",
    "\n",
    "```python\n",
    "[\n",
    "    ['Words', 'in', 'sentence', 'one'],\n",
    "    ['Some', 'more', 'words', 'in', 'sentence', 'two'],\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split all our German *Bundestag* speech sentences into words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b266310a0e4149891ce29749db35bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/99997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split pre-tokeniezd sentences into words\n",
    "sentences = corp.text_tokenized.swifter.apply(lambda x: x.split(' ')).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['On', 'a', 'point', 'of', 'order'],\n",
       " ['Is',\n",
       "  'it',\n",
       "  'true',\n",
       "  'Mr',\n",
       "  'Speaker',\n",
       "  'that',\n",
       "  'your',\n",
       "  'Private',\n",
       "  'Secretary',\n",
       "  'Sir',\n",
       "  'Francis',\n",
       "  'Reid',\n",
       "  'died',\n",
       "  'suddenly',\n",
       "  'over',\n",
       "  'the',\n",
       "  'weekend'],\n",
       " ['If',\n",
       "  'that',\n",
       "  'be',\n",
       "  'true',\n",
       "  'then',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'matter',\n",
       "  'of',\n",
       "  'deep',\n",
       "  'regret',\n",
       "  'to',\n",
       "  'all',\n",
       "  'Members',\n",
       "  'on',\n",
       "  'both',\n",
       "  'sides',\n",
       "  'of',\n",
       "  'the',\n",
       "  'House',\n",
       "  'who',\n",
       "  'shared',\n",
       "  'deep',\n",
       "  'friendships',\n",
       "  'with',\n",
       "  'Sir',\n",
       "  'Francis'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'Mr',\n",
       "  'Speaker',\n",
       "  'be',\n",
       "  'kind',\n",
       "  'enough',\n",
       "  'to',\n",
       "  'convey',\n",
       "  'our',\n",
       "  'deep',\n",
       "  'sympathy',\n",
       "  'to',\n",
       "  'the',\n",
       "  'members',\n",
       "  'of',\n",
       "  'his',\n",
       "  'family']]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 4 sentences\n",
    "sentences[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a `Word2Vec` model instance.\n",
    "The following model hyper-parameters are relevant for our purposes:\n",
    "\n",
    "- `vector_size`: the number of dimensions $d$ of th word embedding matrix\n",
    "- `window`: the number of context words to use for modeling \n",
    "- `min_count`: the minimum number of times a word needs to occur in the corpus to be included in the vocabulary\n",
    "- `sg`: set to 1 to use the Skip-Gram algorithm, otherwise uses CBOW algorithm\n",
    "- `hs`: set to 1 to use the hierarchical softmax\n",
    "- `negative`: number of negative samples to use per focus word when computing the loss\n",
    "- `epochs`: number of times to iterate over corpus (all sentences)\n",
    "- `workers`: number of CPU cores to use for training parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# create a new model instance\n",
    "model = gensim.models.Word2Vec(\n",
    "    vector_size=10, # <= super low dimensionality for testing\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    negative=10, # <== if negative = 2*window, data is \"balanced\"\n",
    "    epochs=5, # <= set very low for testing\n",
    "    workers=10 # <= 10 because I have 10 cores on my machine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary from the list of sentences\n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99997"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7100711, 10061760)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on the list of sentences\n",
    "model.train(\n",
    "    sentences, \n",
    "    total_examples=model.corpus_count, \n",
    "    epochs=model.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(gensim.models.word2vec.Word2Vec, gensim.models.keyedvectors.KeyedVectors)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model), type(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute similarities on a number of hand-picked word pairs to see whether the model has learned sensible word vectors.\n",
    "\n",
    "*Note:* This is more of a \"face validity\" check and no full-fledged validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Government\" - \"law\": 0.426\n",
      "\"Government\" - \"bill\": 0.357\n",
      "\"Opposition\" - \"law\": 0.332\n",
      "\"Opposition\" - \"bill\": 0.204\n",
      "\"Opposition\" - \"proposal\": 0.723\n",
      "\"bill\" - \"proposal\": 0.430\n",
      "\"Labour\" - \"Government\": 0.785\n",
      "\"Labour\" - \"Opposition\": 0.770\n",
      "\"Conservatives\" - \"Government\": 0.661\n",
      "\"Conservatives\" - \"Opposition\": 0.742\n",
      "\"shadow\" - \"Minister\": 0.684\n",
      "\"Opposition\" - \"bench\": 0.820\n",
      "\"chair\" - \"bench\": 0.729\n",
      "\"Chair\" - \"Speaker\": 0.954\n",
      "\"hon\" - \"Member\": 0.515\n",
      "\"honourable\" - \"Member\": 0.257\n",
      "\"hon\" - \"Friend\": 0.632\n",
      "\"Friend\" - \"Member\": 0.884\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "pairs = [\n",
    "    ('Sir', 'Madam'),\n",
    "    ('Pound', 'Euro'),\n",
    "    ('Pound', 'Dollar'),\n",
    "    ('Government', 'law'),\n",
    "    ('Government', 'bill'),\n",
    "    ('Government', 'propoasal'),\n",
    "    ('Opposition', 'law'),\n",
    "    ('Opposition', 'bill'),\n",
    "    ('Opposition', 'proposal'),\n",
    "    ('bill', 'proposal'),\n",
    "    ('Labour', 'Government'),\n",
    "    ('Labour', 'Opposition'),\n",
    "    ('Conservatives', 'Government'),\n",
    "    ('Conservatives', 'Opposition'),\n",
    "    ('shadow', 'Minister'),\n",
    "    ('Opposition', 'bench'),\n",
    "    ('chair', 'bench'),\n",
    "    ('Chair', 'Speaker'),\n",
    "    ('hon', 'Member'),\n",
    "    ('honourable', 'Member'),\n",
    "    ('hon', 'Friend'),\n",
    "    ('Friend', 'Member')\n",
    "]\n",
    "for pair in pairs:\n",
    "    if all(p in model.wv.key_to_index for p in pair):\n",
    "        print(f'\"{pair[0]}\" - \"{pair[1]}\": {model.wv.similarity(pair[0], pair[1]):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Minister' in model.wv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with a corpus read from disk\n",
    "\n",
    "The above code works just fine.\n",
    "But it has a computational bottleneck!\n",
    "To iterate over the list of sentences, we need to have them all loaded into our working memory (RAM).\n",
    "If our corpus is very large -- which it should be to enable learning of reliable word embeddings --, this might be too burdensome for your computer.\n",
    "\n",
    "So instead of loading the entire corpus in the RAW, we can iteratively read batches of sentences from a data file that exists somewhere on the hard drive (\"disk\") of your computer.\n",
    "For this, we need an \"iterable\" class that implements \"yields\" sentences in our corpus one at a time.\n",
    "\n",
    "In the next code cell, we define such a class.\n",
    "This class reads the sentences from a (ZIP-ed) CSV file and splits sentences at white spaces before yielding them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# get corpus sentences\n",
    "class SentenceCorpus(object):\n",
    "    \"\"\"Iterable class for reading a corpus from a file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the corpus file.\n",
    "    sep : str, optional\n",
    "        Separator for splitting the lines into tokens. Default: ','\n",
    "    compressed : bool, optional\n",
    "        Whether the file is compressed. Default: False\n",
    "    skip : int, optional\n",
    "        Number of lines to skip at the beginning of the file. Default: 1\n",
    "    nrows : int, optional\n",
    "        Number of lines to read from the file. Default: None (read all lines)\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    list of str\n",
    "\n",
    "    \"\"\"\n",
    "    # note: I've already added a lot of functionality here (e.g. reading from compressed files, skip rows, limit the number of rows to read).\n",
    "    #       Hence, the actual code is a bit more complex than what you'd need in some use cases.\n",
    "    def __init__(self, filepath, sep='\\t', compressed=False, skip=1, nrows=None):\n",
    "        self.filepath = filepath\n",
    "        self.sep = sep\n",
    "        self.compressed = compressed\n",
    "        self.skip = skip if skip else 0\n",
    "        self.nrows = nrows if nrows else np.inf\n",
    "        if skip and skip > 0:\n",
    "            self.nrows += skip\n",
    "        with gzip.open(self.filepath, 'rt', encoding='utf-8') if self.compressed else open(self.filepath, 'r', encoding='utf-8') as file:\n",
    "            corpus_size = len(file.readlines())\n",
    "        if nrows is not None and nrows < corpus_size:\n",
    "            corpus_size = nrows\n",
    "        else:\n",
    "            nrows = corpus_size\n",
    "        self.corpus_size = nrows\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.corpus_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # open file\n",
    "        with gzip.open(self.filepath, 'rt', encoding='utf-8') if self.compressed else open(self.filepath, 'r', encoding='utf-8') as file:\n",
    "            # compute how many rows to read in total\n",
    "            max_ = self.nrows if self.skip <= 0 else self.nrows+self.skip\n",
    "            # for each line in file\n",
    "            for i, line in enumerate(file):\n",
    "                # skip if you want to skipp first `skip` lines\n",
    "                if i <= self.skip:\n",
    "                    continue\n",
    "                # stop if you have read max number of lines\n",
    "                if max_ and i >= max_:\n",
    "                    break\n",
    "                yield line.strip().split(self.sep)[1].split(' ')\n",
    "                # previsouly : \n",
    "                # now: skip if word splitting raises error\n",
    "                try:\n",
    "                    # try to split text into words (assuming that the text is in the second column)\n",
    "                    # note: \n",
    "                    #   - the `self.sep` says what character you want to use for spliting the line into columns\n",
    "                    #   - the `[1]` just says that you'll assume that the text is in the second column\n",
    "                    words = line.strip().split(self.sep)[1].split(' ')\n",
    "                except:\n",
    "                    continue\n",
    "                yield words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the some 100,000 sentences from our corpus as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(data_path, 'gbr_commons_speech_sentences_tokenized.tsv.gzip')\n",
    "sentences = SentenceCorpus(filepath=fp, compressed=True, nrows=100_000)\n",
    "# note: loading it takes time because it's an iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    vector_size=10, # <= super low dimensionality for testing\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    negative=5,\n",
    "    epochs=5, # <= set very low for testing\n",
    "    workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7101496, 10061800)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(\n",
    "    sentences, \n",
    "    total_examples=model.corpus_count, \n",
    "    epochs=model.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again compute similarities on our hand-picked word pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Government\" - \"law\": 0.377\n",
      "\"Government\" - \"bill\": 0.298\n",
      "\"Opposition\" - \"law\": 0.354\n",
      "\"Opposition\" - \"bill\": 0.113\n",
      "\"Opposition\" - \"proposal\": 0.765\n",
      "\"bill\" - \"proposal\": 0.396\n",
      "\"Labour\" - \"Government\": 0.772\n",
      "\"Labour\" - \"Opposition\": 0.721\n",
      "\"Conservatives\" - \"Government\": 0.665\n",
      "\"Conservatives\" - \"Opposition\": 0.722\n",
      "\"shadow\" - \"Minister\": 0.727\n",
      "\"Opposition\" - \"bench\": 0.693\n",
      "\"chair\" - \"bench\": 0.736\n",
      "\"Chair\" - \"Speaker\": 0.946\n",
      "\"hon\" - \"Member\": 0.463\n",
      "\"honourable\" - \"Member\": 0.172\n",
      "\"hon\" - \"Friend\": 0.585\n",
      "\"Friend\" - \"Member\": 0.871\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs:\n",
    "    if all(p in model.wv.key_to_index for p in pair):\n",
    "        print(f'\"{pair[0]}\" - \"{pair[1]}\": {model.wv.similarity(pair[0], pair[1]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26290960"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join(data_path, 'gbr_commons_speech_sentences_tokenized.tsv.gzip')\n",
    "sentences = SentenceCorpus(filepath=fp, compressed=True)\n",
    "# number of sentences in the corpus\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519617649"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens in the corpus\n",
    "sum([len(text) for text in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    vector_size=100, # <= low dimensionality for testing\n",
    "    window=5,\n",
    "    min_count=25, # <= words should appear at least 25 times in the corpus\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    negative=5,\n",
    "    epochs=10, # <= set very low for testing\n",
    "    workers=10,\n",
    "    sorted_vocab=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(sentences) # <== takes long because corpus is large "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because training is going to take a while, we'll add a \"callback\" function that prints the loss after each epoch\n",
    "\n",
    "*source:* https://stackoverflow.com/a/54891714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gensim\n",
    "\n",
    "class callback(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        # record the start time of the training\n",
    "    \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f'Epoch #{self.epoch} start ...', end=' ')\n",
    "        self.ts = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        # compute the time delta of the epoch relative to self.ts\n",
    "        delta = time.time() - self.ts\n",
    "        # print the delta time and loss after each epoch\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print(f'Epoch #{self.epoch} took {int(delta // 60)}:{int(delta % 60)}m; loss = {loss}')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start ... Epoch #0 took 10:2m; loss = 77463592.0\n",
      "Epoch #1 start ... Epoch #1 took 10:2m; loss = 90601976.0\n",
      "Epoch #2 start ... Epoch #2 took 9:57m; loss = 103585816.0\n",
      "Epoch #3 start ... Epoch #3 took 10:9m; loss = 116231952.0\n",
      "Epoch #4 start ... Epoch #4 took 10:8m; loss = 128471592.0\n",
      "Epoch #5 start ... Epoch #5 took 10:10m; loss = 134217728.0\n",
      "Epoch #6 start ... Epoch #6 took 10:21m; loss = 134217728.0\n",
      "Epoch #7 start ... Epoch #7 took 11:4m; loss = 134217728.0\n",
      "Epoch #8 start ... Epoch #8 took 11:4m; loss = 134217728.0\n",
      "Epoch #9 start ... Epoch #9 took 11:23m; loss = 134217728.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3791416743, 5196176490)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(\n",
    "    sentences, \n",
    "    total_examples=model.corpus_count, \n",
    "    epochs=model.epochs,\n",
    "    # below the thing you need to adapt to use our custom callback that prints progress\n",
    "    compute_loss=True, \n",
    "    callbacks=[callback()]\n",
    ")\n",
    "\n",
    "# note: the 'loss' values printed below are increasing =(\n",
    "#       This is because gensim's reporting is buggy (https://github.com/RaRe-Technologies/gensim/pull/2135)\n",
    "#       So don't make hard jdugements about 'convergence' \n",
    "#        based on the loss values printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Sir\" - \"Madam\": 0.262\n",
      "\"Pound\" - \"Euro\": 0.250\n",
      "\"Pound\" - \"Dollar\": 0.291\n",
      "\"Government\" - \"law\": 0.351\n",
      "\"Government\" - \"bill\": 0.232\n",
      "\"Opposition\" - \"law\": 0.128\n",
      "\"Opposition\" - \"bill\": 0.089\n",
      "\"Opposition\" - \"proposal\": 0.313\n",
      "\"bill\" - \"proposal\": 0.362\n",
      "\"Labour\" - \"government\": 0.638\n",
      "\"Labour\" - \"opposition\": 0.531\n",
      "\"Conservatives\" - \"government\": 0.560\n",
      "\"Conservatives\" - \"opposition\": 0.501\n"
     ]
    }
   ],
   "source": [
    "# Let's again compute similarities on our hand-picked word pairs:\n",
    "for pair in pairs:\n",
    "    if all(p in model.wv.key_to_index for p in pair):\n",
    "        print(f'\"{pair[0]}\" - \"{pair[1]}\": {model.wv.similarity(pair[0], pair[1]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model and it's vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/models/gbr_commons\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/models/gbr_commons/gbr_commons_word2vec_w5_d100'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the model's word vectors in a file\n",
    "models_path = data_path.replace('corpora', 'models')\n",
    "print(models_path)\n",
    "fp = os.path.join(models_path, 'gbr_commons_word2vec_w5_d100')\n",
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a trained model at hand, you have two options for saving it for re-use.\n",
    "You can only save the word vectors (as `KeyedVectors`).\n",
    "Or you can save the full model (which inlcudes the word vectors but has more capabilities).\n",
    "\n",
    "The following table compares the **pros and cons** of these approaches ([source](https://radimrehurek.com/gensim/models/keyedvectors.html#how-to-obtain-word-vectors)):\n",
    "\n",
    "<table class=\"docutils align-default\">\n",
    "<colgroup>\n",
    "<col style=\"width: 24%\">\n",
    "<col style=\"width: 12%\">\n",
    "<col style=\"width: 11%\">\n",
    "<col style=\"width: 54%\">\n",
    "</colgroup>\n",
    "<tbody>\n",
    "<tr class=\"row-odd\">\n",
    "<td><p><em>capability</em></p></td>\n",
    "<td><p><em>KeyedVectors</em></p></td>\n",
    "<td><p><em>full model</em></p></td>\n",
    "<td><p><em>note</em></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>continue training vectors</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>You need the full model to train or update vectors.</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>smaller objects</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>KeyedVectors are smaller and need less RAM, because they\n",
    "don’t need to store the model state that enables training.</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>save/load from native\n",
    "fasttext/word2vec format</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>Vectors exported by the Facebook and Google tools\n",
    "do not support further training, but you can still load\n",
    "them into KeyedVectors.</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>append new vectors</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>Add new-vector entries to the mapping dynamically.</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>concurrency</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>Thread-safe, allows concurrent vector queries.</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>shared RAM</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>Multiple processes can re-use the same data, keeping only\n",
    "a single copy in RAM using\n",
    "<a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Mmap\">mmap</a>.</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>fast load</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>✅</p></td>\n",
    "<td><p>Supports <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Mmap\">mmap</a>\n",
    "to load data from disk instantaneously.</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the full model\n",
    "\n",
    "Just call the save `method()` on your word2vec `mode` object\n",
    "\n",
    "**_Note:_** It's customary to name the file you write the model to with the file extension '.model' (we will use another one when we just save the word vectors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(fp+'.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's then easy to reload the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(gensim.models.word2vec.Word2Vec, gensim.models.keyedvectors.KeyedVectors)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "tmp = Word2Vec.load(fp+'.model')\n",
    "type(tmp), type(tmp.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving only the word vectors\n",
    "\n",
    "Call the save `method()` on your word2vec `mode`'s `wv` attribute (a `KeyedVectors` object recording the model's word vectors):\n",
    "\n",
    "**_Note:_** It's customary to name the file you write the model to with the file extension '.kv' 🥝\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save(fp+'.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "tmp = KeyedVectors.load(fp+'.kv')\n",
    "type(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save in other standard formats\n",
    "\n",
    "Since Word2Vec has been implemented in many other languages (e.g., C and R), there is a standardized file format for storing word vectors.\n",
    "Saving word embeddings in this format allows \"interoperatbility\" between languages. \n",
    "So for example, you could train your word embedding model in Python but compute with the embeddings in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(fp+'.vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code write a large text file.\n",
    "We can read lines from it.\n",
    "\n",
    "\n",
    "**_Note:_** To make the file smaller, set `binary=True` when calling `save_word2vec_format()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79338 100\n",
      "the 0.101355255 0.14979516 -0.016551731 -0.15822083 0.2165262 -0.025575511 -0.027467923 -0.27066264 0.06362151 0.013817692 -0.27832943 0.08193668 -0.07645614 0.053718105 0.23482281 -0.30853987 -0.03073675 -0.06710661 0.18313242 0.21347052 -0.21600038 -0.14045033 0.056353867 0.064991266 -0.0686162 -0.16323717 -0.21860068 -0.10157401 0.024227735 0.1470622 0.13034672 0.17344618 0.017403167 0.13086256 -0.023156911 0.06504448 -0.3455694 0.24614073 -0.09477032 -0.05096083 0.2906817 -0.064221926 -0.09442973 0.016494839 -0.12768437 0.24630766 0.24658822 0.17062102 -0.1795416 -0.07855744 0.22618705 0.3084336 -0.0024432726 -0.14292161 0.028521495 -0.17187124 0.20169954 0.037252676 -0.13526437 -0.13781741 -0.18336569 0.062085446 -0.2638672 0.097755834 0.00085297757 -0.12780148 -0.03433464 -0.03774455 -0.1729083 -0.00498778 0.19696833 0.00024347208 -0.3031193 0.24918249 0.20757464 -0.16696502 -0.1293286 -0.20655353 -0.34562206 -0.3210666 0.1323354 -0.18413182 -0.23619245 0.07494202 -0.18968222 -0.26517445 0.18165287 -0.10982963 0.3853441 -0.12015637 -0.021407505 -0.21981066 0.05854812 0.0013379767 0.19502924 -0.042408675 0.089183725 -0.12018552 -0.10438342 0.16020851\n",
      "to 0.060213584 0.2220669 0.25261927 -0.25779745 0.35862094 0.040558927 -0.012362199 -0.13545533 0.17386842 0.2561962 -0.31879994 0.09448356 -0.16893652 -0.08507629 0.1531762 -0.10963818 0.036089495 -0.11428225 0.11770745 0.08963164 0.115242265 -0.09621167 0.22873122 -0.008655783 -0.062520474 0.16521554 -0.19661072 -0.0029797936 0.049663756 0.2631394 0.11998928 0.063114524 -0.12150073 0.15830277 -0.15478477 0.07408874 -0.36930317 0.37156886 -0.09309576 -0.010300974 0.19436811 -0.1974469 -0.029797032 -0.21536799 -0.1428251 0.21391106 0.2256661 0.06624474 -0.13367651 -0.008456441 0.3331545 0.3158844 0.22911893 -0.1436112 -0.17061444 0.0069536446 -0.15393019 -0.09260639 -0.16538492 0.20680149 -0.18279041 -0.0005186474 -0.35331813 0.18124218 -0.18187074 0.076962575 -0.045358814 0.012383771 -0.07159227 0.24826129 -0.0757467 0.084381916 -0.24463142 0.37139204 -0.008108251 0.046247717 -0.033829723 0.1170563 -0.37293252 -0.29722682 0.12247133 0.08625384 -0.17310469 0.12761384 -0.20387311 -0.08797013 -0.09519947 -0.28910077 0.032100312 -0.13440944 0.2011796 -0.13732395 -0.024287 0.0015410436 0.178734 -0.1795226 0.49598205 0.102847934 0.053549647 0.1498593\n"
     ]
    }
   ],
   "source": [
    "# inspect the file content (first two lines only) in the Terminal\n",
    "!head -n 3 ../data/models/gbr_commons/gbr_commons_word2vec_w5_d100.vectors\n",
    "# note: on Windows, use 'more /n 3' instead of 'head -n 3' and backward instead of forward slashes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
