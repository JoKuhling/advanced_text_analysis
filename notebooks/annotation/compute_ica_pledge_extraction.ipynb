{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a042ea1d",
   "metadata": {},
   "source": [
    "# Computing inter-annotator agreement for span annotation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d21054",
   "metadata": {},
   "source": [
    "<br><a target=\"_blank\" href=\"https://colab.research.google.com/github/haukelicht/advanced_text_analysis/blob/main/notebooks/annotation/compute_ica_pledge_extraction.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b3694",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55863e",
   "metadata": {},
   "source": [
    "### Span annotations\n",
    "\n",
    "**Span annotation** is a _sequence labeling_ task (cf. Jurafsky & Martin [2025](https://web.stanford.edu/~jurafsky/slp3/17.pdf)) that involves marking passages in a text that represent a certain concept, such as an entity.\n",
    "\n",
    "Accordingly, span annotations recorded **character spans** that indicate which passage(s) of a text were marked (if any). \n",
    "At least this is the most granular way of representing sequence labeling annotations.\n",
    "(But see the [CoNLL format](https://www.geeksforgeeks.org/nlp/what-is-conll-data-format/), that records annotations at the token level.)\n",
    "\n",
    "The example below illustrates this for an English sentence.\n",
    "The marked span is represented by a list with two elements that indicate the span's character start and end positions in the text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a58f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We will restrict foreign equity holding in private television broadcasting to 20%.\"\n",
    "span = [8, 39]\n",
    "\n",
    "print(\"first character of the span:\", repr(text[span[0]]))\n",
    "print(\"last character of the span:\", repr(text[span[1]-1]))\n",
    "print(\"extracted span:\", repr(text[span[0]:span[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a344a5",
   "metadata": {},
   "source": [
    "### Computing inter-annotator agreement for sequence labeling tasks.\n",
    "\n",
    "We want to compute to what extent annotators marked spans agree with each other.\n",
    "Specifically, following Krippendorff and others, we want to compute a _chance-adjusted_ inter-annotator agreement metric that adjusts for the probability that an agreement arises by chance.\n",
    "\n",
    "The appraoch to computing inter-coder agreement adopted in this notebook is based on Baylan et al. ([2022](https://doi.org/10.1145/3485447.3512242))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6627d",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6519b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    # shallow clone of current state of main branch \n",
    "    !git clone --branch main --single-branch --depth 1 --filter=blob:none https://github.com/haukelicht/advanced_text_analysis.git\n",
    "    # make repo root findable for python\n",
    "    import sys\n",
    "    sys.path.append(\"/content/advanced_text_analysis/\")\n",
    "\n",
    "    # install required packages\n",
    "    !pip install nltk==3.9.1 krippendorff==0.8.1\n",
    "\n",
    "    # get NLTK tokenizer data\n",
    "    from nltk import download\n",
    "    download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from krippendorff import alpha as k_alpha\n",
    "\n",
    "# NOTE: you may need to adjust your PYTHONPATH so that the src module can be found\n",
    "from src.annotation.agreement import InterAnnotatorAgreement, overlap_distance, split_iaa_by_item\n",
    "\n",
    "base_path = Path(\"/content/advanced_text_analysis/\" if COLAB else \"../../\")\n",
    "data_path = base_path / \"data\" / \"labeled\" / \"fornaciari_we_2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4a028",
   "metadata": {},
   "source": [
    "## Read the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change `\"llms\"` to the name of your group's folder\n",
    "annotations_path = data_path / \"annotations\" / \"extraction\" / \"group2\"\n",
    "\n",
    "# list all annotation files produced by doccano \n",
    "#  (each records annotations by one annotator)\n",
    "fps = list(annotations_path.glob('*.jsonl'))\n",
    "\n",
    "# read the annoations into a long-format DataFrame\n",
    "annotations = pd.concat({fp.stem: pd.read_json(fp, lines=True) for fp in fps}, ignore_index=False).reset_index(level=0, names=['annotator'])\n",
    "\n",
    "# list unique annotators\n",
    "annotations.annotator.unique().tolist()\n",
    "\n",
    "# discard entity type\n",
    "annotations['label'] = annotations.label.apply(lambda x: [anno[:2] for anno in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add metadata and reformat the DataFrame\n",
    "if 'metadata' in annotations.columns:\n",
    "    metadata = annotations['metadata'].apply(pd.Series)\n",
    "    metadata.drop(columns=['label'], inplace=True)\n",
    "    annotations[metadata.columns] = metadata\n",
    "    annotations.drop(columns=['metadata'], inplace=True)\n",
    "\n",
    "annotations = annotations.sort_values(by=['text_id', 'annotator']).reset_index(drop=True)\n",
    "annotations = annotations[['text_id', 'text', 'annotator', 'label']]\n",
    "annotations['spans'] = annotations.apply(lambda x: [x['text'][lab[0]:lab[1]] for lab in x['label']], axis=1)\n",
    "\n",
    "# count number of annotations per annotator and text\n",
    "annotations['n_annos'] = annotations.label.map(len)\n",
    "annotations['no_annos'] = annotations['n_annos']==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13c2db",
   "metadata": {},
   "source": [
    "We are interested to what extent annotators marked spans agree.\n",
    "Do the words marked in a sentence by one annotator agree with those marked by the other annotators?\n",
    "\n",
    "But note that annotated words may vary a lot in their character length.\n",
    "We don't want this varation to influence our agreement measure.\n",
    "\n",
    "So we need to convert the character-level into token-level annotations.\n",
    "For this, we first tokenize the sentence and than map the character-level span start and end indexes to their correspondong token-level indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b214af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# list(tokenizer.span_tokenize(text))\n",
    "def character_to_token_spans(text: str, spans: list[tuple[int, int]]) -> list[tuple[int, int]]:\n",
    "    token_spans = list(tokenizer.span_tokenize(text))\n",
    "    token_span_list = []\n",
    "    for span in spans:\n",
    "        start_char, end_char = span\n",
    "        # Find the first token that starts after or at the start_char\n",
    "        start_token = next((i for i, (s, _) in enumerate(token_spans) if s >= start_char), None)\n",
    "        # Find the last token that ends before or at the end_char\n",
    "        end_token = next((i for i, (_, e) in reversed(list(enumerate(token_spans))) if e <= end_char), None)\n",
    "        if start_token is not None and end_token is not None and start_token <= end_token:\n",
    "            token_span_list.append((start_token, end_token + 1))  # +1 to make it exclusive\n",
    "    return token_span_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f24537",
   "metadata": {},
   "source": [
    "The example below illustrates the functionality of the `character_to_token_spans()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"We will restrict foreign equity holding in private television broadcasting to 20% and prevent cross holding to avoid emergence of monopolies in the media.\"\n",
    "# annotation\n",
    "annotation = [[8, 81], [86, 153]]\n",
    "print('extracted spans:', end='')\n",
    "print('', *[text[slice(*span)] for span in annotation], sep='\\n - ')\n",
    "print()\n",
    "\n",
    "tok_spans = character_to_token_spans(text, annotation)\n",
    "print(\"token-level annotations:\", tok_spans)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print()\n",
    "\n",
    "print('tokens in extracted spans:')\n",
    "for tspan in tok_spans:\n",
    "    print(' - ', tokens[slice(*tspan)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abad0c",
   "metadata": {},
   "source": [
    "Let's apply this logic to all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79cf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens\n",
    "annotations['tokens'] = annotations.apply(lambda x: tokenizer.tokenize(x['text']), axis=1)\n",
    "# determine token-level span informations\n",
    "annotations['token_spans'] = annotations.apply(lambda x: character_to_token_spans(x['text'], x['label']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2a0b4",
   "metadata": {},
   "source": [
    "Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = annotations.text_id.sample(1, random_state=42).iloc[0]\n",
    "ex = annotations.loc[annotations.text_id == id_, ['text', 'annotator', 'spans', 'label', 'tokens', 'token_spans']]\n",
    "print('Text:', repr(ex.text.iloc[0]))\n",
    "print('Annotations:')\n",
    "for _, row in ex.iterrows():\n",
    "    print(' -', row.annotator+':', end='')\n",
    "    print('', *row['spans'], sep='\\n   - ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4db2e",
   "metadata": {},
   "source": [
    "## Compute inter-annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336db2c5",
   "metadata": {},
   "source": [
    "### Sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdb7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = annotations[['annotator', 'text_id', 'no_annos']].copy()\n",
    "tmp['no_annos'] = ~tmp['no_annos']\n",
    "tmp = tmp.pivot(index='annotator', columns='text_id', values='no_annos').fillna(False).astype(int)\n",
    "k_alpha(tmp.values, level_of_measurement='nominal')\n",
    "# NOTE: just scratching the threshold of 0.67 for acceptable agreement (see https://www.sciencedirect.com/science/article/pii/S2215016123005411)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd8fd1",
   "metadata": {},
   "source": [
    "Krippendorff (cited in Neuendorff, 2017) names the following standards\n",
    "\n",
    "- Rely only on variables with reliabilities above α = .800.\n",
    "- Consider variables with reliabilities between α = .667 and α = .800 only for drawing tentative conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f452a",
   "metadata": {},
   "source": [
    "### Span level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2675211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Braylan et al.'s code requires that annotators and items are identified by integer IDs\n",
    "annotations['item_id'] = pd.Categorical(annotations['text_id']).codes\n",
    "annotations['annotator_id'] = pd.Categorical(annotations['annotator']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c52ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an agreement object with the entity distance function\n",
    "iaa = InterAnnotatorAgreement(\n",
    "    annotations, \n",
    "    item_colname=\"item_id\", \n",
    "    uid_colname=\"annotator_id\", \n",
    "    label_colname=\"label\", \n",
    "    distance_fn=overlap_distance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute agreement measures\n",
    "iaa.setup(parallel_calc=False)\n",
    "krippendorff_alpha = iaa.get_krippendorff_alpha()\n",
    "sigma_score = iaa.get_sigma()\n",
    "ks_score = iaa.get_ks()\n",
    "\n",
    "print(f\"Krippendorff's Alpha: {krippendorff_alpha:.4f}\")\n",
    "print(f\"Sigma Score: {sigma_score:.4f}\")\n",
    "print(f\"KS Score: {ks_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5ef97",
   "metadata": {},
   "source": [
    "## Sentence-level disagreement analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b9690",
   "metadata": {},
   "source": [
    "We can also compute agreement at the sentence level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of single-item lists\n",
    "item_ids = annotations['item_id'].unique()\n",
    "split_items = [[item] for item in item_ids]\n",
    "\n",
    "item_iaas = split_iaa_by_item(iaa, split_items)\n",
    "\n",
    "# Now compute item-level ICA measures\n",
    "per_item_ica = []\n",
    "for m in item_iaas:\n",
    "    # m.setup(parallel_calc=False)\n",
    "    m.expected_distances = iaa.expected_distances\n",
    "    try:\n",
    "        per_item_ica.append({\n",
    "            \"item_id\": m.annodf[\"item\"].iloc[0],\n",
    "            \"alpha\": m.get_krippendorff_alpha(),\n",
    "            \"sigma\": m.get_sigma(),\n",
    "            \"ks\": m.get_ks()\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item {m.annodf['item_id'].iloc[0]}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_icas = annotations[['text_id', 'text', 'item_id']].drop_duplicates().merge(pd.DataFrame(per_item_ica), on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc562eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_icas = item_icas.merge(\n",
    "    annotations.groupby('item_id').agg({\n",
    "        'spans': lambda spans: len(set.union(*spans.apply(set).tolist())), \n",
    "         'no_annos': 'all'\n",
    "    }).rename(columns={'spans': 'n_annos'}).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a69a91",
   "metadata": {},
   "source": [
    "Let's plot the distribution of sentence-level agreement estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fde8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(3, 2))\n",
    "item_icas.alpha.hist(bins=50)\n",
    "# plt.yscale('log')\n",
    "plt.xlabel(\"Krippendorff's alpha\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(-1, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac084bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the average sentence-level agreement across sentences?\n",
    "item_icas.alpha.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb344403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and in sentences with min. one annotation?\n",
    "item_icas.query('no_annos == False').alpha.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04474a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# share with \"unacceptable\" agreement?\n",
    "(item_icas.alpha < 0.65).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1dec85",
   "metadata": {},
   "source": [
    "#### clear cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = item_icas[item_icas.alpha == 1.0]\n",
    "tmp = tmp.sort_values(['alpha', 'item_id'])\n",
    "print(tmp.item_id.nunique())\n",
    "print(tmp.item_id.nunique()/len(per_item_ica))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = item_icas[['item_id', 'text_id', 'n_annos']].drop_duplicates().n_annos.value_counts().sort_index().reset_index()\n",
    "tab['prop'] = tab['count'] / tab['count'].sum()\n",
    "tab.columns = ['n_annotations', 'count', 'proportion']\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = item_icas[item_icas.n_annos==3].iloc[1]\n",
    "print(ex.text)\n",
    "print(*annotations[annotations.item_id == ex.item_id].spans.tolist(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252e9d8",
   "metadata": {},
   "source": [
    "#### good(ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = item_icas[np.logical_and(item_icas.alpha >= 0.65, item_icas.alpha < 1.0)]\n",
    "tmp = tmp.sort_values(['alpha', 'item_id'])\n",
    "print(tmp.item_id.nunique())\n",
    "print(tmp.item_id.nunique()/len(per_item_ica))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622aa4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tmp.item_id.unique()\n",
    "examples = tmp[tmp.item_id.isin(items)].merge(annotations[['item_id', 'annotator', 'spans']], on='item_id', how='left')\n",
    "for item, ex in examples.groupby('item_id'):\n",
    "    print(f\"Item: {item}, k-alpha: {ex.alpha.iloc[0]:.4f}\")\n",
    "    print(\"Text:\", repr(ex['text'].iloc[0]))\n",
    "    print(\"Spans:\")\n",
    "    for m, span in zip(ex['annotator'], ex['spans']):\n",
    "        print(f\" - {span} ({m})\")\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612aeccc",
   "metadata": {},
   "source": [
    "#### poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772146e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = item_icas[np.logical_and(item_icas.alpha >= 0.0, item_icas.alpha < 0.65)]\n",
    "tmp = tmp.sort_values(['alpha', 'item_id'])\n",
    "print(tmp.item_id.nunique())\n",
    "print(tmp.item_id.nunique()/len(per_item_ica))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tmp.item_id.unique()\n",
    "examples = tmp[tmp.item_id.isin(items)].merge(annotations[['item_id', 'annotator', 'spans']], on='item_id', how='left')\n",
    "for item, ex in examples.groupby('item_id'):\n",
    "    print(f\"Item: {item}, k-alpha: {ex.alpha.iloc[0]:.4f}\")\n",
    "    print(\"Text:\", repr(ex['text'].iloc[0]))\n",
    "    print(\"Spans:\")\n",
    "    for m, span in zip(ex['annotator'], ex['spans']):\n",
    "        print(f\" - {span} ({m})\")\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b12ab",
   "metadata": {},
   "source": [
    "#### bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = item_icas[item_icas.alpha < 0.0]\n",
    "tmp = tmp.sort_values(['alpha', 'item_id'])\n",
    "print(tmp.item_id.nunique())\n",
    "print(tmp.item_id.nunique()/len(per_item_ica))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tmp.item_id.unique()\n",
    "examples = tmp[tmp.item_id.isin(items)]\n",
    "# draw random sample of 3 examples\n",
    "examples = examples.sample(3, random_state=42)\n",
    "examples = examples.merge(annotations[['item_id', 'annotator', 'spans']], on='item_id', how='left')\n",
    "for item, ex in examples.groupby('item_id'):\n",
    "    print(f\"Item: {item}, k-alpha: {ex.alpha.iloc[0]:.4f}\")\n",
    "    print(\"Text:\", repr(ex['text'].iloc[0]))\n",
    "    print(\"Spans:\")\n",
    "    for m, span in zip(ex['annotator'], ex['spans']):\n",
    "        print(f\" - {span} ({m})\")\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
