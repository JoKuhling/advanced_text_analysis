{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b8dacc",
   "metadata": {},
   "source": [
    "# Sequence labeling annotation aggregation with the RASA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079bccb0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0925db4e",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5454cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    # shallow clone of current state of main branch \n",
    "    !git clone --branch main --single-branch --depth 1 --filter=blob:none https://github.com/haukelicht/advanced_text_analysis.git\n",
    "    # make repo root findable for python\n",
    "    import sys\n",
    "    sys.path.append(\"/content/advanced_content_analysis/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba4daf",
   "metadata": {},
   "source": [
    "#### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06f0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q crowd-kit==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4814f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from crowdkit.aggregation import SegmentationRASA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90716346",
   "metadata": {},
   "source": [
    "#### data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ea5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/content/advanced_text_analysis/\" if COLAB else \"../../\")\n",
    "data_path = base_path / \"data\" / \"labeled\" / \"fornaciari_we_2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1c61b",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6edcf389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeepSeek-V3-0324',\n",
       " 'Llama-4-Maverick-17B-128E-Instruct',\n",
       " 'Qwen3-235B-A22B-Instruct-2507',\n",
       " 'gpt-oss-120b']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_path = data_path / 'annotations' / 'extraction' / 'llms'\n",
    "\n",
    "fps = list(annotations_path.glob('*.jsonl'))\n",
    "annotations = pd.concat({fp.stem: pd.read_json(fp, lines=True) for fp in fps}, ignore_index=False).reset_index(level=0, names=['annotator'])\n",
    "\n",
    "# add metadata and reformat the DataFrame\n",
    "if 'metadata' in annotations.columns:\n",
    "    metadata = annotations['metadata'].apply(pd.Series)\n",
    "    metadata.drop(columns=['label'], inplace=True)\n",
    "    annotations[metadata.columns] = metadata\n",
    "    annotations.drop(columns=['metadata'], inplace=True)\n",
    "\n",
    "annotations = annotations.sort_values(by=['text_id', 'annotator']).reset_index(drop=True)\n",
    "annotations = annotations[['text_id', 'text', 'annotator', 'label']]\n",
    "\n",
    "# discard entity type\n",
    "annotations['label'] = annotations.label.apply(lambda x: [anno[:2] for anno in x])\n",
    "\n",
    "# induce spans from character offsets\n",
    "annotations['spans'] = annotations.apply(lambda x: [x['text'][slice(*lab)] for lab in x['label']], axis=1)\n",
    "\n",
    "# list unique annotators\n",
    "annotations.annotator.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b1699",
   "metadata": {},
   "source": [
    "### Process character spans into token masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ea1d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# list(tokenizer.span_tokenize(text))\n",
    "def character_to_token_spans(text: str, spans: list[tuple[int, int]]) -> list[tuple[int, int]]:\n",
    "    token_spans = list(tokenizer.span_tokenize(text))\n",
    "    token_span_list = []\n",
    "    for span in spans:\n",
    "        start_char, end_char = span\n",
    "        # Find the first token that starts after or at the start_char\n",
    "        start_token = next((i for i, (s, _) in enumerate(token_spans) if s >= start_char), None)\n",
    "        # Find the last token that ends before or at the end_char\n",
    "        end_token = next((i for i, (_, e) in reversed(list(enumerate(token_spans))) if e <= end_char), None)\n",
    "        if start_token is not None and end_token is not None and start_token <= end_token:\n",
    "            token_span_list.append((start_token, end_token + 1))  # +1 to make it exclusive\n",
    "    return token_span_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a97c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['tokens'] = annotations.apply(lambda x: tokenizer.tokenize(x['text']), axis=1)\n",
    "annotations['token_spans'] = annotations.apply(lambda x: character_to_token_spans(x['text'], x['label']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c3ec57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['segmentation'] = annotations.apply(lambda x: np.array([0]*len(x['tokens'])), axis=1)\n",
    "for i, row in annotations.iterrows():\n",
    "    for span in row['token_spans']:\n",
    "        annotations.at[i, 'segmentation'][span[0]:span[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04170dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = annotations[['text_id', 'annotator', 'segmentation']].rename(columns={'text_id': 'task', 'annotator': 'worker'})\n",
    "df['segmentation'] = df.segmentation.apply(lambda x: np.expand_dims(x, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6a73d",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "581005cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = SegmentationRASA(n_iter=1000, tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0367c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"crowdkit\")\n",
    "    posterior_labels = aggregator.fit_predict(df)\n",
    "posterior_labels = posterior_labels.apply(lambda x: x.squeeze())\n",
    "\n",
    "n_annotators = annotations.annotator.nunique()\n",
    "docs = annotations.sort_values(['text_id', 'annotator'])[['text_id', 'text', 'tokens']].iloc[::n_annotators]\n",
    "docs = docs.merge(posterior_labels.to_frame(name='mask').reset_index(names='text_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16049d2a",
   "metadata": {},
   "source": [
    "### Convert posterior token masks int token- and character-level spans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e97690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "def extract_spans(tokens: List[int], mask: List[bool]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Extract start and end indices of all spans from binary mask.\"\"\"\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, val in enumerate(mask):\n",
    "        if val and start is None:\n",
    "            start = i\n",
    "        elif not val and start is not None:\n",
    "            spans.append((start, i))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(mask)))\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1681261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_character_spans(text, spans):\n",
    "    token_spans = list(tokenizer.span_tokenize(text))\n",
    "    char_spans = []\n",
    "    for start, end in spans:\n",
    "        char_start = token_spans[start][0]\n",
    "        char_end = token_spans[end - 1][1]\n",
    "        char_spans.append((char_start, char_end))\n",
    "    return char_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74962daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['token_label'] = docs.apply(lambda x: extract_spans(x['tokens'], x['mask']), axis=1)\n",
    "docs['label'] = docs.apply(lambda x: token_to_character_spans(x['text'], x['token_label']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee66f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['spans'] = docs.apply(lambda x: [x['text'][slice(*lab)] for lab in x['label']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35173b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spans\n",
       "0    34\n",
       "1    16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs['spans'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a29fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: We will also introduce a multi - purpose identity card for all citizens .\n",
      " - 'introduce a multi - purpose identity card for all citizens'\n",
      "\n",
      "Text: a . We will ensure the passage of the Women’s Reservation Bill .\n",
      " - 'We will ensure the passage of the Women’s Reservation Bill'\n",
      "\n",
      "Text: We will restrict foreign equity holding in private television broadcasting to 20% (and prevent cross holding to avoid emergence of monopolies in the media) .\n",
      " - 'We will restrict foreign equity holding in private television broadcasting to 20% (and prevent cross holding to avoid emergence of monopolies in the media)'\n",
      "\n",
      "Text: India’s indigenous thorium technology programme will be expedited and given all financial assistance, correcting the grievous wrong done by the UPA Government .\n",
      " - 'India’s indigenous thorium technology programme will be expedited and given all financial assistance'\n",
      "\n",
      "Text: Immediately after forming the governments in Chhattisgarh, Madhya Pradesh and Rajasthan, as promised, the 3 Congress Governments waived the loans of farmers .\n",
      " - 'waived the loans of farmers'\n",
      "\n",
      "Text: Targeting time spent for tax compliance at 1 hour per month .\n",
      " - 'Targeting time spent for tax compliance at 1 hour per month'\n",
      "\n",
      "Text: Antyodaya cards for all households at risk of hunger will be introduced .\n",
      " - 'Antyodaya cards for all households at risk of hunger will be introduced'\n",
      "\n",
      "Text: We will introduce the goods and services tax from April 1, 2010 .\n",
      " - 'introduce the goods and services tax from April 1, 2010'\n",
      "\n",
      "Text: Every consumer of electricity in India, including farmers, would be connected through digital, tamper - proof meters in the next three years .\n",
      " - 'Every consumer of electricity in India, including farmers, would be connected through digital, tamper - proof meters in the next three years'\n",
      "\n",
      "Text: Congress promises to work with industry to increase the expenditure on science and technology to 2 per cent of GDP .\n",
      " - 'increase the expenditure on science and technology to 2 per cent of GDP'\n",
      "\n",
      "Text: New middle - level technical institutes in clusters where, for example, weavers and artisans are concentrated, will be started .\n",
      " - 'New middle - level technical institutes in clusters where, for example, weavers and artisans are concentrated, will be started'\n",
      "\n",
      "Text: A national programme will be launched, in cooperation with State Governments, to provide bicycles to girls from Below Poverty Line Families who attend school .\n",
      " - 'to provide bicycles to girls from Below Poverty Line Families who attend school'\n",
      "\n",
      "Text: We will recognise the 11 left out Indian Gorkha sub - tribes as Schedule Tribes .\n",
      " - 'recognise the 11 left out Indian Gorkha sub - tribes as Schedule Tribes'\n",
      "\n",
      "Text: Reservations for the poor among ‘Forward Classes’ will be introduced after receiving recommendations of the Commission set up for this purpose .\n",
      " - 'Reservations for the poor among ‘Forward Classes’ will be introduced after receiving recommendations of the Commission set up for this purpose'\n",
      "\n",
      "Text: The Congress will identify those environmental management functions that could be delegated to the states and local bodies .\n",
      " - 'identify those environmental management functions that could be delegated to the states and local bodies'\n",
      "\n",
      "Text: The number of courts and the number of judges will be doubled in five years for quicker judicial process .\n",
      " - 'The number of courts and the number of judges will be doubled in five years for quicker judicial process'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_instances = docs[docs['spans'].apply(len)>0]\n",
    "for i, row in positive_instances.iterrows():\n",
    "    print(f\"Text: {row['text']}\")\n",
    "    for span in row['spans']:\n",
    "        print(f\" - {repr(span)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d31d38",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c8504c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = data_path / \"annotation_set_01.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb717ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[['text_id', 'label']].merge(docs[['text_id', 'spans']], on='text_id')\n",
    "tmp['pred'] = (tmp['spans'].apply(len) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7339d411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred    0   1\n",
       "label        \n",
       "0      25   0\n",
       "1       9  16"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.value_counts(['label', 'pred']).unstack().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9d35f",
   "metadata": {},
   "source": [
    "Let's quantify this alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58bce3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85        25\n",
      "           1       1.00      0.64      0.78        25\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.87      0.82      0.81        50\n",
      "weighted avg       0.87      0.82      0.81        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(tmp['label'], tmp['pred'], zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
