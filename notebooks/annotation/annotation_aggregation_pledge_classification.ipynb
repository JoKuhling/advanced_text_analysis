{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-label classification annotation aggregation with the Dawid-Skene model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://michaelpjcamilleri.wordpress.com/2020/06/22/reaching-a-consensus-in-crowdsourced-data-using-the-dawid-skene-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![Plate diagram of the Dawid-Skene per-annotator model for annotation aggregation](https://tlk.s3.yandex.net/crowd-kit/docs/ds_llm.png){width=\"30%\"} <!--source: https://crowd-kit.readthedocs.io/en/latest/classification/#crowdkit.aggregation.classification.DawidSkene-->\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other classification annotation aggregation algorithms, see the [`crowdkit` package](https://crowd-kit.readthedocs.io/en/latest/classification/#crowdkit.aggregation.classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    # shallow clone of current state of main branch \n",
    "    !git clone --branch main --single-branch --depth 1 --filter=blob:none https://github.com/haukelicht/advanced_text_analysis.git\n",
    "    # make repo root findable for python\n",
    "    import sys\n",
    "    sys.path.append(\"/content/advanced_content_analysis/\")\n",
    "    \n",
    "    # install required packages\n",
    "    !pip install krippendorff==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from src.annotation.dawidskene import DawidSkeneModel\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/content/advanced_text_analysis/\" if COLAB else \"../../\")\n",
    "data_path = base_path / \"data\" / \"labeled\" / \"fornaciari_we_2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the sentence-level classification annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-oss-120b',\n",
       " 'DeepSeek-V3-0324',\n",
       " 'Llama-4-Maverick-17B-128E-Instruct',\n",
       " 'Qwen3-235B-A22B-Instruct-2507']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_path = data_path / \"annotations\" / \"classification\" / \"llms\"\n",
    "\n",
    "# list all annotation files produced by doccano \n",
    "#  (each records annotations by one annotator)\n",
    "fps = list(annotations_path.glob('*.csv'))\n",
    "\n",
    "# read the annoations into a long-format DataFrame\n",
    "annotations = pd.concat({fp.stem: pd.read_csv(fp) for fp in fps}, ignore_index=False).reset_index(level=0, names=['annotator'])\n",
    "\n",
    "# list unique annotators\n",
    "annotations.annotator.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = annotations.label.unique().tolist()\n",
    "n_classes = len(classes)\n",
    "model = DawidSkeneModel(n_classes, max_iter=500, tolerance=10e-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t-116.94591332709433\n",
      "50\t-72.86266319359012\n",
      "100\t-72.86266319359012\n",
      "150\t-72.86266319359012\n"
     ]
    }
   ],
   "source": [
    "posterior_labels = model.fit_transform(annotations, items_col='text_id', annotators_col='annotator', annotations_col='label')\n",
    "posterior_labels.reset_index(names='text_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimated annotator \"abilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Dawid-Skene per-annotator model, annotators are parameterized with a `n_classes` &times; `n_classes` \"reliability\" matrix $\\theta$ (read \"theta\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our application, we have two label classes that are indexed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'No Pledge', 1: 'Pledge'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(enumerate(model.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have four annotators, the ability parameters have the following shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fitted_.theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first annotator's estimated reliability parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotator: gpt-oss-120b\n",
      "[0.969, 0.031]\n",
      "[0.194, 0.806]\n"
     ]
    }
   ],
   "source": [
    "print('annotator:', model.annotators_[0])\n",
    "print(*model.fitted_.theta[0].round(3).tolist(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal indicates an annotators estimated ability to correctly label an instance of the given class:\n",
    "\n",
    "- for the \"No Pledge\" class, the probability that the annotator labels an instance as \"No Pledge\" given that its 'true' (estimated) label is \"No Pledge\" is 0.969 (element (0, 0))\n",
    "- for the \"Pledge\" class, the probability that the annotator labels an instance as \"Pledge\" given that its 'true' (estimated) label is \"Pledge\" is 0.806 (see element (1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of these diagonal entries is the annotator's \"ability\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3-235B-A22B-Instruct-2507         0.958662\n",
       "Llama-4-Maverick-17B-128E-Instruct    0.918119\n",
       "gpt-oss-120b                          0.916399\n",
       "DeepSeek-V3-0324                      0.898662\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.fitted_.worker_reliabilities).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated label class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dawid-Skene model estimates the probability that an item belongs to each class based on the annotators' annotations of it and accounting for their estimated reliabilities.\n",
    "Let's look at these estimates for a samples of items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Pledge</th>\n",
       "      <th>Pledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.946</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    No Pledge  Pledge\n",
       "13      0.000   1.000\n",
       "39      0.946   0.054\n",
       "30      0.999   0.001\n",
       "45      0.000   1.000\n",
       "17      0.999   0.001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior_labels[classes].sample(5, random_state=42).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as yoo can see, the posterior (i.e., estimated) label class probabilties vary between items.\n",
    "For example,\n",
    "\n",
    "- The first item (index 185) is estimated to belong to \"Pledge\" class with a probability of ~1.0.\n",
    "- The second item (index 587) is estimated to belong to \"No Pledge\" class with a probability of ~0.946."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify the level of uncertainty with the entropy measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_labels[\"label_uncertatinty\"] = entropy(posterior_labels[classes].values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_uncertatinty\n",
       "0.000    15\n",
       "0.005    31\n",
       "0.209     2\n",
       "0.238     1\n",
       "0.368     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior_labels[\"label_uncertatinty\"].round(3).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that tere is only three items for which the model has considerable posterior label uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation against the \"true\" labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of the example we use in this notebook, we have \"true\" sentence level labels that have been assigned by the annotators of the original study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to assess how we'll our modle-induced posterior label estimates align with these annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = data_path / \"annotation_set_01.csv\"\n",
    "df = pd.read_csv(fp)\n",
    "\n",
    "id2label = {0: 'No Pledge', 1: 'Pledge'}\n",
    "df['label'] = df['label'].map(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[['text_id', 'label']].merge(posterior_labels, on='text_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>posterior_label</th>\n",
       "      <th>No Pledge</th>\n",
       "      <th>Pledge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Pledge</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pledge</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "posterior_label  No Pledge  Pledge\n",
       "label                             \n",
       "No Pledge               25       0\n",
       "Pledge                   9      16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.value_counts(['label', 'posterior_label']).unstack().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quantify this alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Pledge       0.74      1.00      0.85        25\n",
      "      Pledge       1.00      0.64      0.78        25\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.87      0.82      0.81        50\n",
      "weighted avg       0.87      0.82      0.81        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(tmp['label'], tmp['posterior_label'], zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
