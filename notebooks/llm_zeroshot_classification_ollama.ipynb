{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025017f5",
   "metadata": {},
   "source": [
    "# Zero-shot text classification with ollama and Llama 3.1 8B model\n",
    "\n",
    "This notebook illustrates how to use the Llama 3.1 8B model via `ollama` for text classification.\n",
    "\n",
    "<br>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/haukelicht/advanced_text_analysis/blob/main/notebooks/llm_zeroshot_classification_ollama.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07785dbc",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19195bfa",
   "metadata": {},
   "source": [
    "#### üö®üö® **_If_** your are on **Colab**, follow the next instructions:\n",
    "\n",
    "1. go to the menu (in the top left of this window bleow the file name and right of the Colab icon \"CO\"),\n",
    "2. select \"Runtime\",\n",
    "3. click on \"Change\" runtime type\n",
    "4. in the Hardware accelaration tab, choose \"T4\"\n",
    "5. confirm you selection by clicking \"Save\"\n",
    "6. aggree to starting a new runtim if asked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083e6bb",
   "metadata": {},
   "source": [
    "Now, run the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e903a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    # install required packages\n",
    "    !pip install -q ollama==0.3.3 colab-xterm==0.2.0\n",
    "    # activate xterm\n",
    "    %load_ext colabxterm\n",
    "    # install ollama and start the ollama server \n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    !ollama serve\n",
    "\n",
    "if COLAB:\n",
    "    # download custom utils (not needed)\n",
    "    # !mkdir -p utils\n",
    "    # !curl -o utils/io.py https://raw.githubusercontent.com/haukelicht/advanced_text_analysis/main/notebooks/utils/io.py\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    data_path = 'https://raw.githubusercontent.com/haukelicht/advanced_text_analysis/refs/heads/main/data/'\n",
    "else:\n",
    "    import os\n",
    "    data_path = os.path.join('..', 'data', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383fe58",
   "metadata": {},
   "source": [
    "#### üö®üö® **_If_** your are on **Colab**, follow the next instructions:\n",
    "\n",
    "1. execute the next cell (which runs `%xterm`)\n",
    "2. then run in the console/terminal that opens in the cell the following lines:\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d00b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN THIS WHEN ON COLAB !!!\n",
    "if COLAB:\n",
    "    %xterm\n",
    "    # next run: \n",
    "    #  curl -fsSL https://ollama.com/install.sh | sh\n",
    "    #  ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d4dd3",
   "metadata": {},
   "source": [
    "Now continue: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da313a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ollama import Client\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86883713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ollama client that communicates with the `ollama` server running in the background\n",
    "client = Client()\n",
    "MODEL = 'llama3.1:8b' # currently the latest version of GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72c57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_available_models = lambda : [m['model'] for m in client.list()['models']]\n",
    "\n",
    "if MODEL not in get_available_models():\n",
    "  print(f\"Model not found. Running `client.pull('{MODEL}')` to download it.\")\n",
    "  client.pull(MODEL)\n",
    "\n",
    "# confirm\n",
    "assert MODEL in get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e11918",
   "metadata": {},
   "source": [
    "## Define the task\n",
    "\n",
    "In this example, we adapt the instruction for one of the tweet classification tasks examined in Gilardi et al. ([2023](https://www.pnas.org/doi/10.1073/pnas.2305016120)) \"ChatGPT outperforms crowd workers for text-annotation tasks\"\n",
    "\n",
    "- see [this README file](../data/labeled/gilardi_chatgpt_2023/README.md) for a description of the data and tasks covered in the paper\n",
    "- see [this file](../data/labeled/gilardi_chatgpt_2023/instructions.md) for a copy of their original task instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c19a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "For each tweet in the sample, follow these instructions:\n",
    "\n",
    "1. Carefully read the text of the tweet, paying close attention to details.\n",
    "2. Classify the tweet as either relevant (1) or irrelevant (0)\n",
    "\"\"\"\n",
    "\n",
    "categories = [\"Relevant\", \"Irrelevant\"]\n",
    "\n",
    "defintions = \"\"\"\n",
    "Tweets should be coded as RELEVANT when they directly relate to content moderation, as defined above. This includes tweets that discuss: social media platforms‚Äô content moderation rules and practices, governments‚Äô regulation of online content moderation, and/or mild forms of content moderation like flagging.\n",
    "Tweets should be coded as IRRELEVANT if they do not refer to content moderation, as defined above, or if they are themselves examples of moderated content. This would include, for example, a Tweet by Donald Trump that Twitter has labeled as ‚Äúdisputed‚Äù, a tweet claiming that something is false, or a tweet containing sensitive content. Such tweets might be subject to content moderation, but are not discussing content moderation. Therefore, they should be coded as irrelevant for our purposes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02dde969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following text into one of the given categories: ['Relevant', 'Irrelevant']\n",
      "\n",
      "Tweets should be coded as RELEVANT when they directly relate to content moderation, as defined above. This includes tweets that discuss: social media platforms‚Äô content moderation rules and practices, governments‚Äô regulation of online content moderation, and/or mild forms of content moderation like flagging.\n",
      "Tweets should be coded as IRRELEVANT if they do not refer to content moderation, as defined above, or if they are themselves examples of moderated content. This would include, for example, a Tweet by Donald Trump that Twitter has labeled as ‚Äúdisputed‚Äù, a tweet claiming that something is false, or a tweet containing sensitive content. Such tweets might be subject to content moderation, but are not discussing content moderation. Therefore, they should be coded as irrelevant for our purposes.\n",
      "\n",
      "Only include the selected category in your response and no further text.\n"
     ]
    }
   ],
   "source": [
    "# Let's format the prompt\n",
    "prompt = f\"Classify the following text into one of the given categories: {categories}\\n{defintions}\\nOnly include the selected category in your response and no further text.\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b2477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    # negative examples (\"irrelevant\")\n",
    "    \"\\\"Turns out Mike Bloomberg is exactly what Elizabeth Warren needed to break through in the 2020 Democratic primary. And he‚Äôs not just a foil for her on the campaign trail ‚Äî this is something she believes in, and it shows.\\\" https://t.co/1SyaHXrZlO\",\n",
    "    \"@blackhat___05 ye raha new user name change kiya kamine neüò°üó°Ô∏èüò°üó°Ô∏èüò°üó°Ô∏èüò° karo abhi FNfollow reopt aur block\",\n",
    "    \"The Kid!\\n \\nRETWEET for a chance at a @RawlingsSports baseball signed by Ken Griffey Jr. and tune in to #Junior tonight at 8pm ET/5pm PT on MLB Network.\\n \\nRules: https://t.co/MdkXLh1CdN | NoPurNec, US 18+, Ends 6/22 https://t.co/8Xw0HpHz2G\",\n",
    "    \"TW / gore \\n\\nif you come across an account and want to block them, make sure to cover the bottom half of your screen. the gore is normally at the bottom of the screen. again, stay safe, and take precaution\",\n",
    "    \"@Godlesswh_re Blocked.  Is this another Nick account?\",\n",
    "    # positive examples (\"relevant\")\n",
    "    \"Twitter we want you to suspend Marcon's account.\\n#twitterSuspendMacronAccount #TwitterSuspendMarcon @verified @Twitter @TwitterSupport\",\n",
    "    \"Twitter needs to permanently suspend @realDonaldTrump account.  Who's with me?\",\n",
    "    \"Toei is one of the most active reporters of content on Youtube and everything runs through an auto filter. Today, Toei dropped a ridiculous volume of their own series onto an official Youtube channel and GOT BANNED AND REPORTED BY THEMSELVES, TOEI.\",\n",
    "    \"Marsha Blackburn: We Are Looking at Antitrust Laws and Section 230 on Tech Censorship https://t.co/lsOWzD0Yri\",\n",
    "    \"#Facebook has banned the iconic photograph of a #Soviet solider waving the #USSR flag over the #Reichstag in May 1945. The social network claims the image violates its community guidelines for dangerous people and organizations...\\n\\nMORE: https://t.co/arpDN9Ss0P https://t.co/KGtGwE4D5J\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7520f9",
   "metadata": {},
   "source": [
    "### A single text example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed4861eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647308d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct to conversation history\n",
    "messages = [\n",
    "  # system prompt\n",
    "  {\"role\": \"system\", \"content\": prompt},\n",
    "  # user input\n",
    "  {\"role\": \"user\", \"content\": text_input},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a93c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some options controlling generation behavior\n",
    "opts = {\n",
    "    'seed': 42,         # seed controlling random number generation and thus stochastic generation\n",
    "    'temperature': 0.0, # hyper parameter controlling \"craetivity\", see https://learnprompting.org/docs/basics/configuration_hyperparameters\n",
    "    'max_tokens': 3     # maximum numbers of tokens to generate in completion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abab543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request a chat completion\n",
    "response = client.chat(MODEL, messages, options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6cc4b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Relevant'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the response\n",
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf3fa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1:8b',\n",
       " 'created_at': '2024-09-25T19:44:49.28069Z',\n",
       " 'message': {'role': 'assistant', 'content': 'Relevant'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 1271586625,\n",
       " 'load_duration': 72886000,\n",
       " 'prompt_eval_count': 262,\n",
       " 'prompt_eval_duration': 1087608000,\n",
       " 'eval_count': 3,\n",
       " 'eval_duration': 69776000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596973d",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373b62c",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fb21f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "def classify_tweet(text, system_message, model, options: Optional[Dict]=None):\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "  ]\n",
    "  if not options:\n",
    "    options = {'seed': 42, 'temperature': 0.0, 'max_tokens': 3}\n",
    "  response = client.chat(model, messages, options=options)\n",
    "  \n",
    "  result = response['message']['content']\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6788aed",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bee84cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Relevant',\n",
       " 'Irrelevant',\n",
       " 'Irrelevant',\n",
       " 'Relevant',\n",
       " 'Relevant',\n",
       " 'Relevant',\n",
       " 'Irrelevant',\n",
       " 'Relevant',\n",
       " 'Relevant',\n",
       " 'Relevant']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = [classify_tweet(text, prompt, model=MODEL) for text in texts]\n",
    "classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713ab25",
   "metadata": {},
   "source": [
    "- 3/5 negative examples classified correctly\n",
    "- 3/5 positive examples classified correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_text_annotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
