{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025017f5",
   "metadata": {},
   "source": [
    "# Few-shot text classification with LLMs\n",
    "\n",
    "This notebook illustrates how to use different LLMs for text classification.\n",
    "\n",
    "- closed-source LLMs models by OpenAI\n",
    "- open-weights model hosted via Hugging Face Inference Providers/Endpoints\n",
    "- open-weights LLMs models with `ollama`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5afba0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da313a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from src.utils.io import read_tabular\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f56a5d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False # no support for colab yet\n",
    "base_path = Path(\"/content/advanced_text_analysis/\" if COLAB else \"../../\")\n",
    "data_path = base_path / \"data\" / \"labeled\" / \"benoit_crowdsourced_2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (down)load the data\n",
    "fp = data_path / \"benoit_crowdsourced_2016-policy_area.csv\"\n",
    "if not fp.exists():\n",
    "    url = \"https://cta-text-datasets.s3.eu-central-1.amazonaws.com/labeled/\" + fp.parent.name + '/' + fp.name\n",
    "    df = pd.read_csv(url)\n",
    "    fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(fp, index=False)\n",
    "\n",
    "df = read_tabular(fp, columns=['uid', 'text', 'label', 'metadata__gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5990bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to gold examples (i.e., those labeled by experts)\n",
    "df = df[df.metadata__gold]\n",
    "del df['metadata__gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8944ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    2: 'economic',\n",
    "    3: 'social',\n",
    "    1: 'neither',\n",
    "}\n",
    "df.label = df.label.map(id2label)\n",
    "\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 20 examples per label class\n",
    "inputs = df.groupby('label').sample(20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f817c7",
   "metadata": {},
   "source": [
    "#### sample few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplars = df[~df.uid.isin(inputs.uid)].groupby('label').sample(3, random_state=42).reset_index(drop=True).sample(frac=1.0, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace6133",
   "metadata": {},
   "source": [
    "We use these \"exemplars\" in the conversation history to demonstrate the disred annotation behavior.\n",
    "\n",
    "For this, we have to format them in **turns** of input text and assistants response (using the observed \"true\" label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefe121",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_template = \"Text: '''{text}'''\"\n",
    "\n",
    "def get_exemplar_messages(exemplars):\n",
    "    exemplar_messages = []\n",
    "    for _, row in exemplars.iterrows():\n",
    "        exemplar_messages.append({\"role\": \"user\", \"content\": text_template.format(text=row.text)})\n",
    "        exemplar_messages.append({\"role\": \"assistant\", \"content\": f\"{row.label}\"})\n",
    "    return exemplar_messages\n",
    "\n",
    "exemplar_messages = get_exemplar_messages(exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplar_messages[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e11918",
   "metadata": {},
   "source": [
    "## Define the task\n",
    "\n",
    "In this example, we adapt the instruction for one of the tweet classification tasks examined in Benoit et al. ([2016](https://doi.org/10.1017/S0003055416000058)) \"Crowd-sourced Text Analysis: Reproducible and Agile Production\n",
    "of Political Data\"\n",
    "\n",
    "- see [this README file](../../data/labeled/benoit_crowdsourced_2016/README.md) for a description of the data and tasks covered in the paper\n",
    "- see [this file](../../data/labeled/benoit_crowdsourced_2016/instructions/econ_social_policy.md) for a copy of their original task instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f\"\"\"\n",
    "Act as a text classification system versatile in performing content analysis.\n",
    "\n",
    "You will read a sentence from a political text.\n",
    "Yout will judge whether this sentence deals with economic or social policy.\n",
    "You must classify posts into one of the following categories: \"economic\", \"social\", or \"neither\". \n",
    "\n",
    "## Definitions\n",
    "\n",
    "These categories have the following definitions:\n",
    "\n",
    "- Sentences should be coded as \"economic\" if they deal with aspects of the economy, such as: Taxation, Government spending, Services provided by the government or other public bodies, Pensions, unemployment and welfare benefits, and other state benefits, Property, investment and share ownership, public or private, Interest rates and exchange rates, Regulation of economic activity, public or private, Relations between employers, workers and trade unions\n",
    "- Sentences should be coded as \"social\" if they deal with aspects of social and moral life, relationships between social groups, and matters of national and social identity. These include: Policing, crime, punishment and rehabilitation of offenders; Immigration, relations between social groups, discrimination and multiculturalism; The role of the state in regulating the social and moral behavior of individuals\n",
    "\n",
    "## Step-by-step instructions\n",
    "\n",
    "Follow these steps to classify the sentence:\n",
    "\n",
    "1. Carefully read the text of the sentence, paying close attention to details.\n",
    "2. Assess whether the sentence belongs to any of the categories. If not, return 'neither' as your response.\n",
    "3. Classify the sentence with the category it belongs to. Return only the name of the category.\n",
    "\n",
    "## Response format\n",
    "\n",
    "Only include the selected category in your response and no further text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95844713",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = inputs.text.to_list()\n",
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773717a",
   "metadata": {},
   "source": [
    "## With ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90464e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "MODEL = 'gpt-4o-2024-08-06'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d1d75",
   "metadata": {},
   "source": [
    "#### illustration with a _single_ sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e912bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.iloc[5]\n",
    "print(text)\n",
    "\n",
    "messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    # NOTE: here we inject the few-shot examples between the instruction and the to-be-classified text\n",
    "    *exemplar_messages,\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text_template.format(text=text)},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596973d",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373b62c",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb21f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, exemplars, model):\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # NOTE: here we inject the few-shot examples between the instruction and the to-be-classified text\n",
    "    *exemplars,\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text_template.format(text=text)},\n",
    "  ]\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    "  )\n",
    "  \n",
    "  result = response.choices[0].message.content\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6788aed",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_gpt4o = [\n",
    "    classify_text(text, instructions, exemplar_messages, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=inputs.label,\n",
    "    y_pred=classifications_gpt4o,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771ec7e",
   "metadata": {},
   "source": [
    "Without exemplars (few-shot inference), the macro F1 was 0.93.\n",
    "This was already very strong. \n",
    "\n",
    "So in this case, adding exemplars doesn't achieve and improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd7288",
   "metadata": {},
   "source": [
    "## With Hugging Face _Inference Providers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de141bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "MODEL = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "client = InferenceClient(MODEL, token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc7e19",
   "metadata": {},
   "source": [
    "the **cool thing** is that the `InferenceClient` works exactly like the `openai.Client` class.\n",
    "So the code from above really _doesn't change_!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9161708",
   "metadata": {},
   "source": [
    "#### illustration with a _single_ sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227681fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.iloc[5]\n",
    "print(text)\n",
    "\n",
    "messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    # NOTE: here we inject the few-shot examples between the instruction and the to-be-classified text\n",
    "    *exemplar_messages,\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text_template.format(text=text)},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9781c",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c385588",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, exemplars, model):\n",
    "  # NOTE: `model` actually not needed because we setup the InferenceClient with the model already\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # NOTE: here we inject the few-shot examples between the instruction and the to-be-classified text\n",
    "    *exemplars,\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text_template.format(text=text)},\n",
    "  ]\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    "  )\n",
    "  \n",
    "  result = response.choices[0].message.content\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c57f5a",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_llama3_70b = [\n",
    "    classify_text(text, instructions, exemplar_messages, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47463ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=inputs.label,\n",
    "    y_pred=classifications_llama3_70b,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8cf96",
   "metadata": {},
   "source": [
    "With few-shot inference, the macro F1 was 0.83.\n",
    "So in this case, we could boost it to 0.89.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc725fc7",
   "metadata": {},
   "source": [
    "Let's try also with the R1 model from DeepSeek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8996dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL= \"deepseek-ai/DeepSeek-V3-0324\"\n",
    "\n",
    "client = InferenceClient(MODEL, provider=\"sambanova\", token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_deepseekR1 = [\n",
    "    classify_text(text, instructions, exemplar_messages, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=inputs.label,\n",
    "    y_pred=classifications_deepseekR1,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563c5c6",
   "metadata": {},
   "source": [
    "Here we boost the macro F1 from 0.85 (zero-shot) to 0.90 (9-shot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845ec56",
   "metadata": {},
   "source": [
    "## With Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e180bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client()\n",
    "MODEL = 'gemma3:4b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list models\n",
    "available_models = [m['model'] for m in client.list()['models']]\n",
    "\n",
    "if MODEL not in available_models:\n",
    "    import ollama\n",
    "    ollama.pull(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f385a9d",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdacff",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, exemplars, model):\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # NOTE: here we inject the few-shot examples between the instruction and the to-be-classified text\n",
    "    *exemplars,\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text_template.format(text=text)},\n",
    "  ]\n",
    "\n",
    "  # set some options controlling generation behavior\n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  opts = {\n",
    "      'seed': 42,         # seed controlling random number generation and thus stochastic generation\n",
    "      'temperature': 0.0, # hyper parameter controlling \"craetivity\", see https://learnprompting.org/docs/basics/configuration_hyperparameters\n",
    "      'max_tokens': 3     # maximum numbers of tokens to generate in completion\n",
    "  }\n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  response = client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    options=opts\n",
    "  )\n",
    "  \n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  result = response.message.content.strip()\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_gemma3_4b = [\n",
    "    classify_text(text, instructions, exemplar_messages, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69172cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=inputs.label,\n",
    "    y_pred=classifications_gemma3_4b,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d6b94",
   "metadata": {},
   "source": [
    "Here we boost the macro F1 from 0.80 to 0.87 (by 8.75%).\n",
    "So for the smallest model, we get the strongest relative gain from using few-shot exemplars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce8207",
   "metadata": {},
   "source": [
    "## Similarity-based exemplar selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aca245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embedder = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ae157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_exemplars_by_similarity(text, embeddings, labels, k=3):\n",
    "    \"\"\"\n",
    "    Select the top-k most similar exemplars for each class based on cosine similarity.\n",
    "    \"\"\"\n",
    "    # embed the input text\n",
    "    text_embedding = embedder.encode(text)\n",
    "    # compute cosine similarities\n",
    "    similarities = cosine_similarity([text_embedding], embeddings)[0]\n",
    "    # put similarities and labels in a DataFrame\n",
    "    out = pd.Series(similarities).to_frame('similarity')\n",
    "    out['label'] = labels\n",
    "    # select top-k exemplars per class\n",
    "    out = out.groupby('label')['similarity'].apply(lambda x: x.nlargest(k)).reset_index(level=0, drop=True)\n",
    "    # reshuffle\n",
    "    out = out.sample(frac=1.0, random_state=42)\n",
    "    # return the indices of the selected exemplars\n",
    "    return out.index.tolist()\n",
    "\n",
    "# pre-compute embeddings for all available exemplars (i.e., those not in the set of input texts to classify)\n",
    "exemplars = df[~df.uid.isin(inputs.uid)].reset_index(drop=True)\n",
    "exemplar_embeddings = embedder.encode(exemplars.text.to_list())\n",
    "\n",
    "n_exemplars = 3\n",
    "exemplars_by_text = []\n",
    "# iterate over all texts to select the most similar exemplars for each text\n",
    "for text in tqdm(texts, desc=\"Selecting exemplars by similarity\"):\n",
    "    idxs = select_exemplars_by_similarity(text, exemplar_embeddings, exemplars.label, k=n_exemplars)\n",
    "    exs = exemplars.iloc[idxs]\n",
    "    exemplars_by_text.append(get_exemplar_messages(exs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify all texts using similarity-based exemplar selection\n",
    "classifications_gemma3_4b_sim = []\n",
    "for text, exs in tqdm(zip(texts, exemplars_by_text), total=len(texts)):\n",
    "    pred = classify_text(text, instructions, exs, model=MODEL)\n",
    "    classifications_gemma3_4b_sim.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75742d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=inputs.label,\n",
    "    y_pred=classifications_gemma3_4b_sim,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1ea5a",
   "metadata": {},
   "source": [
    "The macro F1 doesn't change but the performance across label classes becomes more balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8da61",
   "metadata": {},
   "source": [
    "## Inter-LLM agreement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9289c",
   "metadata": {},
   "source": [
    "What if we consider the different LLM's classifications as annotations?\n",
    "Then we compute see the degree of their inter-annotator agreement (ICA).\n",
    "\n",
    "This is equivalent to what we did in the [notebook](../annotation/compute_ica_pledge_classification.ipynb) on computing ICA in our policy pledge codings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d27bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from krippendorff import alpha\n",
    "\n",
    "tmp = pd.DataFrame({\n",
    "    'gpt4o': classifications_gpt4o,\n",
    "    'gemma3_4b': classifications_gemma3_4b,\n",
    "    'llama3_70b': classifications_llama3_70b,\n",
    "    'deepseekR1': classifications_deepseekR1,\n",
    "})\n",
    "\n",
    "label2id = {\n",
    "    'economic': 0,\n",
    "    'social': 1,\n",
    "    'neither': 2,\n",
    "}\n",
    "\n",
    "tmp = tmp.apply(lambda x: x.map(label2id))\n",
    "alpha(tmp.T.values, level_of_measurement='nominal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dcec6e",
   "metadata": {},
   "source": [
    "😳 This is a very strong agreement and about 10% higher than the agreemet between LLMs' zero-shot classifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
