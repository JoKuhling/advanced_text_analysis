{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, counting tokens, and cost calculations\n",
    "\n",
    "Although LLMs allow text-to-text user--computer interaction, behind the scenes the work with numbers.\n",
    "This means that any input text needs to be converted into a sequence of integers (\"encoded\") that represent the words, subwords, and symbols in the input in a way the model can process.\n",
    "This process of converted text inoputs into a sequence of integers is called *tokenization*.\n",
    "\n",
    "You should think about tokenization when you rely on a **commercial API** for using an LLM.\n",
    "Commercial providers charge per **input and output tokens**:\n",
    "\n",
    "- input tokens are those that go into the model\n",
    "- output tokens are those that come out of the model (i.e., that the model generates to responsed to your prompt)\n",
    "\n",
    "Counting input tokens and estimating the number of output tokens is important because it helps you to compute the costs of using a language model and ensure that your input text is within the maximum token limit of the model you are using.\n",
    "\n",
    "In this notebook, we'll use some custom classes I've define to count tokens when you use Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "> The atomic unit of consumption for a language model is not a “word”, but rather a “token”.\n",
    "> You can kind of think of tokens as syllables, and on average they work out to about 750 words per 1,000 tokens.\n",
    "> They represent many concepts beyond just alphabetical characters – such as punctuation, sentence boundaries, and the end of a document.\n",
    "> &mdash; [source](https://github.com/brexhq/prompt-engineering?tab=readme-ov-file#tokens)\n",
    "\n",
    "Learn more about tokenizers and their reason of existence here: https://huggingface.co/docs/transformers/tokenizer_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token limits a.k.a. context window size\n",
    "\n",
    "LLMs are \"stateless\" and thus cannot remember anything about previous requests or converations.\n",
    "This means that so you always need to include everything that it might need to know that is specific to the current session.\n",
    "\n",
    "This is a major downside of LLMs, as it means that the leading language model architecture, the Transformer, has a fixed input and output size – at a certain point the prompt cannot grow any larger.\n",
    "\n",
    "The total size of the prompt, sometimes referred to as the **context window**, is model dependent.\n",
    "For GPT-3, it is 4,096 tokens. \n",
    "For GPT-4, it is 8,192 tokens or 32,768 tokens depending on which variant you use.\n",
    "\n",
    "You can find a detailed overview here: \n",
    "\n",
    "- for GPT-4 and its variants: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
    "- for GPT-3.5-turbo and its variants: https://platform.openai.com/docs/models/gpt-3-5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tiktoken` makes available several encodings that are used by the varios OpenAI models, including GPT-3 and GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'r50k_base',\n",
       " 'p50k_base',\n",
       " 'p50k_edit',\n",
       " 'cl100k_base',\n",
       " 'o200k_base',\n",
       " 'o200k_harmony']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list encoding names\n",
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, GPT-4 (snapshot from June 2023) uses the 'cl100k_base' encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o200k_base'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the encoding model for the desired model\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-2024-08-06')\n",
    "encoding.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `encoding` instance created above, you can tokenize and encode any text input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13225, 11, 2375, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode('Hello, world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are just token's indexes in the tokenizer's vocabulary. They are not the actual token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' world', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(tok).decode() for tok in encoding.encode('Hello, world!')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we can tokenize a text, counting the number of tokens is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = encoding.encode('Hello, world!')\n",
    "len(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined token counter classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.token_counters import OpenAITokenCounter\n",
    "\n",
    "tokens_counter = OpenAITokenCounter(model='gpt-4o-2024-08-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Liberal Alliance er det eneste alternativ til  et træt VKO-flertal, som er bange for både  reformer, udlændinge og vælgere, og en  populistisk S/SF-regering, som er bange  for præcis de samme ting - og som vil indføre endnu flere skatter, afgifter, regler og  forbud,  end  den  nuværende  regering  plager os med.\"\n",
    "tokens_counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_counter([\"Hello, world!\", \"I'm tiktoken!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same with open-source models (but trickier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, open-weights models like Llama 3 are available through the [hugging face model hub](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct).\n",
    "So we can download the tokenizers from there to compute the number of (input) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'REPLICATE_MODELS_TOKENIZERS' from 'src.utils.token_counters' (/Users/hlicht/Dropbox/teaching/advanced_text_analysis/github/src/utils/token_counters.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtoken_counters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HFTokenCounter, REPLICATE_MODELS_TOKENIZERS\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# say you use 'meta/meta-llama-3-70b-instruct' via Replicate (see https://replicate.com/meta/meta-llama-3-70b-instruct)\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# tokenizer_name = REPLICATE_MODELS_TOKENIZERS['meta/meta-llama-3-70b-instruct']\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# print(tokenizer_name)\u001b[39;00m\n\u001b[32m      7\u001b[39m tokenizer_name = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-1.5B-Instruct\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'REPLICATE_MODELS_TOKENIZERS' from 'src.utils.token_counters' (/Users/hlicht/Dropbox/teaching/advanced_text_analysis/github/src/utils/token_counters.py)"
     ]
    }
   ],
   "source": [
    "from src.utils.token_counters import HFTokenCounter, REPLICATE_MODELS_TOKENIZERS\n",
    "\n",
    "# say you use 'meta/meta-llama-3-70b-instruct' via Replicate (see https://replicate.com/meta/meta-llama-3-70b-instruct)\n",
    "\n",
    "# tokenizer_name = REPLICATE_MODELS_TOKENIZERS['meta/meta-llama-3-70b-instruct']\n",
    "# print(tokenizer_name)\n",
    "tokenizer_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "tokens_counter = HFTokenCounter(tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_counter([\"Hello, world!\", \"I'm tiktoken!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing API usage costs\n",
    "\n",
    "OpenAI charges model usage costs based on the number of tokens processed by the model.\n",
    "This means that you need to be aware of the number of tokens in your input text and the (expected) number of tokens in its response to avoid unexpected costs.\n",
    "\n",
    "To see what OpenAI charges you per 1,000,000 (one million) input and output tokens, see https://openai.com/pricing\n",
    "\n",
    "On September 16, 2024, the cost for using GPT-4o (`gpt-4o-2024-08-06`) are: $3.50 per 1M input tokens, and $10.00 per 1M output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o-2024-08-06'\n",
    "tokens_counter = OpenAITokenCounter(model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example calculations\n",
    "\n",
    "Say you have a dataset with ten sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I absolutely love this product. It's incredibly user-friendly.\",\n",
    "    \"I'm really disappointed with the service I received.\",\n",
    "    \"The weather today is absolutely beautiful, it makes me feel so happy.\",\n",
    "    \"I'm feeling really down today, nothing seems to be going right.\",\n",
    "    \"This is the best day of my life, I couldn't be happier!\",\n",
    "    \"I'm so frustrated with the lack of communication from the team.\",\n",
    "    \"The movie was a masterpiece, the storyline was captivating and the acting was superb.\",\n",
    "    \"I'm feeling really stressed about the upcoming exam.\",\n",
    "    \"The food at the restaurant was delicious, I'll definitely be going back.\",\n",
    "    \"I'm really angry about the decision, it's completely unfair.\"\n",
    "]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And say your instructions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You will be provided with a sentence. \n",
    "\n",
    "Your task is to classify the sentence's sentiment as either positive, negative, or neutral.\n",
    "\n",
    "Please choose one of the following categories: positive, negative, neutral.\n",
    "\n",
    "Only respond with your chosen category and no further text or explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of input tokens\n",
    "\n",
    "compontents:\n",
    "\n",
    "- `n = len(texts)`\n",
    "- `n_tokens_prompt = tokens_counter(prompt)`\n",
    "- `n_tokens_data = sum(tokens_counter(texts))`\n",
    "\n",
    "finally: `n_input_tokens_total = sum(n*n_tokens_prompt + n_tokens_data)`\n",
    "\n",
    "#### Number of output tokens\n",
    "\n",
    "`n_input_tokens_total = n * 2` (this is specific to single-label text class. task)\n",
    "\n",
    "#### Finally\n",
    "\n",
    "`total_cost = n_input_tokens_total*input_token_cost + n_input_tokens_total*output_token_cost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our label classes have the following numbers of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens per label class?\n",
    "tokens_counter(['positive', 'negative', 'neutral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each sentence, we need to send the instructions plus the sentences as input and we will receive one of the three answer categories.\n",
    "So we can calulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(dataset)\n",
    "n_input_tokens = sum(tokens_counter(dataset)) + n * tokens_counter(instructions)\n",
    "n_output_tokens = len(dataset)*2\n",
    "\n",
    "print('# of input tokens:', n_input_tokens)\n",
    "print('# of output tokens:', n_output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we can compute the cost (in U.S. $) for requesting classifications of the ten examples in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    n_input_tokens/1_000_000 * 2.50 # $2.50 per 1M input tokens\n",
    "    +\n",
    "    n_output_tokens/1_000_000 * 10.00 # $10 per 1M output tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is 19/100 of a U.S. Dollar cent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Note that additional charges for value added tax (VAT) may apply. Check this when you plan your budget."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
