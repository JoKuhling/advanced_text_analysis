{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI's chat completions API\n",
    "\n",
    "\n",
    "OpenAI's chat models take a list of messages as input and return a model-generated message as output.\n",
    "The chat format is designed to make multi-turn conversations easy.\n",
    "But it is just as useful for single-turn tasks without any conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The message object\n",
    "\n",
    "The message message object records the contributions to a conversation in a [**list**](link):\n",
    "\n",
    "```json\n",
    "[\n",
    "    <first message>,\n",
    "    <second message>,\n",
    "    ...,\n",
    "    <last message>\n",
    "]\n",
    "```\n",
    "\n",
    "Each list element is a [**dictionary**](link) with elements **_role_** and **_content_**:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"role\": \"system\" | \"user\" | \"assistant\", // <== choose one\n",
    "    \"content\": <string> // <== insert text\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Example*\n",
    "\n",
    "Below, we mimic the tweet sentiment classifier in OpenAI's playground:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Act like a sentiment classification system.\n",
      "\n",
      "You will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\n",
      "\n",
      "Categorize the text enclosed in triple quotes into one of the following categories: \"positive\", \"neutral\", \"negative\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "Act like a sentiment classification system.\n",
    "\n",
    "You will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\n",
    "\n",
    "Categorize the text enclosed in triple quotes into one of the following categories: \"positive\", \"neutral\", \"negative\"\n",
    "\"\"\"\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": instructions.strip()\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Text: '''I loved the new Batman movie!'''\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Act like a sentiment classification system.\\n\\nYou will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\\n\\nCategorize the text enclosed in triple quotes into one of the following categories: \"positive\", \"neutral\", \"negative\"'},\n",
       " {'role': 'user', 'content': \"Text: '''I loved the new Batman movie!'''\"}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roles\n",
    "\n",
    "There are three **roles**:\n",
    "\n",
    "- **\"system\"**: a 'hidden' prompt to provide the model with relevant task-specific information.\n",
    "- **\"user\"**: the user who is sending inputs and requests to the model\n",
    "- **\"assistant\"**: the model who is responding to the user\n",
    "\n",
    "**_Note:_** When you use ChatGPT in the browser, you only see the messages send by you (the user) and the model's responses (the assistants message)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a typical chat conversation from a translation example:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Act as a translation system that translates English texts to French\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, how are you?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Bonjour, comment vas-tu?\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The system message\n",
    "\n",
    "The system message is only defined once at the beginning of a conversation.\n",
    "It provides the LLM (\"assistant\") with behavioral instructions.\n",
    "That is, it sets the behavior of the assistant.\n",
    "For example, you can modify the personality (\"persona\") of the assistant or provide specific instructions about how it should behave throughout the conversation.\n",
    "\n",
    "Note, however that the system message is optional.\n",
    "The model's behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User and assistant messages\n",
    "\n",
    "After the system message, conversations can be as short as one message or include many back and forth turns between the user and the assistant.\n",
    "The user messages provide requests or comments for the assistant to respond to.\n",
    "If after the system message you only include one user message, the model will return an assistant message &mdash; the model's response to the user's message (given the instructions in the system message).\n",
    "\n",
    "But as assistant messages store previous assistant responses, you can also write them to give examples of desired behavior.\n",
    "Including conversation history is important when user instructions refer to prior messages.\n",
    "Because chat models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting a chat completion\n",
    "\n",
    "The way you use the chat completions API for text coding is by specifying a system prompt and a user input and requesting a model-generated assisant response.\n",
    "\n",
    "To allow communication with the OpenAI API, you first need to create a client that takes care of sending API requests and receiving responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x7ca42680b490>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a message object and request a model-generated response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Act as a translation system that translates English texts to French\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, how are you?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the identifier of the model we want to ask for a response, we send the message object to the API and receive a response from the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-CJcjUtIKDwkQYFpJAzrr7lvLmzix0',\n",
       " 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Bonjour, comment Ã§a va ?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))],\n",
       " 'created': 1758793156,\n",
       " 'model': 'gpt-4o-2024-08-06',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_f33640a400',\n",
       " 'usage': CompletionUsage(completion_tokens=6, prompt_tokens=28, total_tokens=34, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)),\n",
       " '_request_id': 'req_eaa5ff3972274d8ead508bb070703891'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `response` object returned by calling `chat.completion.create` is a little complex.\n",
    "The relevant information is stored in the `choices` list, which contains a single element by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Bonjour, comment Ã§a va ?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a `Choices` object that contains, among others, a `message` atttribute that contains the content of the assistant's response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='Bonjour, comment Ã§a va ?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, comment Ã§a va ?'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text classification example\n",
    "\n",
    "In our applications, we apply this logic to task a GPT model to classify texts according to our coding instructions and coding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"\"\"\\\n",
    "Act as a sentiment classification system that classifies the sentiment of a given text. \n",
    "Classify the text in the input into one of the following categories: positive, negative, neutral. \n",
    "Only respond with the chosen category and no further text.\n",
    "\"\"\"\n",
    "\n",
    "messages = [ \n",
    "    # system prompt: coding instruction and specify coding scheme\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": instruction\n",
    "    },\n",
    "    # user prompt: a single text to be classified\n",
    "    {\"role\": \"user\", \"content\": \"I am so happy today!\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I loved the new Batman movie!\"\n",
    "\n",
    "messages = [ \n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": instruction # <== copy-paste your custom instructions here\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"'''{text}'''\"} # <== copy-paste your text here\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    logprobs=True,\n",
    "    top_logprobs=20,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 0.9998764455604927,\n",
       " 'Positive': 0.0001233945049762016,\n",
       " '_positive': 4.139424490804616e-08,\n",
       " ' positive': 3.65302929161789e-08,\n",
       " '-positive': 4.362921314631966e-09,\n",
       " 'posit': 2.9985890437551646e-09,\n",
       " '``': 1.6050290551017823e-09,\n",
       " '```': 1.1031192617194595e-09,\n",
       " 'negative': 7.58162041807597e-10,\n",
       " '**': 6.690756535524169e-10,\n",
       " \"'''\": 3.160489599914966e-10,\n",
       " '`': 2.789122282575785e-10,\n",
       " \"''\": 1.9169338420513415e-10,\n",
       " 'positivo': 1.9169338420513415e-10,\n",
       " \"'\": 1.317488077793474e-10,\n",
       " '\"': 7.052005505764733e-11,\n",
       " '\\u200b': 5.492107410113433e-11,\n",
       " 'pos': 2.9397132579170476e-11,\n",
       " '\\u200b\\u200b': 2.9397132579170476e-11,\n",
       " 'POS': 2.9397132579170476e-11}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "top_token_probs = {lp.token: np.exp(lp.logprob) for lp in response.choices[0].logprobs.content[0].top_logprobs}\n",
    "top_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6974579283637646e-09"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = (top_token_probs['positive'] + top_token_probs['Positive'] + top_token_probs[' positive'] + top_token_probs['_positive'] + top_token_probs['-positive'])\n",
    "neg = top_token_probs['negative']\n",
    "np.log(pos)/np.log(neg) # odds ratio positive vs. negative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
