{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea65f496",
   "metadata": {},
   "source": [
    "# Application: Pledge extraction through in-context learning\n",
    "\n",
    "This notebook illustrates how to apply in-context learning to identify policy pledges in party manifesto sentences.\n",
    "\n",
    "We will rely on LLMs served via the Hugging Face _Inference Providers_ API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfae2f9",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "We can apply in-context learning to task a large pre-trained generative language model with an annotation task.\n",
    "This is commonly referred to as [\"prompting\"](https://learnprompting.org/vocabulary/prompting).\n",
    "\n",
    "The **most important ingredients** for this approach are:\n",
    "\n",
    "1. A large pre-trained generative language model (LLM) that can generate [chat completions](https://huggingface.co/docs/inference-providers/en/tasks/chat-completion).\n",
    "2. A task description that describes the to-be-completed annotation task to the LLM.\n",
    "3. To-be-annotated texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49901a6",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6f816",
   "metadata": {},
   "source": [
    "### Defining the prompt\n",
    "\n",
    "The formatted string below specifies the task instruction that we will use as a [**system prompt**](https://promptengineering.org/system-prompts-in-large-language-models/) for the model.\n",
    "\n",
    "The task instructions are based on the **defintion of a pledge** used in the [_Comparative Pledges Project_](https://comparativepledges.net/).\n",
    "As is common, the instruction include\n",
    "    \n",
    "1. a task description,\n",
    "2. a definition of the focal concept,\n",
    "3. instructions for how to format the response, and\n",
    "4. (optionally) a few examples that illustrate the desired annotation behavior.\n",
    "\n",
    "Note that in the example below, \n",
    "\n",
    "1. We use a *persona*. This is a common practice although there is [no clear evidence](https://arxiv.org/abs/2311.10054) that this improves LLMs' performance.\n",
    "2. we use a strategy called [\"Chain-of-Thought\" prompting](https://learnprompting.org/vocabulary/CoT_Prompting), that is, a prompt that describes the \"thought process\" the LLM should apply to generate its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7c6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert annotator of political texts. \\  \n",
    "Your task is to identify **pledges** and extract them from a sentence taken from a party manifesto.  \n",
    "\n",
    "### Definition of a Pledge  \n",
    "\n",
    "A pledge is a specific kind of statement in which the author commits to a concrete action or outcome.\n",
    "To be counted as a pledge, a statement must meet two criteria: \n",
    "\n",
    "1. **Specific and Concrete**: The statement should outline a clear, measurable action or outcome.\n",
    "2. **Clear Commitment**: The statement should represent a firm commitment to take that action, not just a general principle or stance.\n",
    "\n",
    "Importantly, this is a *narrow* definition: only statements with clear, measurable, and time-bound outcomes are counted as pledges.\n",
    "\n",
    "### Reasoning Steps (Chain-of-Thought)  \n",
    "\n",
    "1. **Read the input text carefully.**  \n",
    "2. **Identify candidate statements** in the text that might be pledges.  \n",
    "3. For each candidate, verify the two criteria:  \n",
    "   - Is it specific and concrete? (Does it mention a measurable action or outcome?)  \n",
    "   - Does it express a clear commitment to act?  \n",
    "4. Only if both criteria are met, consider a statement as a pledge.\n",
    "5. **Extract the pledge verbatim** from the text by copying it exactly as it appears.\n",
    "\n",
    "### Examples  \n",
    "\n",
    "sentence: \"We will build 100,000 affordable housing units by 2030.\"\n",
    "output: {\"pledges\": [\"build 100,000 affordable housing units by 2030\"]}\n",
    "explanation: the phrase \"We will build 100,000 affordable housing units by 2030\" is a concrete commitment with a measurable outcome.\n",
    "\n",
    "sentences: \"We believe in fair housing.\"\n",
    "output: {\"pledges\": []}\n",
    "explanation: This statement merely notes a policy _stance_ but does not contain any pledges.\n",
    "\n",
    "sentence: \"This year's election will be the most important in our history.\"\n",
    "output: {\"pledges\": []}\n",
    "explanation: This statement does not contain any pledges.\n",
    "\n",
    "sentence: \"We will ensure all public schools are inclusive and welcoming.\"\n",
    "output: {\"pledges\": []}\n",
    "explanation: While the phrase \"ensure all public schools are inclusive and welcoming\" might be viewed as a pledge, our narrow definition would not count it due to the lack of a specific metric.\n",
    "\n",
    "sentence: \"We will foster a culture of innovation in our economy.\"\n",
    "output: {\"pledges\": []}\n",
    "explanation: While \"foster a culture of innovation in our economy\" might be seen as a pledge in a broader sense, but under our narrow criteria, it is too abstract to count.\n",
    "\n",
    "### Instruction  \n",
    "\n",
    "Follow the reasoning steps above. \\\n",
    "Then, output your final decision by listing all verbatim pledges in a JSON array. If no pledges are found, return an empty array.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e9d3d",
   "metadata": {},
   "source": [
    "### Defining the output format\n",
    "\n",
    "When we use generative LLMs to complete annotation tasks, it is a **best practice** to specify how the [outputs should be structured](https://humanloop.com/blog/structured-outputs).\n",
    "After all, as in content analysis more generally, we don't want the model to responde in natural language but with data.\n",
    "\n",
    "In **extractive tasks**, \n",
    "\n",
    "- we commonly want the model to extract the verbatim passages in the text that represent a certain context\n",
    "- each text can contain none, one, or more such passages\n",
    "\n",
    "Therefore, we can think of the intended output format as a _list_ of _strings_ (that must occur _literally_ in the input text).\n",
    "\n",
    "Below, we use `pydantic` to define a custom class that defines the intended structure of an LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618da6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, fields\n",
    "from typing import List, Tuple, Optional\n",
    "from huggingface_hub.inference._generated.types.chat_completion import (\n",
    "    ChatCompletionInputResponseFormatJSONSchema, \n",
    "    ChatCompletionInputJSONSchema\n",
    ")\n",
    "\n",
    "class PledgeExtractionOutput(BaseModel):\n",
    "    # attribute capturing the list of verbatim pledges extracted from the text\n",
    "    pledges: List[str] = fields.Field(default_factory=list, description=\"A list of verbatim pledges extracted from the text. If no pledges are found, this list should be empty.\", required=True)\n",
    "    # and optional attribute capturing the character spans of the pledges in the original text\n",
    "    spans: Optional[List[Tuple[int, int]]] = None\n",
    "\n",
    "    def get_spans(self, text: str) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Get the character spans of the extracted pledges in the original text.\n",
    "        \"\"\"\n",
    "        spans = []\n",
    "        for pledge in self.pledges:\n",
    "            start = text.find(pledge)\n",
    "            while start != -1:\n",
    "                end = start + len(pledge)\n",
    "                spans.append((start, end))\n",
    "                start = text.find(pledge, end)\n",
    "        return spans\n",
    "    \n",
    "    def _set_spans(self, text: str):\n",
    "        \"\"\"\n",
    "        Set the character spans of the extracted pledges in the original text.\n",
    "        \"\"\"\n",
    "        setattr(self, 'spans', self.get_spans(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac34be",
   "metadata": {},
   "source": [
    "Let's get the corresponding JSON schema of this output class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02fdc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"properties\": {\n",
      "    \"pledges\": {\n",
      "      \"description\": \"A list of verbatim pledges extracted from the text. If no pledges are found, this list should be empty.\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"required\": true,\n",
      "      \"title\": \"Pledges\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"spans\": {\n",
      "      \"anyOf\": [\n",
      "        {\n",
      "          \"items\": {\n",
      "            \"maxItems\": 2,\n",
      "            \"minItems\": 2,\n",
      "            \"prefixItems\": [\n",
      "              {\n",
      "                \"type\": \"integer\"\n",
      "              },\n",
      "              {\n",
      "                \"type\": \"integer\"\n",
      "              }\n",
      "            ],\n",
      "            \"type\": \"array\"\n",
      "          },\n",
      "          \"type\": \"array\"\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"null\"\n",
      "        }\n",
      "      ],\n",
      "      \"default\": null,\n",
      "      \"title\": \"Spans\"\n",
      "    }\n",
      "  },\n",
      "  \"title\": \"PledgeExtractionOutput\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "schema = PledgeExtractionOutput.model_json_schema()\n",
    "print(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201471bc",
   "metadata": {},
   "source": [
    "It specifies the \"properties\" of an instance of the output class.\n",
    "\n",
    "- pledges: a JSON array of strings that capture the extracted passages\n",
    "- spans: a JSON array of 2-tuples that represent the start and end character possitions of a given phrase in the \"pledges\" elememnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a853cf3",
   "metadata": {},
   "source": [
    "Say given an input text, an LLM returns\n",
    "\n",
    "```\n",
    "'{\"pledges\": [\"reduce unemployment to 5%\"]}'\n",
    "```\n",
    "\n",
    "Then this JSON-formatted string can be parsed into a python object like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e5821a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PledgeExtractionOutput(pledges=['reduce unemployment to 5%'], spans=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = '{\"pledges\": [\"reduce unemployment to 5%\"]}'\n",
    "output = PledgeExtractionOutput.model_validate_json(response)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cd2e6",
   "metadata": {},
   "source": [
    "If we then pass the text from which this pledge was extracted ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a1c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Our goal is to reduce unemployment to 5% by next year.\"\n",
    "output._set_spans(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e5c89",
   "metadata": {},
   "source": [
    "... we have all the info we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda7af9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PledgeExtractionOutput(pledges=['reduce unemployment to 5%'], spans=[(15, 40)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e5623",
   "metadata": {},
   "source": [
    "Combined with the origanl input text, we can always reconstruct the extracted pledges from the character spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37613bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'Our goal is to reduce unemployment to 5% by next year.'\n",
      "pledges:\n",
      " - 'reduce unemployment to 5%'\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The spans can be used to subset the original text to get the exact pledge phrases\n",
    "print(\"text:\", repr(text))\n",
    "print(\"pledges:\")\n",
    "for span in output.spans:\n",
    "    print(\" -\", repr(text[slice(*span)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc818a",
   "metadata": {},
   "source": [
    "**_Note_:** If you don't understand why we need a custom output class, read this [blog post](https://humanloop.com/blog/structured-outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841aed24",
   "metadata": {},
   "source": [
    "### Setup the model API\n",
    "\n",
    "We will rely on LLMs served via the Hugging Face _Inference Providers_ API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13aa35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"meta-llama/Meta-Llama-3-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52ded4",
   "metadata": {},
   "source": [
    "We can use this model for **chat completion** by passing it a conversation history, that is, a list of turns between the user and assistant.\n",
    "To instruct the LLM what it should do, we pass the **task instruction prompt** as a system message.\n",
    "The user then inputs a to-be-annotated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e739bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"We will reduce carbon emissions by 50% by 2030 and achieve net-zero emissions by 2050.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text.strip()\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304bff9",
   "metadata": {},
   "source": [
    "The LLM completes this conversation with an assistant message we can parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d66c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's go through the reasoning steps to identify pledges in the given sentence:\n",
      "\n",
      "1. Read the input text carefully: \"We will reduce carbon emissions by 50% by 2030 and achieve net-zero emissions by 2050.\"\n",
      "2. Identify candidate statements: There are two statements in the sentence that might be pledges.\n",
      "3. Verify the two criteria for each candidate:\n",
      "   - \"reduce carbon emissions by 50% by 2030\":\n",
      "     - Is it specific and concrete? Yes, it mentions a measurable reduction (50%) and a specific deadline (2030).\n",
      "     - Does it express a clear commitment to act? Yes, it uses the phrase \"We will\".\n",
      "   - \"achieve net-zero emissions by 2050\":\n",
      "     - Is it specific and concrete? Yes, it mentions a measurable outcome (net-zero emissions) and a specific deadline (2050).\n",
      "     - Does it express a clear commitment to act? Yes, it uses the phrase \"We will\".\n",
      "\n",
      "Both candidate statements meet the criteria, so they are considered pledges.\n",
      "\n",
      "4. Extract the pledges verbatim from the text: \n",
      "   - \"reduce carbon emissions by 50% by 2030\"\n",
      "   - \"achieve net-zero emissions by 2050\"\n",
      "\n",
      "Output: {\"pledges\": [\"reduce carbon emissions by 50% by 2030\", \"achieve net-zero emissions by 2050\"]}\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(messages=messages)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2785e",
   "metadata": {},
   "source": [
    "⚡️ Wait! The model generates an assistant response that has much more than the JSON-formatted output we need.\n",
    "\n",
    "This is where our **custom output class** and the JSON schema it defines come into play.\n",
    "We process the JSON schema string into a suitable python object and then pass it to the `response_format` argument of the `client.chat.completions.create()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78c043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"pledges\": [\"reduce carbon emissions by 50% by 2030\", \"achieve net-zero emissions by 2050\"]}\n"
     ]
    }
   ],
   "source": [
    "json_schema = ChatCompletionInputJSONSchema(name=\"extracted pledge statements JSON schema\", schema=schema)\n",
    "response_format =ChatCompletionInputResponseFormatJSONSchema(type=\"json_schema\", json_schema=json_schema)\n",
    "\n",
    "completion = client.chat.completions.create(messages=messages, response_format=response_format)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f98c6",
   "metadata": {},
   "source": [
    "This looks much better! 💫\n",
    "\n",
    "As the generated **response adheres to out output format**, we can _parse_ it with out custom output class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3924d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PledgeExtractionOutput(pledges=['reduce carbon emissions by 50% by 2030', 'achieve net-zero emissions by 2050'], spans=[(8, 46), (51, 85)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = PledgeExtractionOutput.model_validate_json(completion.choices[0].message.content)\n",
    "output._set_spans(text)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb88519",
   "metadata": {},
   "source": [
    "## Bringing everything together in a handy custom \"extractor\" class\n",
    "\n",
    "Instead of executing the steps above one by one, we can define a class (we call it `PledgeExtractor`) that can be used to initialize a given model model to behave like a pledge extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d49e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "class PledgeExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract pledge statements from text using a language model.\n",
    "    \"\"\"\n",
    "    system_prompt = prompt # NOTE: use prompt defined above\n",
    "    response_format = response_format # NOTE: use response_format defined above\n",
    "    \n",
    "    def __init__(self, model: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the PledgeExtractor with a given model.\n",
    "\n",
    "        Args:\n",
    "            model: The name of the model to use for extraction.\n",
    "            kwargs: Additional keyword arguments to pass to the InferenceClient.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.client = InferenceClient(model=self.model, **kwargs)\n",
    "    \n",
    "    def _extract(self, text: str) -> PledgeExtractionOutput:\n",
    "        \"\"\"\n",
    "        Extract pledge statements from a single text input.\n",
    "\n",
    "        Args:\n",
    "            text: The input text from which to extract pledges.\n",
    "\n",
    "        Returns:\n",
    "            A [`PledgeExtractionOutput`] object containing the extracted pledges and their spans.\n",
    "        \"\"\"\n",
    "        \n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text.strip()}\n",
    "            ],\n",
    "            response_format=self.response_format,\n",
    "            seed=42,\n",
    "            extra_body={\"do_sample\": False}, # for deterministic output\n",
    "            temperature=0.0, # just to be save\n",
    "        )\n",
    "        output = PledgeExtractionOutput.model_validate_json(completion.choices[0].message.content)\n",
    "        output._set_spans(text)\n",
    "        return output.__dict__\n",
    "    \n",
    "    def __call__(self, texts: List[str]) -> List[PledgeExtractionOutput]:\n",
    "        \"\"\"\n",
    "        Extract pledge statements from a list of text inputs.\n",
    "        \n",
    "        Args:\n",
    "            texts: An input text or list of input texts from which to extract pledges.\n",
    "\n",
    "        Returns:\n",
    "            If the input is a list of texts, a list of `PledgeExtractionOutput` objects, one for each input text.\n",
    "            If a single string is provided, a single `PledgeExtractionOutput` object is returned.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(texts, str):\n",
    "            return self._extract(texts)\n",
    "        outputs = []\n",
    "        for text in tqdm(texts, desc=\"Extracting pledges\"):\n",
    "            outputs.append(self._extract(text))\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647aa5b",
   "metadata": {},
   "source": [
    "**_Note_:** defining the `__call__` method allows us to use the class instance similar to how we would use a function (see [here](https://www.geeksforgeeks.org/python/__call__-in-python/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c54f7",
   "metadata": {},
   "source": [
    "Now we can check the list of models available through Hugging Face Inference Providers: https://huggingface.co/inference/models\n",
    "\n",
    "Let's use a state-of-the-art large reasoning model by Qwen AI: `Qwen3-235B-A22B-Instruct-2507` \n",
    "\n",
    "**_Important_:** Given that we want to generate _structure_ outputs, we can only use models that have \"Yes\" in the lst column of this table (\"Structured\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb9e8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: check for availanble models and status here: https://huggingface.co/inference/models\n",
    "model_id = \"Qwen/Qwen3-235B-A22B-Instruct-2507\"\n",
    "pledge_extractor = PledgeExtractor(model=model_id, provider=\"together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82cc9037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pledges': ['reduce carbon emissions by 50% by 2030',\n",
       "  'achieve net-zero emissions by 2050'],\n",
       " 'spans': [(8, 46), (51, 85)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pledge_extractor(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139869d",
   "metadata": {},
   "source": [
    "## Scale the annotation task to multiple texts\n",
    "\n",
    "We now have a custom extractor class that allows us to annotate multiple texts.\n",
    "\n",
    "**_Note_:** Below we process texts sequentially (i.e., send texts one at a time to the API). batch processing - a better alternative - is beyond the scope of this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe80df",
   "metadata": {},
   "source": [
    "Let's load some suitable sentence-level data from [Fornaciari et al. 2021](https://aclanthology.org/2021.findings-acl.301/).\n",
    "\n",
    "Their data has been human-labeled into two categories: sentences with and sentences without pledge(s).\n",
    "(Here, we use a balanced subsample of the data.)\n",
    "Our extraction approach is more granular but having some labels is better than nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91c5a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_path = Path(\"../../data/labeled/fornaciari_we_2021\")\n",
    "\n",
    "fp = data_path / \"annotation_set_01.csv\"\n",
    "df = pd.read_csv(fp)\n",
    "\n",
    "# get the text column into a list of strings\n",
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d93701",
   "metadata": {},
   "source": [
    "We can not pass the texts to out extractor initialized above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f8506b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20604e2b157447a8fc72ace7136d9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting pledges:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = pledge_extractor(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3cd1cd",
   "metadata": {},
   "source": [
    "In this run, we annotate these 50 sentence in 37 seconds.\n",
    "This is likely much quicker than a human annotator would take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d20f74",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We use the sentence-level labels in for this sample to evaluate our LLM annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6634e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of dicts into a dataframe and merge with the original dataframe\n",
    "outputs_df = pd.Series(outputs).apply(pd.Series)\n",
    "# add the annotations a new columns to the original dataframe\n",
    "df[outputs_df.columns] = outputs_df\n",
    "# binarize the annotations into sentence-level labels\n",
    "df['pred'] = df.pledges.apply(lambda x: len(x)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b45211",
   "metadata": {},
   "source": [
    "With binary observed and predicted labels at hand, we can compute standard (binary) classification metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d6b2e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.96      0.86        25\n",
      "           1       0.95      0.72      0.82        25\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.86      0.84      0.84        50\n",
      "weighted avg       0.86      0.84      0.84        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df['label'], df['pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35354b4b",
   "metadata": {},
   "source": [
    "Even with this very concise prompt and only 5 [few-shot examples](https://learnprompting.org/vocabulary/few-shot_prompting), we achieve a solid F1! 🥳"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e4f72",
   "metadata": {},
   "source": [
    "But still, there are some \"errors\" according to the \"true\" labels recorded in the original data.\n",
    "Let's look at a sample of these instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77f80e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: '11 . We will strengthen the legal and institutional framework to\n",
      "        protect our children .'\n",
      "Label: False; Pred: True\n",
      "Pledges: ['strengthen the legal and institutional framework to protect our children']\n",
      "\n",
      "Text: 'The Total Sanitation Campaign, launched by the NDA Government in 1999,\n",
      "        has been a remarkable success .'\n",
      "Label: True; Pred: False\n",
      "\n",
      "Text: 'Immediately after forming the governments in Chhattisgarh, Madhya\n",
      "        Pradesh and Rajasthan, as promised, the 3 Congress Governments\n",
      "        waived the loans of farmers .'\n",
      "Label: True; Pred: False\n",
      "\n",
      "Text: 'In consonance with its policy, the BJP supports the creation of\n",
      "        Telangana as a separate State of the Union of India .'\n",
      "Label: True; Pred: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "error_sample = df.query(\"label != pred\").groupby('label')[['label', 'pred', 'text', 'pledges']].apply(lambda x: x.sample(min(len(x), 3), random_state=42), include_groups=True).reset_index(drop=True)\n",
    "for i, row in error_sample.iterrows():\n",
    "    print(f\"Text: '{textwrap.fill(row['text'], width=70, subsequent_indent=' '*8)}'\")\n",
    "    print(f\"Label: {bool(row['label'])}; Pred: {row['pred']}\")\n",
    "    if row['pred']:\n",
    "        print(f\"Pledges: {row['pledges']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69f39f",
   "metadata": {},
   "source": [
    "The first example is a _false positive_ that captures a candidate pledge that does _not_ satisfy the criterion of measurability of the promised outcome.\n",
    "But it certainly is a conceptual boundary case.\n",
    "\n",
    "Yet, the among the three examples of _supposed_ false negative, the first two are arguably incorrectly labeled in the \"ground truth\" data.\n",
    "Both are past-oriented which disqualifies them as pledges.\n",
    "\n",
    "This suggests that the LLM's performance might be _underestimated_ and it raises the issue of \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cffaf8",
   "metadata": {},
   "source": [
    "### Export as annotations\n",
    "\n",
    "We can save these annotations to disk (including metadata) for later use (e.g., token classifier fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9b8e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_jsonlines(df, fp, cols=[\"text\", \"spans\"], metadata_exclude=('metadata', 'pred', 'pledges')):\n",
    "    # gather all columns not in cols in metadata dictionary\n",
    "    metadata_cols = [col for col in df.columns if col not in cols if col not in metadata_exclude]\n",
    "    metadata = df[metadata_cols].to_dict(orient=\"records\")\n",
    "    out = df[cols].copy()\n",
    "    out['metadata'] = metadata\n",
    "    out.rename(columns={\"spans\": \"label\"}, inplace=True)\n",
    "    out.to_json(fp, orient=\"records\", lines=True, index=False, force_ascii=False, default_handler=str)\n",
    "\n",
    "annotations_dir = data_path / \"annotations\" / \"extraction\"\n",
    "annotations_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "to_jsonlines(df, annotations_dir / f\"{model_id.split('/')[-1]}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d56597",
   "metadata": {},
   "source": [
    "## Run with multiple models\n",
    "\n",
    "One of the coolest things of using LLMs through Hugging Face _Inference Providers_ is the large number of models that are readily avaliable for use.\n",
    "This invites experimentation and [ensemble approaches](https://arxiv.org/abs/2502.18036) to LLM annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7cbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"meta-llama/Llama-4-Maverick-17B-128E-Instruct\", \"sambanova\"),\n",
    "    (\"openai/gpt-oss-120b\", \"nebius\"),\n",
    "    (\"deepseek-ai/DeepSeek-V3-0324\", \"sambanova\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9be6b2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: openai/gpt-oss-120b (provider: nebius)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e13abe97cc4212974aeaaae92b37da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting pledges:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.96      0.80        25\n",
      "           1       0.93      0.56      0.70        25\n",
      "\n",
      "    accuracy                           0.76        50\n",
      "   macro avg       0.81      0.76      0.75        50\n",
      "weighted avg       0.81      0.76      0.75        50\n",
      "\n",
      "Processing model: deepseek-ai/DeepSeek-V3-0324 (provider: sambanova)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60581e645044144b59db01ab76e07ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting pledges:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89        25\n",
      "           1       1.00      0.76      0.86        25\n",
      "\n",
      "    accuracy                           0.88        50\n",
      "   macro avg       0.90      0.88      0.88        50\n",
      "weighted avg       0.90      0.88      0.88        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: inside the loop, we just  repeat the logic above for each model\n",
    "for model_id, provider in models:\n",
    "    print(f\"Processing model: {model_id} (provider: {provider})\")\n",
    "    pledge_extractor = PledgeExtractor(model=model_id, provider=provider)\n",
    "\n",
    "    outputs = pledge_extractor(texts)\n",
    "    outputs_df = pd.Series(outputs).apply(pd.Series)\n",
    "    df[outputs_df.columns] = outputs_df\n",
    "    df['pred'] = df.pledges.apply(lambda x: len(x)>0)\n",
    "    \n",
    "    print(classification_report(df['label'], df['pred'], zero_division=0))\n",
    "    \n",
    "    to_jsonlines(df, annotations_dir / f\"{model_id.split('/')[-1]}.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
