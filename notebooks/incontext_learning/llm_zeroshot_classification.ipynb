{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025017f5",
   "metadata": {},
   "source": [
    "# Zero-shot text classification with LLMs\n",
    "\n",
    "This notebook illustrates how to use different LLMs for text classification.\n",
    "\n",
    "- closed-source LLMs models by OpenAI\n",
    "- open-weights model hosted via Hugging Face Inference Providers/Endpoints\n",
    "- open-weights LLMs models with `ollama`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5afba0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da313a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from src.utils.io import read_tabular\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f56a5d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c7d6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False # no support for colab yet\n",
    "base_path = Path(\"/content/advanced_text_analysis/\" if COLAB else \"../../\")\n",
    "data_path = base_path / \"data\" / \"labeled\" / \"benoit_crowdsourced_2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8fc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (down)load the data\n",
    "fp = data_path / \"benoit_crowdsourced_2016-policy_area.csv\"\n",
    "if not fp.exists():\n",
    "    url = \"https://cta-text-datasets.s3.eu-central-1.amazonaws.com/labeled/\" + fp.parent.name + '/' + fp.name\n",
    "    df = pd.read_csv(url)\n",
    "    fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(fp, index=False)\n",
    "\n",
    "df = read_tabular(fp, columns=['uid', 'text', 'label', 'metadata__gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5990bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to gold examples (i.e., those labeled by experts)\n",
    "df = df[df.metadata__gold]\n",
    "del df['metadata__gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8944ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "economic    225\n",
      "neither     181\n",
      "social      100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    2: 'economic',\n",
    "    3: 'social',\n",
    "    1: 'neither',\n",
    "}\n",
    "df['label'] = df.label.map(id2label)\n",
    "\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562dda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get five examples per label class\n",
    "expls = df.groupby('label').sample(20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df9b2b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e11918",
   "metadata": {},
   "source": [
    "## Define the task\n",
    "\n",
    "In this example, we adapt the instruction for one of the tweet classification tasks examined in Benoit et al. ([2016](https://doi.org/10.1017/S0003055416000058)) \"Crowd-sourced Text Analysis: Reproducible and Agile Production\n",
    "of Political Data\"\n",
    "\n",
    "- see [this README file](../../data/labeled/benoit_crowdsourced_2016/README.md) for a description of the data and tasks covered in the paper\n",
    "- see [this file](../../data/labeled/benoit_crowdsourced_2016/instructions/econ_social_policy.md) for a copy of their original task instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "903e86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f\"\"\"\n",
    "Act as a text classification system versatile in performing content analysis.\n",
    "\n",
    "You will read a sentence from a political text.\n",
    "Yout will judge whether this sentence deals with economic or social policy.\n",
    "You must classify the sentence into one of the following categories: \"economic\", \"social\", or \"neither\". \n",
    "\n",
    "## Definitions\n",
    "\n",
    "These categories have the following definitions:\n",
    "\n",
    "- Sentences should be coded as \"economic\" if they deal with aspects of the economy, such as: Taxation, Government spending, Services provided by the government or other public bodies, Pensions, unemployment and welfare benefits, and other state benefits, Property, investment and share ownership, public or private, Interest rates and exchange rates, Regulation of economic activity, public or private, Relations between employers, workers and trade unions\n",
    "- Sentences should be coded as \"social\" if they deal with aspects of social and moral life, relationships between social groups, and matters of national and social identity. These include: Policing, crime, punishment and rehabilitation of offenders; Immigration, relations between social groups, discrimination and multiculturalism; The role of the state in regulating the social and moral behavior of individuals\n",
    "\n",
    "## Step-by-step instructions\n",
    "\n",
    "Follow these steps to classify the sentence:\n",
    "\n",
    "1. Carefully read the text of the sentence, paying close attention to details.\n",
    "2. Assess whether the sentence belongs to any of the categories. If not, return 'neither' as your response.\n",
    "3. Classify the sentence with the category it belongs to. Return only the name of the category.\n",
    "\n",
    "## Response format\n",
    "\n",
    "Only include the selected category in your response and no further text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95844713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They are no longer content that some of the most important decisions in their lives what school their children attend, for example, or whether or not to go on strike should be taken by officialdom or trade union bosses.',\n",
       " 'Any extra burden on business will destroy jobs.',\n",
       " 'We will increase the bonus by paying a double pension in the first week of December.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = expls.text.to_list()\n",
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773717a",
   "metadata": {},
   "source": [
    "## With ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90464e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "MODEL = 'gpt-4o-2024-08-06'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d1d75",
   "metadata": {},
   "source": [
    "#### illustration with a _single_ sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e912bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And that Labour's much-vaunted pay pact with the unions collapsed in the industrial anarchy of the winter of discontent, n which the dead went unburied, rubbish piled up in the streets and the country was gripped by a creeping paralysis which Labour was powerless to cure?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'economic'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.text.iloc[5]\n",
    "print(text)\n",
    "\n",
    "messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d778cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'economic'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596973d",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373b62c",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fb21f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, model):\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "  ]\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    "  )\n",
    "  \n",
    "  result = response.choices[0].message.content\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33e7a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o-2024-08-06'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)\n",
    "MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6788aed",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bee84cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b536372ce11e4eaaa83075e6554a6e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifications_gpt4o = [\n",
    "    classify_text(text, instructions, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc6a2191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       0.95      0.90      0.92        20\n",
      "     neither       1.00      0.90      0.95        20\n",
      "      social       0.87      1.00      0.93        20\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.94      0.93      0.93        60\n",
      "weighted avg       0.94      0.93      0.93        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr = classification_report(\n",
    "    y_true=expls.label,\n",
    "    y_pred=classifications_gpt4o,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106fc12",
   "metadata": {},
   "source": [
    "\n",
    "#### Caveate {style=\"color: orange\"}\n",
    "\n",
    "The annoying thing about OpenAI is that their models are closed-source, meaning we have no access to them.\n",
    "This limits reproducibility (see Palmer et al. [2024](https://www.nature.com/articles/s43588-023-00585-1)).\n",
    "\n",
    "So instead of relying them, we can use \"open-weights\" models.\n",
    "These are models for which we can freely download the model weights (i.e., paramters).\n",
    "We examine two options below: \n",
    "\n",
    "1. using Hugging Face _Inference Providers_ (via API)\n",
    "2. using Ollama (run locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd7288",
   "metadata": {},
   "source": [
    "## With Hugging Face _Inference Providers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de141bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "MODEL = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "client = InferenceClient(MODEL, token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc7e19",
   "metadata": {},
   "source": [
    "the **cool thing** is that the `InferenceClient` works exactly like the `openai.Client` class.\n",
    "So the code from above really _doesn't change_!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9161708",
   "metadata": {},
   "source": [
    "#### illustration with a _single_ sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "227681fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And that Labour's much-vaunted pay pact with the unions collapsed in the industrial anarchy of the winter of discontent, n which the dead went unburied, rubbish piled up in the streets and the country was gripped by a creeping paralysis which Labour was powerless to cure?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'social'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.text.iloc[5]\n",
    "print(text)\n",
    "\n",
    "messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4d1fdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'economic'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9781c",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c385588",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d50b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, model):\n",
    "  # NOTE: `model` actually not needed because we setup the InferenceClient with the model already\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "  ]\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    "  )\n",
    "  \n",
    "  result = response.choices[0].message.content\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c57f5a",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cff31c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a547ad183e4e7583d8c873ac84c944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifications_llama3_70b = [\n",
    "    classify_text(text, instructions, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47463ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       0.85      0.85      0.85        20\n",
      "     neither       1.00      0.70      0.82        20\n",
      "      social       0.77      1.00      0.87        20\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.87      0.85      0.85        60\n",
      "weighted avg       0.87      0.85      0.85        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr = classification_report(\n",
    "    y_true=expls.label,\n",
    "    y_pred=classifications_llama3_70b,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8cf96",
   "metadata": {},
   "source": [
    "Alright, the performance is slightly lower but this is only one of many available models.\n",
    "\n",
    "What if we try the very famous R1 model from DeepSeek?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8996dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL= \"deepseek-ai/DeepSeek-V3-0324\"\n",
    "\n",
    "client = InferenceClient(MODEL, provider=\"sambanova\", token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "536fd670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'social'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_text(text=df.text.iloc[5], system_message=instructions, model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7833e949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdff44b02bab46d7a66ba115973da6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifications_deepseekR1 = [\n",
    "    classify_text(text, instructions, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4823f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "classification_report(\n",
      "    y_true,\n",
      "    y_pred,\n",
      "    *,\n",
      "    labels=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    target_names=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    digits=\u001b[32m2\u001b[39m,\n",
      "    output_dict=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    zero_division=\u001b[33m'warn'\u001b[39m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Build a text report showing the main classification metrics.\n",
      "\n",
      "Read more in the :ref:`User Guide <classification_report>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "    Ground truth (correct) target values.\n",
      "\n",
      "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "    Estimated targets as returned by a classifier.\n",
      "\n",
      "labels : array-like of shape (n_labels,), default=None\n",
      "    Optional list of label indices to include in the report.\n",
      "\n",
      "target_names : array-like of shape (n_labels,), default=None\n",
      "    Optional display names matching the labels (same order).\n",
      "\n",
      "sample_weight : array-like of shape (n_samples,), default=None\n",
      "    Sample weights.\n",
      "\n",
      "digits : int, default=2\n",
      "    Number of digits for formatting output floating point values.\n",
      "    When ``output_dict`` is ``True``, this will be ignored and the\n",
      "    returned values will not be rounded.\n",
      "\n",
      "output_dict : bool, default=False\n",
      "    If True, return output as dict.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "\n",
      "zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "    Sets the value to return when there is a zero division. If set to\n",
      "    \"warn\", this acts as 0, but warnings are also raised.\n",
      "\n",
      "    .. versionadded:: 1.3\n",
      "       `np.nan` option was added.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "report : str or dict\n",
      "    Text summary of the precision, recall, F1 score for each class.\n",
      "    Dictionary returned if output_dict is True. Dictionary has the\n",
      "    following structure::\n",
      "\n",
      "        {'label 1': {'precision':0.5,\n",
      "                     'recall':1.0,\n",
      "                     'f1-score':0.67,\n",
      "                     'support':1},\n",
      "         'label 2': { ... },\n",
      "          ...\n",
      "        }\n",
      "\n",
      "    The reported averages include macro average (averaging the unweighted\n",
      "    mean per label), weighted average (averaging the support-weighted mean\n",
      "    per label), and sample average (only for multilabel classification).\n",
      "    Micro average (averaging the total true positives, false negatives and\n",
      "    false positives) is only shown for multi-label or multi-class\n",
      "    with a subset of classes, because it corresponds to accuracy\n",
      "    otherwise and would be the same for all metrics.\n",
      "    See also :func:`precision_recall_fscore_support` for more details\n",
      "    on averages.\n",
      "\n",
      "    Note that in binary classification, recall of the positive class\n",
      "    is also known as \"sensitivity\"; recall of the negative class is\n",
      "    \"specificity\".\n",
      "\n",
      "See Also\n",
      "--------\n",
      "precision_recall_fscore_support: Compute precision, recall, F-measure and\n",
      "    support for each class.\n",
      "confusion_matrix: Compute confusion matrix to evaluate the accuracy of a\n",
      "    classification.\n",
      "multilabel_confusion_matrix: Compute a confusion matrix for each class or sample.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.metrics import classification_report\n",
      ">>> y_true = [0, 1, 2, 2, 2]\n",
      ">>> y_pred = [0, 0, 2, 2, 1]\n",
      ">>> target_names = ['class 0', 'class 1', 'class 2']\n",
      ">>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "              precision    recall  f1-score   support\n",
      "<BLANKLINE>\n",
      "     class 0       0.50      1.00      0.67         1\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.67      0.80         3\n",
      "<BLANKLINE>\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "<BLANKLINE>\n",
      ">>> y_pred = [1, 1, 0]\n",
      ">>> y_true = [1, 1, 1]\n",
      ">>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "              precision    recall  f1-score   support\n",
      "<BLANKLINE>\n",
      "           1       1.00      0.67      0.80         3\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "<BLANKLINE>\n",
      "   micro avg       1.00      0.67      0.80         3\n",
      "   macro avg       0.33      0.22      0.27         3\n",
      "weighted avg       1.00      0.67      0.80         3\n",
      "<BLANKLINE>\n",
      "\u001b[31mFile:\u001b[39m      ~/miniforge/envs/advanced_text_analysis/lib/python3.11/site-packages/sklearn/metrics/_classification.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "classification_report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c08384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['economic', 'social', 'neither'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cb2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       1.00      0.90      0.95        20\n",
      "      social       0.69      1.00      0.82        20\n",
      "     neither       1.00      0.60      0.75        20\n",
      "\n",
      "   micro avg       0.85      0.83      0.84        60\n",
      "   macro avg       0.90      0.83      0.84        60\n",
      "weighted avg       0.90      0.83      0.84        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr = classification_report(\n",
    "    y_true=expls.label,\n",
    "    y_pred=classifications_deepseekR1,\n",
    "    labels=list(id2label.values())\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563c5c6",
   "metadata": {},
   "source": [
    "Well this didn't get any better but we could try other models very flexible, see [here](https://huggingface.co/inference/models) for available models by different providers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845ec56",
   "metadata": {},
   "source": [
    "## With Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4e180bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client()\n",
    "MODEL = 'gemma3:4b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d16d5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deepseek-r1:32b',\n",
       " 'gemma3:27b',\n",
       " 'sikamikanikobg/OlympicCoder-7B:latest',\n",
       " 'qwq:latest',\n",
       " 'qwq:32b',\n",
       " 'llama3.3:70b',\n",
       " 'mistral-small:latest',\n",
       " 'mistral-small:24b',\n",
       " 'qwen2.5:32b',\n",
       " 'llama3.1:8b',\n",
       " 'aya:35b',\n",
       " 'llama3.3:latest',\n",
       " 'mxbai-embed-large:335m',\n",
       " 'phi4:latest',\n",
       " 'phi4:14b']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list models\n",
    "available_models = [m['model'] for m in client.list()['models']]\n",
    "available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef13cd72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "pull model manifest: 412: \n\nThe model you are attempting to pull requires a newer version of Ollama.\n\nPlease download the latest version at:\n\n\thttps://ollama.com/download\n\n (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m MODEL \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_models:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge/envs/advanced_text_analysis/lib/python3.11/site-packages/ollama/_client.py:434\u001b[39m, in \u001b[36mClient.pull\u001b[39m\u001b[34m(self, model, insecure, stream)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpull\u001b[39m(\n\u001b[32m    423\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    424\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    427\u001b[39m   stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    428\u001b[39m ) -> Union[ProgressResponse, Iterator[ProgressResponse]]:\n\u001b[32m    429\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[33;03m  Raises `ResponseError` if the request could not be fulfilled.\u001b[39;00m\n\u001b[32m    431\u001b[39m \n\u001b[32m    432\u001b[39m \u001b[33;03m  Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mProgressResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/pull\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPullRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m      \u001b[49m\u001b[43minsecure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minsecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge/envs/advanced_text_analysis/lib/python3.11/site-packages/ollama/_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge/envs/advanced_text_analysis/lib/python3.11/site-packages/ollama/_client.py:124\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    126\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: pull model manifest: 412: \n\nThe model you are attempting to pull requires a newer version of Ollama.\n\nPlease download the latest version at:\n\n\thttps://ollama.com/download\n\n (status code: 500)"
     ]
    }
   ],
   "source": [
    "if MODEL not in available_models:\n",
    "    import ollama\n",
    "    ollama.pull(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f385a9d",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdacff",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, model):\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "  ]\n",
    "\n",
    "  # set some options controlling generation behavior\n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  opts = {\n",
    "      'seed': 42,         # seed controlling random number generation and thus stochastic generation\n",
    "      'temperature': 0.0, # hyper parameter controlling \"craetivity\", see https://learnprompting.org/docs/basics/configuration_hyperparameters\n",
    "      'max_tokens': 3     # maximum numbers of tokens to generate in completion\n",
    "  }\n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  response = client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    options=opts\n",
    "  )\n",
    "  \n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  result = response.message.content.strip()\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first call, it migth take some tome because the model needs to be loaded first\n",
    "classify_text(texts[5], instructions, MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627de57f",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_gemma3_4b = [\n",
    "    classify_text(text, instructions, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69172cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=expls.label,\n",
    "    y_pred=classifications_gemma3_4b,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647dd2c0",
   "metadata": {},
   "source": [
    "## With `transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37404fbc",
   "metadata": {},
   "source": [
    "**_Caveat:_** We can only use a very small LLM for illustrative purposes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac366c3",
   "metadata": {},
   "source": [
    "Note: on CUDA GPU you can alos use quantization:\n",
    "\n",
    "```pyhton\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(..., quantization_config=bnb_config, ...)\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9d58701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "\n",
    "# load the model and tokenizer\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, pad_token_id=tokenizer.eos_token_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "497e1d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And that Labour's much-vaunted pay pact with the unions collapsed in the industrial anarchy of the winter of discontent, n which the dead went unburied, rubbish piled up in the streets and the country was gripped by a creeping paralysis which Labour was powerless to cure?\n"
     ]
    }
   ],
   "source": [
    "text = df.text.iloc[5]\n",
    "print(text)\n",
    "\n",
    "messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea12272",
   "metadata": {},
   "source": [
    "We first need to apply the chat template so we can perfrom [chat completion](https://huggingface.co/docs/inference-providers/en/tasks/chat-completion) instead of mere text generation/completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cabaea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "\n",
      "Act as a text classification system versatile in performing content analysis.\n",
      "\n",
      "You will read a sentence from a political text.\n",
      "Yout will judge whether this sentence deals with economic or social policy.\n",
      "You must classify the sentence into one of the following categories: \"economic\", \"social\", or \"neither\". \n",
      "\n",
      "## Definitions\n",
      "\n",
      "These categories have the following definitions:\n",
      "\n",
      "- Sentences should be coded as \"economic\" if they deal with aspects of the economy, such as: Taxation, Government spending, Services provided by the government or other public bodies, Pensions, unemployment and welfare benefits, and other state benefits, Property, investment and share ownership, public or private, Interest rates and exchange rates, Regulation of economic activity, public or private, Relations between employers, workers and trade unions\n",
      "- Sentences should be coded as \"social\" if they deal with aspects of social and moral life, relationships between social groups, and matters of national and social identity. These include: Policing, crime, punishment and rehabilitation of offenders; Immigration, relations between social groups, discrimination and multiculturalism; The role of the state in regulating the social and moral behavior of individuals\n",
      "\n",
      "## Step-by-step instructions\n",
      "\n",
      "Follow these steps to classify the sentence:\n",
      "\n",
      "1. Carefully read the text of the sentence, paying close attention to details.\n",
      "2. Assess whether the sentence belongs to any of the categories. If not, return 'neither' as your response.\n",
      "3. Classify the sentence with the category it belongs to. Return only the name of the category.\n",
      "\n",
      "## Response format\n",
      "\n",
      "Only include the selected category in your response and no further text.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "And that Labour's much-vaunted pay pact with the unions collapsed in the industrial anarchy of the winter of discontent, n which the dead went unburied, rubbish piled up in the streets and the country was gripped by a creeping paralysis which Labour was powerless to cure?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_messages = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(chat_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66dc6e",
   "metadata": {},
   "source": [
    "As you see, this just converts the list of messages into text by adding special tokens that demarcate text by the assistant and user.\n",
    "\n",
    "Next, we need to tokenizer this input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a99997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(chat_messages, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device) # move to same device as model (GPU if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98988144",
   "metadata": {},
   "source": [
    "The tokenized inputs can be processed through the model to generate a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab176f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=4, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d243be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = inputs['input_ids'].shape[1]\n",
    "response = tokenizer.decode(outputs[0][offset:].cpu(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da6ae833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'economical'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8da61",
   "metadata": {},
   "source": [
    "## Inter-LLM agreement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9289c",
   "metadata": {},
   "source": [
    "What if we consider the different LLM's classifications as annotations?\n",
    "Then we compute see the degree of their inter-annotator agreement (ICA).\n",
    "\n",
    "This is equivalent to what we did in the [notebook](../annotation/compute_ica_pledge_classification.ipynb) on computing ICA in our policy pledge codings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d27bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from krippendorff import alpha\n",
    "\n",
    "tmp = pd.DataFrame({\n",
    "    'gpt4o': classifications_gpt4o,\n",
    "    'gemma3_4b': classifications_gemma3_4b,\n",
    "    'llama3_70b': classifications_llama3_70b,\n",
    "    'deepseekR1': classifications_deepseekR1,\n",
    "})\n",
    "\n",
    "label2id = {\n",
    "    'economic': 0,\n",
    "    'social': 1,\n",
    "    'neither': 2,\n",
    "}\n",
    "\n",
    "tmp = tmp.apply(lambda x: x.map(label2id))\n",
    "alpha(tmp.T.values, level_of_measurement='nominal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dcec6e",
   "metadata": {},
   "source": [
    "😳 This is a very strong agreement between LLMs' classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e20732",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
