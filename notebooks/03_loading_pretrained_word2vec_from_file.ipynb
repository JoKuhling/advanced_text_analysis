{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word vectors from text files\n",
    "\n",
    "\n",
    "| Author | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2023-09-27 |\n",
    "\n",
    "This notebook illustrates how to use `gensim` to load pre-train word vectors.\n",
    "\n",
    "## Intro\n",
    "\n",
    "Sometimes you will find a resource online that provides word vectors in the form of a text file.\n",
    "For example, the German organization *Deepset* provides download links to word2vec embeddings they have trained on German-language Wikipedia entries.\n",
    "\n",
    "In cases like this one, we need read the word embeddings from a text file and convert them to a `gensim` `KeyedVectors` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "The absolute minimum information you need to use pre-trained word embddings are \n",
    "\n",
    "1. the vocabulary\n",
    "2. the word vectors\n",
    "3. which vector belong to which word in the vocabulary\n",
    "\n",
    "Ways to share this information with others (in increasing levels of ):\n",
    "\n",
    "1. Just save the `KeyedVectors` object as a serialized (binary) file. Then the file can be loaded in python with `KeyedVectors.load(file, binary=True)`\n",
    "2. Save the `KeyedVectors` in the standard word2vec format: \n",
    "    - The first lines records the vocabulary size $v$ and the number of embedding dimensions $d$ (numbers separated by white space).\n",
    "    - All following lines record, first, the respective word and then its $d$ embedding values &mdash; all separated by a white space\n",
    "3. Save the vocabulary and the word vectors in different files. In this case, we assume that the first word vector corresponds to the first word in the vocabulary file.\n",
    "\n",
    "Regarding file formats, people use a number of different options\n",
    "\n",
    "1. Files saved with Option 1 (i.e., with `KeyedVectors.save`) are usually just written into a file with the extension '.kv' (or sometimes '.vectors').\n",
    "2. Files saved with Option 1 are ususally named with the file extension '.vectors'. If you can't just view the file in a text editor (try `head -n 1 <file-path>`in Mac's Terminal or `more /n 1 <file-path>` in Windows' command prompt), it's save as a binary file (see below)\n",
    "3. Vocab files are typically saved as '.txt' files (just read with `pandas` or native python) or '.pkl' (read with python's `pickle` module). Word vector files are saved as '.txt' files, as '.pkl', or as '.npy' (read with `numpy`'s `load()` method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "data_path = os.path.join('..', 'data', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for file download\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# write a function to download the file to a local file path\n",
    "def download_file(url, fp):\n",
    "    os.makedirs(os.path.dirname(fp), exist_ok=True)\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(fp, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Load our *UK Commons* word embeddings\n",
    "\n",
    "When we save the vectors of the word2vec model we trained on *UK Houes of Commons* speeches, we used the `KeyedVectors` `save()` methdod.\n",
    "\n",
    "So we can just `load()` them again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fp = os.path.join(data_path, 'gbr_commons', 'gbr_commons_word2vec_w5_d100.kv')\n",
    "vectors = KeyedVectors.load(fp)\n",
    "type(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: The *Google News Corpus* word2vec model\n",
    "\n",
    "Remeber how I told you that after downloading the 'word2vec-google-news-300' with `gensim`'s downloader module, it's saved on our computer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hlicht/gensim-data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['word2vec-google-news-300', 'information.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(api.BASE_DIR)\n",
    "os.listdir(api.BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll the 'word2vec-google-news-300' folder in this location has a gzipped file that contains the word vectors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word2vec-google-news-300.gz', '__init__.py', '__pycache__']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'word2vec-google-news-300'\n",
    "os.listdir(os.path.join(api.BASE_DIR, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** `gensim` stores it there with the `load_word2vec_format()` method the first time you download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can just load this file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join(api.BASE_DIR, model_name, model_name+'.gz')\n",
    "vectors = KeyedVectors.load_word2vec_format(fp, binary=True)\n",
    "type(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: German word embeddings\n",
    "\n",
    "The German word embeddings at https://devmount.github.io/GermanWordEmbeddings are an example of Option 1.\n",
    "\n",
    "The file we are interested in is called `'german.model'`!\n",
    "\n",
    "To find out that this file has been saved in the word2vec format, I had to find the relevant [line of code](https://github.com/devmount/GermanWordEmbeddings/blob/662af36834643963f5d10485a3d86e186a9ead17/training.py#L65) used to train the model.\n",
    "There you'll see that the developer has saved the model's word vectors using gensim's `save_word2vec_format()` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first download the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file URL\n",
    "url = 'https://cloud.devmount.de/d2bc5672c523b086/german.model'\n",
    "\n",
    "# file destination on our computer\n",
    "model_dir = os.path.join(data_path, 'german_word_embeddings')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "fp = os.path.join(model_dir, 'word2vec.model')\n",
    "# note: we gonna name the file with the extension .kv\n",
    "#        so in the future we'll know that it records \n",
    "#        gensim KeyedVectors\n",
    "\n",
    "download_file(url, fp) # takes 0.5-1 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word vectors from file\n",
    "from gensim.models import KeyedVectors\n",
    "vectors = KeyedVectors.load_word2vec_format(fp, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30662322"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similarity('Haus', 'Maus') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Deepset German word embeddings\n",
    "\n",
    "source: https://www.deepset.ai/german-word-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first download the text file recording the embeddings from the link they provide on their website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://int-emb-word2vec-de-wiki.s3.eu-central-1.amazonaws.com/vectors.txt'\n",
    "\n",
    "model_dir = os.path.join(data_path, 'deepnet_german_word2vec')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "fp = os.path.join(model_dir, 'vectors.txt')\n",
    "\n",
    "download_file(url, fp) # takes 3-4 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first line of the tex file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'UNK' -0.07903 0.01641 0.006979 -0.035038 0.006474 0.002469 -0.050103 0.142654 -0.03505 0.003106 -0\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the first line\n",
    "with open(fp, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        break\n",
    " # print first 100 characters in the line\n",
    "line[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the first entry in the line is the word (`UNK``) and then all subsequent entries are numbers.\n",
    "The numebrs are the word's embeddding.\n",
    "\n",
    "*Note:* You might also have notived that the word looks a little weird (`b'UNK'` -- we'll deal with this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we need to do is iterate over all lines in the file and for each get the word and its corresponding embedding.\n",
    "Ther code below does just that, storing all embeddings in a standard python dictionary and already converting each embedding to a 1-dimensional numpy array (i.e., vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17fca82c5a4439aba2412cef65f2ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/854776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm # for progress bar\n",
    "\n",
    "vectors = {}\n",
    "n_lines = sum(1 for l in open(fp, 'r') )\n",
    "with open(fp, 'r') as file:\n",
    "    for line in tqdm(file, total=n_lines):\n",
    "        # split entries at white spaces\n",
    "        line = line.split(sep=' ')\n",
    "        # get the word (first entry)\n",
    "        k = line[0]\n",
    "        # convert embedding to 1-d numpy array and add to dict\n",
    "        vectors[k] = np.array(line[1:], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 words in the embeddings dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'UNK'\",\n",
       " \"b'der'\",\n",
       " \"b'und'\",\n",
       " \"b'die'\",\n",
       " \"b'in'\",\n",
       " \"b'von'\",\n",
       " \"b'im'\",\n",
       " \"b'des'\",\n",
       " \"b'den'\",\n",
       " \"b'kategorie'\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectors.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apprantly the creators of the text file have messed up the encoding a little (see https://stackoverflow.com/a/53730346).\n",
    "So' well fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the next line is typically not necessary\n",
    "vectors = {ast.literal_eval(k).decode(): v for k, v in vectors.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a `KeyedVectors` object from our dictionary of word--embedding pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# create a gensim KeyedVectors object from the dictionary of word vectors\n",
    "kv = KeyedVectors(vector_size=vectors['UNK'].shape[0])\n",
    "kv.add_vectors(list(vectors.keys()), list(vectors.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it works just right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6345489"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.similarity('die', 'der')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save it as binary file to disk (in word2vec format) so it's easier to load it again sometime later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "kv.save_word2vec_format(fp.replace('.txt', '.kv'), binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Option 3\n",
    "\n",
    "One of your fellow course participants pointed out that Stanford NLP provides word embeddings trained on the historical COHA corpus https://www.english-corpora.org/coha/).\n",
    "\n",
    "The emebddings are available for download here: https://nlp.stanford.edu/projects/histwords/\n",
    "\n",
    "For example, the link underlying \"All English (1800s-1990s)\" (`http://snap.stanford.edu/historical_embeddings/eng-all_sgns.zip`) will dowload a very large zip file.\n",
    "\n",
    "IF you unzip this file, the `sgns/` folder will record a number of files:\n",
    "\n",
    "- '1800-vocab.pkl'\n",
    "- '1800-w.npy'\n",
    "- '1810-vocab.pkl'\n",
    "- ...\n",
    "\n",
    "In my understanding, these fildes correspond to a bunch of decade-specific word embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://snap.stanford.edu/historical_embeddings/eng-all_sgns.zip'\n",
    "fp = os.path.join(data_path, 'sgns.zip')\n",
    "download_file(url, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "model_dir = fp.removesuffix('.zip')\n",
    "with zipfile.ZipFile(fp, 'r') as f:\n",
    "    f.extractall(model_dir)\n",
    "\n",
    "# Step 2: Remove the archive file\n",
    "os.remove(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 300)\n",
      "100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'to', 'and', 'in', 'a', 'that', 'is', 'it', 'be']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load word vectors from file\n",
    "vectors = np.load(os.path.join(model_dir, '1800-w.npy'))\n",
    "print(vectors.shape)\n",
    "\n",
    "import pickle\n",
    "# load word vocab from file\n",
    "with open(os.path.join(model_dir, '1840-vocab.pkl'), 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(len(vocab))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gensim KeyedVectors object from the vectors and vocab\n",
    "kv = KeyedVectors(vector_size=vectors.shape[1])\n",
    "kv.add_vectors(vocab, vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
