{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c510886",
   "metadata": {},
   "source": [
    "# Sequence scorer finetuning with pairwise comparisons annotations\n",
    "\n",
    "This notebook is based on an approach for fine-tuning a text item-level scoring model based on pairwise comparison annotations presented in\n",
    "\n",
    "> Licht, Hauke, Rupak Sarkar, Patrick Y. Wu, et al. 2025. “Measuring Scalar Constructs in Social Science with LLMs.” arXiv:2509.03116. Preprint, arXiv, September 3. https://doi.org/10.48550/arXiv.2509.03116.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ea6e4",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/haukelicht/advanced_text_analysis/blob/main/notebooks/encoder_finetuning/finetune_sequence_scorer_pairwise.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f8c01",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "### Pairwise comparison\n",
    "\n",
    "**goal:** use pairwise data to estimate items' scores \n",
    "\n",
    "methodological motivation:\n",
    "\n",
    "- many concepts are latent and conceptually continuous\n",
    "- text items' locations on this scale are hard to observe directly\n",
    "- human annotators are bad at directly scoring items on the scales (inconsistent over-time, different scales, different anchors)\n",
    "- comparison captures relative differences between pairs of items\n",
    "- underlying scale can be estimated from pairwise comparisons data (e.g., Bradley--Terry model)\n",
    "\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "- _text pair-level_ classification: assign pairs of documents (e.g., sentences) to categories\n",
    "- single-label classification: assign pair to one and of two categories (\"first\" or \"second\" text)\n",
    "\n",
    "### methods papers and research applications\n",
    "\n",
    "- Licht, Hauke, Rupak Sarkar, Patrick Y. Wu, et al. 2025. “Measuring Scalar Constructs in Social Science with LLMs.” arXiv:2509.03116. Preprint, arXiv, September 3. https://doi.org/10.48550/arXiv.2509.03116.\n",
    "- Wu P, Nagler J, Tucker JA, Messing S. Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models. 2023. working paper\n",
    "- Benoit K, Munger K, Spirling A. Measuring and Explaining Political So- phistication through Textual Complexity. 2019. *American Journal of Political Science*; 63(2): 491–508.\n",
    "- Carlson D, Montgomery JM. A Pairwise Comparison Framework for Fast, Flexible, and Reliable Human Coding of Political Texts. 2017. *American Political Science Review*; 111(4): 835–843.\n",
    "\n",
    "### pairwise comparison implementation\n",
    "\n",
    "#### fine-tuning\n",
    "\n",
    "given \n",
    "\n",
    "- $n$ training examples ($x_i$, $x_j$, $y_{ij}$) where \n",
    "\t- $x_i$ is item i's text, \n",
    "\t- $x_j$ is item j's text, and\n",
    "\t- $y_{ii}$ is \"1\" if first item wins and \"2\" if second item wins\n",
    "- a (pre-trained) model that \n",
    "\t- intakes text representations ($x_k$)\n",
    "\t- outputs score $\\hat{s}_k$ that estimates item $k$'s strength\n",
    "- loss function: $\\log(\\sigma(s_w - s_l))$ (\"reward loss\") the smaller, the bigger the difference between the \"winner\" and \"loser\" items' predicted scores\n",
    "\n",
    "## pairwise comparison implementation\n",
    "\n",
    "![Illustration of fine-tuning scoring model from pairwise comparisons data](../.assets/task_types-pairwise_comparison.svg)\n",
    "\n",
    "\n",
    "#### prompting\n",
    "\n",
    "given \n",
    "\n",
    "- $n$ input pairs ($x_i$, $x_j$) where \n",
    "\t- $x_i$ is item i's text and\n",
    "\t- $x_j$ is item j's text\n",
    "- a pre-trained generative LLM\n",
    "- prompt \n",
    "    - explaining comparison task\n",
    "    - presenting texts 1 and 2\n",
    "    - giving choice options \"1\" and \"2\"\n",
    "\n",
    "produces pairwise comparison annotations for item score scaling (e.g., with Bradley-Terry model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09316e8b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e72522",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on Colab\n",
    "COLAB = True\n",
    "try:\n",
    "  from google import colab\n",
    "except:\n",
    "  COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "    # shallow clone of current state of main branch \n",
    "    !git clone --branch main --single-branch --depth 1 --filter=blob:none https://github.com/haukelicht/advanced_text_analysis.git\n",
    "    \n",
    "    # make repo root findable for python\n",
    "    import sys\n",
    "    sys.path.append(\"/content/advanced_text_analysis/\")\n",
    "\n",
    "    !pip install -q -U accelerate bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65413e",
   "metadata": {},
   "source": [
    "#### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7bbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.io import read_jsonlines\n",
    "from src.finetuning import unpair_data\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    RewardTrainer,\n",
    "    RewardConfig, # similar to transformers' TrainingArguments\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'answerdotai/ModernBERT-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/content/advanced_text_analysis/\" if COLAB else \"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468a3c8",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fe4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = base_path / \"data\" / \"labeled\" / \"carlson_pairwise_2017\"\n",
    "fp = data_path /  \"carlson_pairwise_2017-immigration_fear.jsonl\"\n",
    "if not fp.exists():\n",
    "    url = \"https://cta-text-datasets.s3.eu-central-1.amazonaws.com/labeled/carlson_pairwise_2017/carlson_pairwise_2017-immigration_fear.jsonl\"\n",
    "    df = pd.read_json(url, lines=True)\n",
    "    fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_json(fp, lines=True, orient='records', force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79653d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_jsonlines(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61506ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only relevant fields\n",
    "fields = ['pair_id', 'id1', 'id2', 'text1', 'text2', 'label']\n",
    "data = [{field: d[field] for field in fields} for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: a single example has the texts that were compared, their IDs, and the label\n",
    "# label = 2 if text2>text1, 1 if text1>text2, 0 otherwise (\"tie\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95876885",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = max(tokenizer([d['text'] for d in unpair_data(data)], return_length=True, truncation=False)['length'])\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918acbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuning import split_data\n",
    "data_splits = split_data(data, test_size=0.2, dev_size=0.2, seed=42, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict({\n",
    "    s: Dataset.from_list(data)\n",
    "    for s, data in data_splits.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert max_tokens <= tokenizer.model_max_length, f\"Error: The max_seq_length passed ({max_tokens}) is larger than the maximum length for the model ({tokenizer.model_max_length}).\"\n",
    "max_seq_length = min(max_tokens, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "        \"is_tie\": [],\n",
    "    }\n",
    "    for text1, text2, label in zip(examples[\"text1\"], examples[\"text2\"], examples[\"label\"]):\n",
    "        _tokenize = lambda x: tokenizer(x, max_length=max_seq_length, truncation=True)\n",
    "        is_tie = label == 0\n",
    "        if label == 1 or label == 0:\n",
    "            tokenized_chosen, tokenized_rejected = _tokenize(text1), _tokenize(text2)\n",
    "        elif label == 2:\n",
    "            tokenized_chosen, tokenized_rejected = _tokenize(text2), _tokenize(text1)\n",
    "        else:\n",
    "            raise ValueError(\"Label must be `1` or `2` to indicate index of chosen item, `0` for ties.\")\n",
    "\n",
    "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "        new_examples[\"is_tie\"].append(is_tie)\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f84e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.map(paired_preprocess_function, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3b157",
   "metadata": {},
   "source": [
    "## setup the reward modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ee52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = base_path / \"models\" / \"carlson_pairwise_2017-immigration_fear-scorer\"\n",
    "\n",
    "reward_config = RewardConfig(\n",
    "    output_dir=model_path,\n",
    "    max_length=max_seq_length,\n",
    "    \n",
    "    do_train=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-05,\n",
    "    optim='adamw_torch',\n",
    "    \n",
    "    do_eval=True,\n",
    "    eval_strategy='epoch',\n",
    "    per_device_eval_batch_size=16,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(MODEL_NAME, lora_task_type='SEQ_CLS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = get_quantization_config(model_config)\n",
    "model_kwargs = dict(\n",
    "    revision=model_config.model_revision,\n",
    "    trust_remote_code=model_config.trust_remote_code,\n",
    "    device_map=get_kbit_device_map() if quantization_config is not None else \"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55199024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from typing import Dict\n",
    "\n",
    "def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "    # taken from: https://github.com/huggingface/trl/blob/dcee683d968444179f57bffa5a49a7ec13f57654/trl/trainer/utils.py#L634\n",
    "    predictions, labels = eval_pred\n",
    "    # Here, predictions is rewards_chosen and rewards_rejected.\n",
    "    # We want to see how much of the time rewards_chosen > rewards_rejected.\n",
    "    if np.array(predictions[:, 0] == predictions[:, 1], dtype=float).sum() > 0:\n",
    "        warnings.warn(\n",
    "            f\"There are {np.array(predictions[:, 0] == predictions[:, 1]).sum()} out of {len(predictions[:, 0])} instances where the predictions for both options are equal. As a consequence the accuracy can be misleading.\"\n",
    "        )\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    scores = {\n",
    "        \"accuracy\": accuracy_score(y_true=labels, y_pred=predictions),\n",
    "        # NOTE: the \"chosen\" text (i.e. the one selected in pairwise comparison) is always put first, so we need to look at label 0\n",
    "        \"f1\": f1_score(y_true=labels, y_pred=predictions, average=None, labels=[0])[0],\n",
    "    }\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_config.model_name_or_path, num_labels=1, **model_kwargs)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=reward_config,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['dev'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    peft_config=get_peft_config(model_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605bc69",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train(ignore_keys_for_eval=[\"hidden_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a42db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(datasets['test'], metric_key_prefix='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8adb726",
   "metadata": {},
   "source": [
    "### item-level scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd33be",
   "metadata": {},
   "source": [
    "Remember that the underyling model is a sequence classification model with a single output neuron (i.e., a scoring/regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model type:\", type(trainer.model).__name__)\n",
    "print(\"number of output neurons:\", trainer.model.config.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde5251",
   "metadata": {},
   "source": [
    "So we can pass it indiviudal texts to score them with the fine-tuned model.\n",
    "\n",
    "For this, we first need to unnest/unpack/unpair the text pair-level data into individual text items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the pair-level data to item-level data\n",
    "items_dataset = unpair_data(datasets['test'])\n",
    "\n",
    "# look at the first item\n",
    "items_dataset[0]\n",
    "\n",
    "# make it a Dataset instance\n",
    "items_dataset = Dataset.from_list(items_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for convenicene we use huggingface's pipeline class for inference with out scoring/regression model \n",
    "scorer = pipeline(task=\"text-classification\", model=trainer.model, tokenizer=tokenizer, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "scorer(items_dataset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc714695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch inference (see https://huggingface.co/docs/transformers/pipeline_tutorial#batch-inference)\n",
    "kd = KeyDataset(items_dataset, \"text\")\n",
    "\n",
    "pred_scores = np.array([p['score'] for p in tqdm(scorer(kd, batch_size=32), total=len(items_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes of 5 highest scoring items\n",
    "idxs = np.argsort(pred_scores)[-5:]\n",
    "print(*items_dataset.select(idxs)['text'], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645cf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes of 5 lowest scoring items\n",
    "idxs = np.argsort(pred_scores)[:5]\n",
    "print(*items_dataset.select(idxs)['text'], sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
