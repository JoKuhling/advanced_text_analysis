{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FAg_PHdL53j"
      },
      "source": [
        "# Fine-tune token classifier for social group mention detection and extraction\n",
        "\n",
        "In this notebook, we use annotations from\n",
        "\n",
        "> Licht, Hauke, and Ronja Sczepanski. 2025. “Detecting Group Mentions in Political Rhetoric A Supervised Learning Approach.” British Journal of Political Science 55: e119. https://doi.org/10.1017/S0007123424000954.\n",
        "\n",
        "to finetune a classifier capable of identifying and extracting phrases in texts that refer to social groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erF6j7KsL53p"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/haukelicht/advanced_text_analysis/blob/main/notebooks/encoder_finetuning/finetune_token_classifier.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2sGVvu-L53q"
      },
      "source": [
        "## Backgrounf\n",
        "\n",
        "span extraction:\n",
        "\n",
        "- **task**: extract spans of words from a text that are mentions/indicators of a target concept\n",
        "- **approaches**\n",
        "    - supervised learning: token classification\n",
        "    - prompting: prompt to respond with structured output of list of strings (verbatim extractions from input text, see Kasner et al, [2025](https://doi.org/10.48550/arXiv.2504.08697))\n",
        "\n",
        "### examples\n",
        "\n",
        "- **named entities** (place, organization, person)\n",
        "\n",
        "    > \"This year, **COMPTEXT** was in **Vienna**.\"\n",
        "\n",
        "- **group mention** (social group, political group, etc.)\n",
        "\n",
        "    > \"**Labour** fights for **hard-working people**.\"\n",
        "\n",
        "- **policy pledge**\n",
        "\n",
        "    > \"We will **lower taxes by 50%**.\"\n",
        "    \n",
        "- **valence attack** (i.e., criticism of political opponent's character/abilit/credibility)\n",
        "\n",
        "    <!-- > \"The Prime Minister **has not been honest with us**. -->\n",
        "\n",
        "    > \"The government has **betrayed the people** over and over again.\"\n",
        "\n",
        "\n",
        "### token classification\n",
        "\n",
        "![Illustration of token classification](https://github.com/haukelicht/advanced_text_analysis/blob/main/notebooks/.assets/task_types-token_classification.svg?raw=1){ height=50% }\n",
        "\n",
        "\n",
        "- _token_-level classification: assign each token documents (e.g., sentences) categories\n",
        "- single-label classification: assign each token to one and only one category\n",
        "\n",
        "## methods papers and research applications\n",
        "\n",
        "- Licht, Hauke, and Ronja Sczepanski. 2025. “Detecting Group Mentions in Political Rhetoric A Supervised Learning Approach.” British Journal of Political Science 55: e119. https://doi.org/10.1017/S0007123424000954.\n",
        "- Kasner, Zdeněk, Vilém Zouhar, Patrícia Schmidtová, et al. 2025. “Large Language Models as Span Annotators.” arXiv:2504.08697. Version 1. Preprint, arXiv, April 11. https://doi.org/10.48550/arXiv.2504.08697.\n",
        "- Klamm C, Rehbein I, Ponzetto SP. Our kind of people? Detecting populist references in political debates. 2023. *Findings of the Association for Computational Linguistics: EACL 2023*. 1227–1243. doi:[10.18653/v1/2023.findings-eacl.91](https://doi.org/10.18653/v1/2023.findings-eacl.91)\n",
        "- Skorupa Parolin E, Hosseini MS, Hu Y, Khan L, Brandt PT, Osorio J, D'Orazio V. Multi-CoPED: A Multilingual Multi-Task Approach for Coding Political Event Data on Conflict and Mediation Domain. 2022. *Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES '22)*. 700–711. doi:[10.1145/3514094.3534178](https://doi.org/10.1145/3514094.3534178)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g18I3gUgL53r"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iapuI-aqL53r"
      },
      "source": [
        "### Setup Colab (if using Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "miM1qWng65d0"
      },
      "outputs": [],
      "source": [
        "# check if on Colab\n",
        "COLAB = True\n",
        "try:\n",
        "  from google import colab\n",
        "except:\n",
        "  COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYnfY0V1L53u",
        "outputId": "dbced28b-fe0d-4929-c2c8-abfcabfb076c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for soft_seqeval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Collecting src\n",
            "  Using cached src-0.0.7.zip (6.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: src\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for src (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for src\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for src\n",
            "Failed to build src\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (src)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# install soft-seqeval (latest version)\n",
        "!pip install -q --upgrade --force-reinstall --no-deps git+https://github.com/haukelicht/soft-seqeval.git@main\n",
        "!pip install seqeval\n",
        "!pip install src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S8WIP6pL53v"
      },
      "source": [
        "### Load required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "XTWGIqKf65d2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    EarlyStoppingCallback,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "from soft_seqeval.metrics import compute_sequence_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "yL654zJEL53w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from nltk.data import find as nltk_find\n",
        "from nltk import download as nltk_download\n",
        "nltk_res = ['punkt', 'punkt_tab']\n",
        "for res in nltk_res:\n",
        "    try:\n",
        "        nltk_find(os.path.join('tokenizers', res))\n",
        "    except LookupError:\n",
        "        nltk_download(nltk_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "rHHN5POtL53x"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from soft_seqeval.classes import LabeledSequence, Entities, Entity\n",
        "from collections import OrderedDict\n",
        "\n",
        "from typing import List, Dict, Any, Mapping, Union\n",
        "\n",
        "def read_jsonlines_corpus(\n",
        "    file: str,\n",
        "    id_field: str='id',\n",
        "    text_field: str='text',\n",
        "    annotations_field: str='label',\n",
        "    remove_unsure: bool=True,\n",
        "    lang: str='english'\n",
        ") -> Mapping[Union[str, int], LabeledSequence]:\n",
        "    \"\"\"Read a jsonlines corpus and return a dictionary of LabeledSequence objects.\n",
        "    Args:\n",
        "        file (str): Path to the jsonlines file.\n",
        "        id_field (str): Name of the field containing the document ID.\n",
        "        text_field (str): Name of the field containing the document text.\n",
        "        annotations_field (str): Name of the field containing the annotations.\n",
        "        remove_unsure (bool): Whether to remove annotations that end with 'unsure'.\n",
        "        lang (str): Language of the documents.\n",
        "\n",
        "    Returns:\n",
        "        Mapping[Union[str, int], LabeledSequence]: A dictionary mapping document IDs to LabeledSequence objects.\n",
        "    \"\"\"\n",
        "    with open(file, 'r') as f:\n",
        "        data = []\n",
        "        for line in f:\n",
        "            try:\n",
        "                line = json.loads(line)\n",
        "                data.append(line)\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "\n",
        "    documents = [\n",
        "        # doc[text_field]:\n",
        "        LabeledSequence(\n",
        "            text=doc[text_field],\n",
        "            entities=Entities([\n",
        "                Entity(*lab)\n",
        "                for lab in doc[annotations_field]\n",
        "                if (not lab[2].lower().endswith('unsure') if remove_unsure else True)\n",
        "            ]),\n",
        "            id = doc[id_field],\n",
        "            lang=lang\n",
        "        )\n",
        "        for doc in data\n",
        "    ]\n",
        "\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "TBcLluXyL53x"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"answerdotai/ModernBERT-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3lA8aqkx65d3",
        "outputId": "548aaf58-ab86-4dc3-9ce1-67580c17c817"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "SQg_xeX465d1"
      },
      "outputs": [],
      "source": [
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "rW0rUOMx65d3"
      },
      "outputs": [],
      "source": [
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "XPQl4UUiL53z"
      },
      "outputs": [],
      "source": [
        "base_path = Path(\"/content/advanced_text_analysis/\" if COLAB else \"../../\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhMKwqt8L53z"
      },
      "source": [
        "## Load and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "pmopZBoZL53z"
      },
      "outputs": [],
      "source": [
        "data_path = base_path / \"data\" / \"labeled\" / \"licht_detecting_2025\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "MUW0kUx0L53z"
      },
      "outputs": [],
      "source": [
        "fp = data_path / \"licht_detecting_2025-uk_manifestos.jsonl\"\n",
        "if not fp.exists():\n",
        "    url = \"https://cta-text-datasets.s3.eu-central-1.amazonaws.com/labeled/licht_detecting_2025/licht_detecting_2025-uk_manifestos.jsonl\"\n",
        "    df = pd.read_json(url, lines=True)\n",
        "    fp.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_json(fp, lines=True, orient='records', force_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "hzony5N4L530"
      },
      "outputs": [],
      "source": [
        "corpus = read_jsonlines_corpus(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z-1jWpUL530",
        "outputId": "285e15ed-64d7-4033-d2c1-710a48d294cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[1m48b1c6ba33bb5e538c420148ec993090\u001b[0m: \"This would provide a boost of over £100 million, which we believe will provide important new opportunities for \u001b[43m\u001b[1mproduction companies\u001b[0m\u001b[43m [other]\u001b[49m and \u001b[43m\u001b[1mthe creative sector\u001b[0m\u001b[43m [other]\u001b[49m in Scotland.\""
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "QRGhRZIl65d4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    PreTrainedTokenizer,\n",
        "    DefaultDataCollator,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback,\n",
        "    EarlyStoppingCallback,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "\n",
        "from typing import List, Dict, Union, Optional, Callable, Tuple\n",
        "\n",
        "# ------------------------------------------------\n",
        "#  General utils\n",
        "# ------------------------------------------------\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    return torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "# ------------------------------------------------\n",
        "#  Dataset splitting\n",
        "# ------------------------------------------------\n",
        "\n",
        "def _check_split_sizes(dev_size, test_size, n):\n",
        "        if dev_size and isinstance(dev_size, float): assert 0 < dev_size < 1, \"dev_size must be in (0, 1)\"\n",
        "        if test_size and isinstance(test_size, float): assert 0 < test_size < 1, \"test_size must be in (0, 1)\"\n",
        "        if (\n",
        "            dev_size and isinstance(dev_size, float)\n",
        "            and\n",
        "            test_size and isinstance(test_size, float)\n",
        "        ):\n",
        "            assert (dev_size + test_size) < 1, \"dev_size + test_size must be less than 1\"\n",
        "\n",
        "        if dev_size and isinstance(dev_size, int): assert dev_size < n, \"dev_size must be in less than \"+str(n)\n",
        "        if test_size and isinstance(test_size, int): assert test_size < n, \"test_size must be less than \"+str(n)\n",
        "\n",
        "        # compute sizes\n",
        "        n_test = 0 if test_size is None else test_size if isinstance(test_size, int) else int(test_size * n)\n",
        "        n_dev =  0 if dev_size  is None else dev_size  if isinstance(dev_size, int)  else int(dev_size  * n)\n",
        "        assert n_test + n_dev < n, \"test_size + dev_size must be less than the number of examples\"\n",
        "\n",
        "        return n_dev, n_test\n",
        "\n",
        "def _split_data_frame(\n",
        "        df: pd.DataFrame,\n",
        "        dev_size: float=0.15,\n",
        "        test_size: float=0.15,\n",
        "        stratify_by: Optional[Union[str, List[str]]]=None,\n",
        "        seed: int=42,\n",
        "        return_dict: bool=False\n",
        "    ):\n",
        "    n = len(df)\n",
        "    dev_size, test_size = _check_split_sizes(dev_size, test_size, n)\n",
        "\n",
        "    if stratify_by:\n",
        "        if isinstance(stratify_by, str):\n",
        "            stratify_by = [stratify_by]\n",
        "        for col in stratify_by:\n",
        "            assert col in df.columns, f\"Column '{col}' not found in ``df``. cannot use for stratified splitting.\"\n",
        "        # create a grouping indicator based on the stratification columns\n",
        "        df['__stratum__'] = df.groupby(stratify_by).ngroup()\n",
        "    else:\n",
        "        df['__stratum__'] = 0\n",
        "\n",
        "    idxs = df.index\n",
        "    tmp, test_idxs = train_test_split(idxs, test_size=test_size, random_state=seed, stratify=df['__stratum__'] if stratify_by else None) if test_size > 0 else (idxs, [])\n",
        "    train_idxs, dev_idxs = train_test_split(tmp, test_size=dev_size, random_state=seed, stratify=df.loc[tmp, '__stratum__'] if stratify_by else None) if dev_size > 0 else (tmp, [])\n",
        "\n",
        "    del df['__stratum__']\n",
        "\n",
        "    out = {'train': df.loc[train_idxs]}\n",
        "    out['dev'] = df.loc[dev_idxs] if dev_size > 0 else None\n",
        "    out['test'] = df.loc[test_idxs] if test_size > 0 else None\n",
        "    del df, tmp, test_idxs, train_idxs, dev_idxs\n",
        "    gc.collect()\n",
        "\n",
        "    if return_dict:\n",
        "        return {s: d for s, d in out.items() if d is not None}\n",
        "    else:\n",
        "        return tuple(out.values())\n",
        "\n",
        "def _split_corpus(\n",
        "        corpus: List[Dict],\n",
        "        test_size: Union[None, float, int]=0.2,\n",
        "        dev_size: Union[None, float, int]=0.2,\n",
        "        stratify_by: Optional[Union[str, List[str]]]=None,\n",
        "        seed: int=42,\n",
        "        return_dict: bool=False\n",
        "    ):\n",
        "    n = len(corpus)\n",
        "    dev_size, test_size = _check_split_sizes(dev_size, test_size, n)\n",
        "\n",
        "    if stratify_by:\n",
        "        assert all('metadata' in doc for doc in corpus), \"Stratification requires 'metadata' field in each document's dictionary\"\n",
        "        if isinstance(stratify_by, str):\n",
        "            stratify_by = [stratify_by]\n",
        "        for field in stratify_by:\n",
        "            assert all(field in doc['metadata'] for doc in corpus), f\"Field '{field}' not found in 'metadata' of all documents\"\n",
        "        # create a grouping indicator based on the stratification columns\n",
        "        strata = ['__'.join([str(doc['metadata'][field]) for field in stratify_by]) for doc in corpus]\n",
        "    else:\n",
        "        strata = None\n",
        "\n",
        "    idxs = list(range(n))\n",
        "    tmp, test_idxs = train_test_split(idxs, test_size=test_size, random_state=seed, stratify=strata) if test_size > 0 else (idxs, [])\n",
        "    strata = [strata[i] for i in test_idxs] if stratify_by else None\n",
        "    train_idxs, dev_idxs = train_test_split(tmp, test_size=dev_size, random_state=seed, stratify=strata) if dev_size > 0 else (tmp, [])\n",
        "\n",
        "    out = {'train': [corpus[i] for i in train_idxs]}\n",
        "    out['dev']  = [corpus[i] for i in dev_idxs]  if dev_size > 0 else None\n",
        "    out['test'] = [corpus[i] for i in test_idxs] if test_size > 0 else None\n",
        "\n",
        "    if return_dict:\n",
        "        return {s: d for s, d in out.items() if d is not None}\n",
        "    else:\n",
        "        return tuple(out.values())\n",
        "\n",
        "def split_data(\n",
        "        data: Union[pd.DataFrame, List[Dict]],\n",
        "        test_size: Union[None, float, int]=0.2,\n",
        "        dev_size: Union[None, float, int]=0.2,\n",
        "        stratify_by: Optional[Union[str, List[str]]]=None,\n",
        "        seed: int=42,\n",
        "        return_dict: bool=False\n",
        "    ):\n",
        "    \"\"\"Split a dataset into training, development, and test sets.\n",
        "\n",
        "    df: List[Dict]\n",
        "        The data to split. Must be a data frame or a list of dictionaries.\n",
        "    dev_size: float\n",
        "        The proportion of the data to include in the development set.\n",
        "    test_size: float\n",
        "        The proportion of the data to include in the test set.\n",
        "    stratify_by: str or list of str, optional\n",
        "        Metadata field(s)/column(s) to use for stratified splitting. If a single field is\n",
        "        provided, the data will be stratified by the values in that field/column.\n",
        "        If multiple columns are provided, the data will be stratified by\n",
        "        the unique combinations of values in these fields/columns.\n",
        "    seed: int\n",
        "        Random seed for reproducibility.\n",
        "    return_dict: bool\n",
        "        Whether to return the splits as a dictionary.\n",
        "    \"\"\"\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        return _split_data_frame(data, dev_size, test_size, stratify_by, seed, return_dict)\n",
        "    elif isinstance(data, list) and all(isinstance(doc, dict) for doc in data):\n",
        "        return _split_corpus(data, test_size, dev_size, stratify_by, seed, return_dict)\n",
        "    elif isinstance(data, list):\n",
        "        return _split_corpus(data, test_size, dev_size, None, seed, return_dict)\n",
        "    else:\n",
        "        raise ValueError('`data` must be a pandas DataFrame or a list of dictionaries')\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "#  Sequence classification\n",
        "# ------------------------------------------------\n",
        "\n",
        "def create_sequence_classification_dataset(\n",
        "        corpus: Union[pd.DataFrame, List[Dict]],\n",
        "        text_field: str='text',\n",
        "        label_field: str='label'\n",
        "    ) -> Dataset:\n",
        "    dataset = Dataset.from_list(corpus) if isinstance(corpus, list) else Dataset.from_pandas(corpus)\n",
        "    if text_field != 'text':\n",
        "        dataset = dataset.rename_column(text_field, 'text')\n",
        "    if label_field != 'label':\n",
        "        dataset = dataset.rename_column(label_field, 'label')\n",
        "    required = ['text', 'label']\n",
        "    rm = [c for c in dataset.column_names if c not in required]\n",
        "    if len(rm) > 0:\n",
        "        dataset = dataset.remove_columns(rm)\n",
        "    return dataset\n",
        "\n",
        "def preprocess_sequence_classification_dataset(examples, tokenizer, label2id: Optional[Dict[str, int]]=None, **kwargs):\n",
        "    output = tokenizer(examples['text'], **kwargs)\n",
        "    output['labels'] = [label2id[l] for l in examples['label']] if label2id else examples['label']\n",
        "    return output\n",
        "\n",
        "# ------------------------------------------------\n",
        "#  Pairwise finetuning\n",
        "# ------------------------------------------------\n",
        "\n",
        "def unpair_data(data: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Convert paired data into separate examples\n",
        "    \"\"\"\n",
        "    seen_ids = set()\n",
        "    unpacked_data = []\n",
        "    for row in data:\n",
        "        for i in [1, 2]:\n",
        "            if row[f\"id{i}\"] not in seen_ids:\n",
        "                seen_ids.add(row[f\"id{i}\"])\n",
        "                unpacked_data.append({\n",
        "                    \"id\": row[f\"id{i}\"],\n",
        "                    \"text\": row[f\"text{i}\"],\n",
        "                })\n",
        "    return unpacked_data\n",
        "\n",
        "def create_pairwise_classification_dataset(\n",
        "        corpus: List[Dict],\n",
        "        text_fields: List[str]=['text1', 'text2'],\n",
        "        label_field: str='label'\n",
        "    ) -> Dataset:\n",
        "    dataset = Dataset.from_list(corpus)\n",
        "    if len(text_fields) != 2:\n",
        "        raise ValueError('text_fields must be a list of length 2')\n",
        "    if label_field != 'label':\n",
        "        dataset = dataset.rename_column(label_field, 'label')\n",
        "\n",
        "    if text_fields[0] != 'text1':\n",
        "        dataset = dataset.rename_column(text_fields[0], 'text1')\n",
        "    if text_fields[1] != 'text2':\n",
        "        dataset = dataset.rename_column(text_fields[1], 'text2')\n",
        "    required = ['text1', 'text2', 'label']\n",
        "    rm = [c for c in dataset.column_names if c not in required]\n",
        "    if len(rm) > 0:\n",
        "        dataset = dataset.remove_columns(rm)\n",
        "    return dataset\n",
        "\n",
        "def preprocess_pairwise_classification_dataset_for_reward_modeling(\n",
        "        examples,\n",
        "        tokenizer,\n",
        "        max_seq_length: Optional[int]= None,\n",
        "        **kwargs\n",
        "    ):\n",
        "    new_examples = {\n",
        "        # \"labels\": [],\n",
        "        \"input_ids_chosen\": [],\n",
        "        \"attention_mask_chosen\": [],\n",
        "        \"input_ids_rejected\": [],\n",
        "        \"attention_mask_rejected\": [],\n",
        "    }\n",
        "    for text1, text2, label in zip(examples[\"text1\"], examples[\"text2\"], examples[\"label\"]):\n",
        "        _tokenize = lambda x: tokenizer(x, **kwargs)\n",
        "        if label == 1:\n",
        "            lab = 0\n",
        "            tokenized_chosen, tokenized_rejected = _tokenize(text1), _tokenize(text2)\n",
        "        elif label == 2:\n",
        "            lab = 1\n",
        "            tokenized_rejected, tokenized_chosen = _tokenize(text1), _tokenize(text2)\n",
        "        else:\n",
        "            raise ValueError(\"Label must be `1` or `2` to indicate index of chosen item.\")\n",
        "\n",
        "        # new_examples[\"labels\"].append(lab)\n",
        "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
        "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
        "    return new_examples\n",
        "\n",
        "# ------------------------------------------------\n",
        "#  Token Classification\n",
        "# ------------------------------------------------\n",
        "\n",
        "def create_token_classification_dataset(\n",
        "    corpus: List[Dict],\n",
        "    tokens_field: str='tokens',\n",
        "    labels_field: Union[None, str]='labels'\n",
        "):\n",
        "    dataset = Dataset.from_list(corpus)\n",
        "    if tokens_field != 'tokens':\n",
        "        dataset = dataset.rename_column(tokens_field, 'tokens')\n",
        "    if labels_field is not None and labels_field != 'labels':\n",
        "        dataset = dataset.rename_column(labels_field, 'labels')\n",
        "    required = ['tokens'] if labels_field is None else ['tokens', 'labels']\n",
        "    rm = [c for c in dataset.column_names if c not in required]\n",
        "    if len(rm) > 0:\n",
        "        dataset = dataset.remove_columns(rm)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def preprocess_token_classification_dataset(examples, tokenizer, label2id: Optional[Dict[str, int]]=None, **kwargs):\n",
        "    # source: simplied from  https://github.com/huggingface/transformers/blob/730a440734e1fb47c903c17e3231dac18e3e5fd6/examples/pytorch/token-classification/run_ner.py#L442\n",
        "    tokenized_inputs = tokenizer(examples['tokens'], is_split_into_words=True, **kwargs)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['labels']):\n",
        "        # map tokens to their respective word\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "        \t# set the special tokens to -100\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # only label the first token of a given word\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    if label2id:\n",
        "        labels = [[-100 if l==-100 else label2id[l] for l in label] for label in labels]\n",
        "\n",
        "    tokenized_inputs['labels'] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# ------------------------------------------------\n",
        "#  Trainer\n",
        "# ------------------------------------------------\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "class ClassWeightsTrainer(Trainer):\n",
        "\n",
        "    def __init__(self, class_weights: Union[List, Dict[Union[int, str], float]], **kwargs):\n",
        "        \"\"\"\n",
        "        argument ``class_weights`` should be a dictionary mapping class labels to weights or a list of only the weights\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        # self.model = self.model.to(self.model.device)\n",
        "        if len(class_weights) != self.model.config.num_labels:\n",
        "            raise ValueError(f'length of `class_weights` must be {self.model.config.num_labels}')\n",
        "        if isinstance(class_weights, dict):\n",
        "            if set(class_weights.keys()) != set(self.model.config.id2label.keys()):\n",
        "                raise ValueError(f'keys of `class_weights` mismatch label classes {list(self.model.config.id2label.keys())}')\n",
        "            class_weights = [v for k, v in sorted(class_weights.items(), key=lambda item: item[1])]\n",
        "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.model.device)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get('labels')\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "        # compute custom loss\n",
        "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "class WriteValidationResultsCallback(TrainerCallback):\n",
        "    \"\"\"Trainer callback to write validation set results to disk while training\"\"\"\n",
        "    def __init__(self, path='validation_results.jsonl', overwrite=True):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        if overwrite:\n",
        "            with open(self.path, 'w') as f:\n",
        "                f.write('')\n",
        "\n",
        "    def on_evaluate(self, args, state, control, **kwargs):\n",
        "        validation_results = state.log_history[-1]\n",
        "        with open(self.path, \"a\") as f:\n",
        "            f.write(json.dumps(validation_results) + \"\\n\")\n",
        "\n",
        "\n",
        "def train_and_test(\n",
        "    experiment_name: str,\n",
        "    experiment_results_path: str,\n",
        "    run_id: Union[None, str],\n",
        "    model_init: Callable,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    data_collator: DefaultDataCollator,\n",
        "    train_dat: Dataset,\n",
        "    dev_dat: Union[None, Dataset],\n",
        "    test_dat: Union[None, Dataset],\n",
        "    compute_metrics: Callable,\n",
        "    metric: str,\n",
        "    class_weights: Optional[Union[List, Dict[Union[int, str], float]]]=None,\n",
        "    epochs: int = TrainingArguments.num_train_epochs,\n",
        "    learning_rate: float = TrainingArguments.learning_rate,\n",
        "    train_batch_size: int = TrainingArguments.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps: int = TrainingArguments.gradient_accumulation_steps,\n",
        "    fp16_training: bool = True,\n",
        "    eval_batch_size: int = TrainingArguments.per_device_eval_batch_size,\n",
        "    weight_decay: float = TrainingArguments.weight_decay,\n",
        "    early_stopping: bool = True,\n",
        "    early_stopping_patience: int = 3,\n",
        "    early_stopping_threshold: float = 0.03,\n",
        "    seed: int = 42,\n",
        "    save_best_model: bool = True,\n",
        "    save_tokenizer: bool = True,\n",
        ") -> Tuple[Trainer, str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Fine-tune and evaluate a Transformer model.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str):\n",
        "            Name of the experiment. Used for creating directories for saving results.\n",
        "        experiment_results_path (str):\n",
        "            Base path where experiment results will be saved.\n",
        "        run_id (Union[None, str]):\n",
        "            Optional unique identifier for the run. If None, an identifier will be generated.\n",
        "        model_init (Callable):\n",
        "            A function that initializes the model to be trained.\n",
        "        tokenizer (PreTrainedTokenizer):\n",
        "            Tokenizer for preprocessing text data.\n",
        "        data_collator (DefaultDataCollator):\n",
        "            Data collator that batches data samples.\n",
        "        train_dat (Dataset):\n",
        "            The dataset used for training.\n",
        "        dev_dat (Union[None, Dataset]):\n",
        "            The dataset used for validation. If None, validation is skipped.\n",
        "        test_dat (Union[None, Dataset]):\n",
        "            The dataset used for testing. If None, testing is skipped.\n",
        "        compute_metrics (Callable):\n",
        "            Function to compute metrics based on predictions and true labels.\n",
        "        metric (str):\n",
        "            Name of the metric to be used for evaluation.\n",
        "        epochs (int):\n",
        "            Number of training epochs. Defaults to TrainingArguments.num_train_epochs.\n",
        "        learning_rate (float):\n",
        "            Learning rate for the optimizer. Defaults to TrainingArguments.learning_rate.\n",
        "        train_batch_size (int):\n",
        "            Batch size for training. Defaults to TrainingArguments.per_device_train_batch_size.\n",
        "        gradient_accumulation_steps (int):\n",
        "            Number of steps to accumulate gradients before updating model parameters. Defaults to TrainingArguments.gradient_accumulation_steps.\n",
        "        fp16_training (bool):\n",
        "            Whether to use mixed precision training. Defaults to True.\n",
        "        eval_batch_size (int):\n",
        "            Batch size for evaluation. Defaults to TrainingArguments.per_device_eval_batch_size.\n",
        "        weight_decay (float):\n",
        "            Weight decay for the optimizer. Defaults to TrainingArguments.weight_decay.\n",
        "        early_stopping (bool):\n",
        "            Whether to use early stopping. Defaults to True.\n",
        "        early_stopping_patience (int):\n",
        "            Number of evaluations with no improvement after which training will be stopped. Defaults to 3.\n",
        "        early_stopping_threshold (float):\n",
        "            Minimum change in the monitored metric to qualify as an improvement. Defaults to 0.03.\n",
        "        seed (int):\n",
        "            Random seed for reproducibility. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        Trainer:\n",
        "            Trainer object used for training.\n",
        "        str:\n",
        "            Path to the best model checkpoint.\n",
        "        dict:\n",
        "            Evaluation results on the test set.\n",
        "    \"\"\"\n",
        "    results_path = os.path.join(experiment_results_path, experiment_name)\n",
        "    os.makedirs(results_path, exist_ok=True)\n",
        "\n",
        "    output_path = os.path.join(results_path, 'checkpoints')\n",
        "    logs_path = os.path.join(results_path, 'logs')\n",
        "\n",
        "    # note: the following training options depend on the availability of a dev set and will be disabled if none is provided\n",
        "    #  - evaluating after each epoch\n",
        "    #  - early stopping\n",
        "    #  - saving at most 2 models during training\n",
        "    #  - saving the best model at the end\n",
        "    #  - saving the dev results\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        # hyperparameters\n",
        "        num_train_epochs=epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        per_device_eval_batch_size=eval_batch_size,\n",
        "        weight_decay=weight_decay,\n",
        "        optim='adamw_torch',\n",
        "        # how to select \"best\" model\n",
        "        do_eval=dev_dat is not None,\n",
        "        metric_for_best_model=metric,\n",
        "        load_best_model_at_end=True,\n",
        "        # when to evaluate\n",
        "        evaluation_strategy='epoch',\n",
        "        # when to save\n",
        "        save_strategy='epoch',\n",
        "        save_total_limit=2 if dev_dat is not None else None, # don't save all model checkpoints\n",
        "        # where to store results\n",
        "        output_dir=output_path,\n",
        "        overwrite_output_dir=True,\n",
        "        # logging\n",
        "        logging_dir=logs_path,\n",
        "        logging_strategy='epoch',\n",
        "        report_to='none',\n",
        "        # efficiency\n",
        "        fp16=fp16_training if torch.cuda.is_available() else False,\n",
        "        fp16_full_eval=False,\n",
        "        # reproducibility\n",
        "        seed=seed,\n",
        "        data_seed=seed,\n",
        "        full_determinism=True\n",
        "    )\n",
        "\n",
        "    # build callbacks\n",
        "    callbacks = []\n",
        "    if early_stopping:\n",
        "        if dev_dat is None:\n",
        "            raise ValueError('Early stopping requires a dev data set')\n",
        "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=early_stopping_patience, early_stopping_threshold=early_stopping_threshold))\n",
        "    if dev_dat:\n",
        "        fn = run_id+'-dev_results.jsonl' if run_id else 'dev_results.jsonl'\n",
        "        fp = os.path.join(results_path, fn)\n",
        "        callbacks.append(WriteValidationResultsCallback(path=fp))\n",
        "\n",
        "    # train\n",
        "    trainer_args = dict(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dat,\n",
        "        eval_dataset=dev_dat if dev_dat is not None else None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator if data_collator is not None else None,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "    if class_weights:\n",
        "        trainer_args['class_weights'] = class_weights\n",
        "        trainer = ClassWeightsTrainer(**trainer_args)\n",
        "    else:\n",
        "        trainer = Trainer(**trainer_args)\n",
        "\n",
        "    print('Training ...')\n",
        "    _ = trainer.train()\n",
        "\n",
        "    # save best model to results folder\n",
        "    # CAVEAT: this is not the \"best\" model if no dev_dat is provided\n",
        "    dest = run_id+'-best_model' if run_id else 'best_model'\n",
        "    dest = os.path.join(results_path, dest)\n",
        "    if os.path.exists(dest):\n",
        "        shutil.rmtree(dest)\n",
        "    if save_best_model:\n",
        "        trainer.save_model(dest)\n",
        "        # save tokenizer to best_model folder\n",
        "        if save_tokenizer:\n",
        "            tokenizer.save_pretrained(dest)\n",
        "\n",
        "    # evaluate\n",
        "    if test_dat:\n",
        "        print('Evaluating ...')\n",
        "        res = trainer.evaluate(test_dat, metric_key_prefix='test')\n",
        "        print(res)\n",
        "        fn = run_id+'-test_results.json' if run_id else 'test_results.json'\n",
        "        fp = os.path.join(results_path, fn)\n",
        "        with open(fp, 'w') as file:\n",
        "            json.dump(res, file)\n",
        "    else:\n",
        "      res = None\n",
        "\n",
        "    # finally: clean up\n",
        "    if os.path.exists(output_path):\n",
        "        # TODO: reconsider this when dev_dat is None (in this case, no best model will be copied and deliting the output path would delete any model checkpoints)\n",
        "        shutil.rmtree(output_path)\n",
        "    if os.path.exists(logs_path):\n",
        "        shutil.rmtree(logs_path)\n",
        "\n",
        "    return trainer, dest, res\n",
        "\n",
        "\n",
        "\n",
        "data_splits = split_data(corpus, dev_size=0.1, test_size=0.15, seed=SEED, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "EiugXObD65d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e25678-5e07-4f12-af02-4c0cbdfb5c19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'O': 0, 'I-other': 1, 'I-SG': 2, 'B-other': 3, 'B-SG': 4}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from the annotations, get all entity \"types\" and construct a label2id mapping\n",
        "#  where the labels are the IOB2-scheme for each entity type\n",
        "\n",
        "types = list(set(ent.type for dataset in data_splits.values() for doc in dataset for ent in doc.entities))\n",
        "scheme = ['O'] + ['I-'+t for t in types] + ['B-'+t for t in types]\n",
        "label2id = {l: i for i, l in enumerate(scheme)}\n",
        "id2label = {i: l for i, l in enumerate(scheme)}\n",
        "NUM_LABELS = len(label2id)\n",
        "\n",
        "label2id\n",
        "# NOTE: the span-level annotations will be converted to token-level annotations using the IOB2 scheme.append\n",
        "#       This means that\n",
        "#        - a word that are not part of any entity will be labeled as \"O\",\n",
        "#        - a word at the beginning of a span will be labeled as \"B-<entity_type>\", and\n",
        "#        - a word inside a span will be labeled as \"I-<entity_type>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "oN1QhGfI65d4"
      },
      "outputs": [],
      "source": [
        "# NOTE: here we use the LabeledSequence instances to_labeled_tokens methods to convert\n",
        "#       the span-level annotations to token-level annotations\n",
        "#       This method returns a LabeledTokens instance with the token-level annotations,\n",
        "#       which we then convert into a dictionary with fields 'tokens' and 'labels'.\n",
        "data_splits = {\n",
        "    s: [doc.to_labeled_tokens(label2id).to_dict() for doc in dataset]\n",
        "    for s, dataset in data_splits.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKL8PxbDL531",
        "outputId": "e56e8c2d-bd38-4020-f3d2-d25165c300fa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Finally' ==> O\n",
            "',' ==> O\n",
            "' we' ==> O\n",
            "' will' ==> O\n",
            "' transform' ==> O\n",
            "' our' ==> O\n",
            "' mental' ==> B-other\n",
            "' health' ==> I-other\n",
            "' service' ==> I-other\n",
            "' by' ==> O\n",
            "' treating' ==> O\n",
            "' it' ==> O\n",
            "' with' ==> O\n",
            "' the' ==> O\n",
            "' same' ==> O\n",
            "' urgency' ==> O\n",
            "' as' ==> O\n",
            "' physical' ==> O\n",
            "' health' ==> O\n",
            "'.' ==> O\n"
          ]
        }
      ],
      "source": [
        "# uncomment to show example\n",
        "i = 2\n",
        "for t, l in zip(data_splits['train'][i]['tokens'], data_splits['train'][i]['labels']):\n",
        "    print(repr(t), '==>', id2label[l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "sc59Fj2Q65d4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# use custom function defined above to convert corpus to a datasets.Dataset instance (used by transformers' Trainer below)\n",
        "datasets = DatasetDict({\n",
        "    s: create_token_classification_dataset(dataset)\n",
        "    for s, dataset in data_splits.items()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlNu4cYfL532",
        "outputId": "ce13a178-41ec-4df9-9043-e84f57bf0bca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': 6433, 'dev': 857, 'test': 1286}"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets.num_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APIOYwFLL532",
        "outputId": "41d43f93-14a9-496a-ad2b-84cf9cec6165"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tokens': ['Obviously',\n",
              "  ' there',\n",
              "  ' is',\n",
              "  ' a',\n",
              "  ' price',\n",
              "  ' we',\n",
              "  ' would',\n",
              "  ' not',\n",
              "  ' be',\n",
              "  ' prepared',\n",
              "  ' to',\n",
              "  ' pay',\n",
              "  '.'],\n",
              " 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "ddbec503ad4945348f3ababf6b50614d",
            "5573765ac52f41b08f2b1617d9d7b23c",
            "47e47bd310c64f12b88fc63c9de1d034",
            "a6bd639a429349b78ca99824ba7d739c",
            "dce679bbb17848e6bb61b2cb9f5caa2f",
            "c0580e7f2e4d44559b5f57782387f9ad"
          ]
        },
        "id": "hoTx4rS665d4",
        "outputId": "ad34764e-be6a-4a49-fd49-4fa06910adf4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddbec503ad4945348f3ababf6b50614d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5573765ac52f41b08f2b1617d9d7b23c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47e47bd310c64f12b88fc63c9de1d034",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6bd639a429349b78ca99824ba7d739c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6433 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dce679bbb17848e6bb61b2cb9f5caa2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0580e7f2e4d44559b5f57782387f9ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1286 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, add_prefix_space=True)\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
        "\n",
        "# apply the custom function defined above to set subword tokens' labels to -100\n",
        "# this is necessary because the tokenization may split a word into multiple subwords\n",
        "datasets = datasets.map(lambda example: preprocess_token_classification_dataset(example, tokenizer=tokenizer), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulQhLYui65d4",
        "outputId": "a7367723-f431-4621-97c2-507d560dc736"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-100 \t '[CLS]'\n",
            "0 \t ' Finally'\n",
            "0 \t ','\n",
            "0 \t ' we'\n",
            "0 \t ' will'\n",
            "0 \t ' transform'\n",
            "0 \t ' our'\n",
            "3 \t ' mental'\n",
            "1 \t ' health'\n",
            "1 \t ' service'\n",
            "0 \t ' by'\n",
            "0 \t ' treating'\n",
            "0 \t ' it'\n",
            "0 \t ' with'\n",
            "0 \t ' the'\n",
            "0 \t ' same'\n",
            "0 \t ' urgency'\n",
            "0 \t ' as'\n",
            "0 \t ' physical'\n",
            "0 \t ' health'\n",
            "0 \t '.'\n",
            "-100 \t '[SEP]'\n"
          ]
        }
      ],
      "source": [
        "# uncomment to show example\n",
        "example = datasets['train'][2]\n",
        "for t, l in zip(example['input_ids'], example['labels']):\n",
        "    if t == tokenizer.pad_token_id:\n",
        "        break\n",
        "    print(l, '\\t', repr(tokenizer.decode(t)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "-8BJgynSq4s2"
      },
      "outputs": [],
      "source": [
        "# NOTE: after tokenization, text tokens are represented with their token IDs\n",
        "#        so we can remove them from the dataset (need to load these to the GPU)\n",
        "datasets = datasets.remove_columns(['tokens'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydk77YN0L533"
      },
      "source": [
        "## Prepare the model fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "uvg_ldS965d5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "# NOTE: the `model_init` function is used by the Trainer to initialize the model\n",
        "#   and is called each time before training starts.\n",
        "#  So we define it here to load the model from the Huggingface model hub\n",
        "#   and set the number of labels to the number of unique labels in the dataset\n",
        "#   and the label2id and id2label mappings\n",
        "def model_init():\n",
        "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    config.num_labels = NUM_LABELS\n",
        "    config.label2id = label2id\n",
        "    config.id2label = id2label\n",
        "    return AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config, device_map='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irXLmDVLL534",
        "outputId": "3414b55f-0af4-418a-c915-1515a686fe20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'seqeval-macro_f1': 1.0,\n",
              " 'seqeval-macro_precision': 1.0,\n",
              " 'seqeval-macro_recall': 1.0,\n",
              " 'seqeval-micro_f1': 1.0,\n",
              " 'seqeval-micro_precision': 1.0,\n",
              " 'seqeval-micro_recall': 1.0,\n",
              " 'seqeval-other_f1': 1.0,\n",
              " 'seqeval-other_precision': 1.0,\n",
              " 'seqeval-other_recall': 1.0,\n",
              " 'seqeval-SG_f1': 1.0,\n",
              " 'seqeval-SG_precision': 1.0,\n",
              " 'seqeval-SG_recall': 1.0,\n",
              " 'softseqeval-macro_f1': 1.0,\n",
              " 'softseqeval-macro_precision': 1.0,\n",
              " 'softseqeval-macro_recall': 1.0,\n",
              " 'softseqeval-micro_f1': 1.0,\n",
              " 'softseqeval-micro_precision': 1.0,\n",
              " 'softseqeval-micro_recall': 1.0,\n",
              " 'softseqeval-other_f1': 1.0,\n",
              " 'softseqeval-other_precision': 1.0,\n",
              " 'softseqeval-other_recall': 1.0,\n",
              " 'softseqeval-SG_f1': 1.0,\n",
              " 'softseqeval-SG_precision': 1.0,\n",
              " 'softseqeval-SG_recall': 1.0,\n",
              " 'doclevel-micro_precision': 1.0,\n",
              " 'doclevel-micro_recall': 1.0,\n",
              " 'doclevel-micro_f1': 1.0,\n",
              " 'doclevel-other_precision': 1.0,\n",
              " 'doclevel-other_recall': 1.0,\n",
              " 'doclevel-other_f1': 1.0,\n",
              " 'doclevel-SG_precision': 1.0,\n",
              " 'doclevel-SG_recall': 1.0,\n",
              " 'doclevel-SG_f1': 1.0,\n",
              " 'wordlevel-accuracy': 1.0,\n",
              " 'wordlevel-macro_f1': 1.0,\n",
              " 'wordlevel-macro_precision': 1.0,\n",
              " 'wordlevel-macro_recall': 1.0,\n",
              " 'wordlevel-O_f1': 1.0,\n",
              " 'wordlevel-O_precision': 1.0,\n",
              " 'wordlevel-O_recall': 1.0,\n",
              " 'wordlevel-other_f1': 1.0,\n",
              " 'wordlevel-other_precision': 1.0,\n",
              " 'wordlevel-other_recall': 1.0,\n",
              " 'wordlevel-SG_f1': 1.0,\n",
              " 'wordlevel-SG_precision': 1.0,\n",
              " 'wordlevel-SG_recall': 1.0}"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NOTE: we define a custom function for computing the fine-tuned model's performance in its\n",
        "#       prediction output for the dev or test set examples\n",
        "\n",
        "# uncomment for example (with perfect scores)\n",
        "y_true = datasets['test']['labels'][:25]\n",
        "y_pred = datasets['test']['labels'][:25]\n",
        "\n",
        "compute_sequence_metrics(y_true, y_pred, id2label, flatten_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "m7eJk00ZL535"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    # convert predictions and labels to list of lists of ints\n",
        "    predictions = predictions.astype(int).tolist()\n",
        "    labels = labels.astype(int).tolist()\n",
        "    return compute_sequence_metrics(y_true=labels, y_pred=predictions, id2label=id2label, flatten_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "pfZorzvTL535"
      },
      "outputs": [],
      "source": [
        "# NOTE: at the beginning of the script, we have defined args.metric as the metric to be used for early stopping\n",
        "#       and model selection among saved checkpoints after stopping\n",
        "#       This metric must be available in the output of our `compute_metrics` function defined above\n",
        "#       So let's check this\n",
        "metric = \"seqeval-macro_f1\"\n",
        "ex = ['O', 'B-social group', 'I-social group', 'O']\n",
        "scores = compute_sequence_metrics([ex], [ex], id2label, flatten_output=True)\n",
        "if metric not in scores.keys():\n",
        "    raise ValueError(f\"Invalid metric: {metric}, valid metrics are: {', '.join(scores.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck_Z8j7DL535"
      },
      "source": [
        "### Define the training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "LiWdodhqL535"
      },
      "outputs": [],
      "source": [
        "model_path = base_path / \"models\" / \"licht_detecting_2025-group_mention_detector\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "8uNihbZSL536"
      },
      "outputs": [],
      "source": [
        "out_dir = model_path\n",
        "checkpoints_dir = out_dir / 'checkpoints'\n",
        "logs_dir = out_dir / 'logs'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "    # hyperparameters\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=4e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.3,\n",
        "    optim='adamw_torch',\n",
        "\n",
        "    # when to evaluate\n",
        "    eval_strategy='epoch',\n",
        "    # how to select \"best\" model\n",
        "    do_eval=bool('dev' in datasets),\n",
        "    metric_for_best_model=\"seqeval-macro_f1\",\n",
        "    load_best_model_at_end=True,\n",
        "    # when to save\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=2 if 'dev' in datasets else None, # don't save all model checkpoints\n",
        "    # where to store results\n",
        "    output_dir=checkpoints_dir,\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    # logging\n",
        "    logging_dir=logs_dir,\n",
        "    logging_strategy='epoch',\n",
        "    report_to='none', # Disable Weights & Biases logging\n",
        "\n",
        "    # reproducibility\n",
        "    seed=SEED,\n",
        "    data_seed=SEED,\n",
        "    full_determinism=True\n",
        ")\n",
        "\n",
        "\n",
        "# build callbacks\n",
        "callbacks = []\n",
        "if 'dev' in datasets:\n",
        "    callbacks.append(EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.03))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_IZI9euL54A"
      },
      "source": [
        "### Create the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL3V-jzbL54B",
        "outputId": "e7d6d669-04aa-4925-9370-b1a5fe4b1b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=datasets['train'],\n",
        "    eval_dataset=datasets['dev'] if 'dev' in datasets else None,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNfqg2EkL54B"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "B9ruuEZUL54B",
        "outputId": "d0dd0a21-f61d-413a-f5e8-ba27f6a69d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='808' max='2020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 808/2020 09:10 < 13:47, 1.46 it/s, Epoch 4/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Seqeval-macro F1</th>\n",
              "      <th>Seqeval-macro Precision</th>\n",
              "      <th>Seqeval-macro Recall</th>\n",
              "      <th>Seqeval-micro F1</th>\n",
              "      <th>Seqeval-micro Precision</th>\n",
              "      <th>Seqeval-micro Recall</th>\n",
              "      <th>Seqeval-other F1</th>\n",
              "      <th>Seqeval-other Precision</th>\n",
              "      <th>Seqeval-other Recall</th>\n",
              "      <th>Seqeval-sg F1</th>\n",
              "      <th>Seqeval-sg Precision</th>\n",
              "      <th>Seqeval-sg Recall</th>\n",
              "      <th>Softseqeval-macro F1</th>\n",
              "      <th>Softseqeval-macro Precision</th>\n",
              "      <th>Softseqeval-macro Recall</th>\n",
              "      <th>Softseqeval-micro F1</th>\n",
              "      <th>Softseqeval-micro Precision</th>\n",
              "      <th>Softseqeval-micro Recall</th>\n",
              "      <th>Softseqeval-other F1</th>\n",
              "      <th>Softseqeval-other Precision</th>\n",
              "      <th>Softseqeval-other Recall</th>\n",
              "      <th>Softseqeval-sg F1</th>\n",
              "      <th>Softseqeval-sg Precision</th>\n",
              "      <th>Softseqeval-sg Recall</th>\n",
              "      <th>Doclevel-micro Precision</th>\n",
              "      <th>Doclevel-micro Recall</th>\n",
              "      <th>Doclevel-micro F1</th>\n",
              "      <th>Doclevel-other Precision</th>\n",
              "      <th>Doclevel-other Recall</th>\n",
              "      <th>Doclevel-other F1</th>\n",
              "      <th>Doclevel-sg Precision</th>\n",
              "      <th>Doclevel-sg Recall</th>\n",
              "      <th>Doclevel-sg F1</th>\n",
              "      <th>Wordlevel-accuracy</th>\n",
              "      <th>Wordlevel-macro F1</th>\n",
              "      <th>Wordlevel-macro Precision</th>\n",
              "      <th>Wordlevel-macro Recall</th>\n",
              "      <th>Wordlevel-o F1</th>\n",
              "      <th>Wordlevel-o Precision</th>\n",
              "      <th>Wordlevel-o Recall</th>\n",
              "      <th>Wordlevel-other F1</th>\n",
              "      <th>Wordlevel-other Precision</th>\n",
              "      <th>Wordlevel-other Recall</th>\n",
              "      <th>Wordlevel-sg F1</th>\n",
              "      <th>Wordlevel-sg Precision</th>\n",
              "      <th>Wordlevel-sg Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.229400</td>\n",
              "      <td>0.150491</td>\n",
              "      <td>0.672303</td>\n",
              "      <td>0.664315</td>\n",
              "      <td>0.680782</td>\n",
              "      <td>0.676415</td>\n",
              "      <td>0.667598</td>\n",
              "      <td>0.685468</td>\n",
              "      <td>0.707173</td>\n",
              "      <td>0.689145</td>\n",
              "      <td>0.726170</td>\n",
              "      <td>0.637433</td>\n",
              "      <td>0.639485</td>\n",
              "      <td>0.635394</td>\n",
              "      <td>0.745851</td>\n",
              "      <td>0.774832</td>\n",
              "      <td>0.740149</td>\n",
              "      <td>0.787308</td>\n",
              "      <td>0.818905</td>\n",
              "      <td>0.780748</td>\n",
              "      <td>0.761916</td>\n",
              "      <td>0.794404</td>\n",
              "      <td>0.751871</td>\n",
              "      <td>0.729785</td>\n",
              "      <td>0.755260</td>\n",
              "      <td>0.728427</td>\n",
              "      <td>0.922987</td>\n",
              "      <td>0.922987</td>\n",
              "      <td>0.922987</td>\n",
              "      <td>0.933489</td>\n",
              "      <td>0.933489</td>\n",
              "      <td>0.933489</td>\n",
              "      <td>0.918320</td>\n",
              "      <td>0.918320</td>\n",
              "      <td>0.918320</td>\n",
              "      <td>0.956713</td>\n",
              "      <td>0.877655</td>\n",
              "      <td>0.911985</td>\n",
              "      <td>0.849046</td>\n",
              "      <td>0.976583</td>\n",
              "      <td>0.967354</td>\n",
              "      <td>0.985989</td>\n",
              "      <td>0.831360</td>\n",
              "      <td>0.854744</td>\n",
              "      <td>0.809221</td>\n",
              "      <td>0.825021</td>\n",
              "      <td>0.913858</td>\n",
              "      <td>0.751926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.104000</td>\n",
              "      <td>0.143183</td>\n",
              "      <td>0.697367</td>\n",
              "      <td>0.686802</td>\n",
              "      <td>0.708301</td>\n",
              "      <td>0.699623</td>\n",
              "      <td>0.689239</td>\n",
              "      <td>0.710325</td>\n",
              "      <td>0.720412</td>\n",
              "      <td>0.713073</td>\n",
              "      <td>0.727903</td>\n",
              "      <td>0.674322</td>\n",
              "      <td>0.660532</td>\n",
              "      <td>0.688699</td>\n",
              "      <td>0.769879</td>\n",
              "      <td>0.794397</td>\n",
              "      <td>0.768031</td>\n",
              "      <td>0.810213</td>\n",
              "      <td>0.837276</td>\n",
              "      <td>0.808536</td>\n",
              "      <td>0.770181</td>\n",
              "      <td>0.797684</td>\n",
              "      <td>0.762714</td>\n",
              "      <td>0.769576</td>\n",
              "      <td>0.791110</td>\n",
              "      <td>0.773349</td>\n",
              "      <td>0.941657</td>\n",
              "      <td>0.941657</td>\n",
              "      <td>0.941657</td>\n",
              "      <td>0.934656</td>\n",
              "      <td>0.934656</td>\n",
              "      <td>0.934656</td>\n",
              "      <td>0.928821</td>\n",
              "      <td>0.928821</td>\n",
              "      <td>0.928821</td>\n",
              "      <td>0.959495</td>\n",
              "      <td>0.887148</td>\n",
              "      <td>0.906779</td>\n",
              "      <td>0.869087</td>\n",
              "      <td>0.978205</td>\n",
              "      <td>0.972498</td>\n",
              "      <td>0.983979</td>\n",
              "      <td>0.832920</td>\n",
              "      <td>0.868103</td>\n",
              "      <td>0.800477</td>\n",
              "      <td>0.850318</td>\n",
              "      <td>0.879736</td>\n",
              "      <td>0.822804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.053200</td>\n",
              "      <td>0.166651</td>\n",
              "      <td>0.705948</td>\n",
              "      <td>0.667228</td>\n",
              "      <td>0.750358</td>\n",
              "      <td>0.708259</td>\n",
              "      <td>0.667513</td>\n",
              "      <td>0.754302</td>\n",
              "      <td>0.723946</td>\n",
              "      <td>0.669118</td>\n",
              "      <td>0.788562</td>\n",
              "      <td>0.687951</td>\n",
              "      <td>0.665339</td>\n",
              "      <td>0.712154</td>\n",
              "      <td>0.767507</td>\n",
              "      <td>0.780901</td>\n",
              "      <td>0.773300</td>\n",
              "      <td>0.815002</td>\n",
              "      <td>0.831688</td>\n",
              "      <td>0.820507</td>\n",
              "      <td>0.769439</td>\n",
              "      <td>0.784166</td>\n",
              "      <td>0.768748</td>\n",
              "      <td>0.765574</td>\n",
              "      <td>0.777636</td>\n",
              "      <td>0.777853</td>\n",
              "      <td>0.935823</td>\n",
              "      <td>0.935823</td>\n",
              "      <td>0.935823</td>\n",
              "      <td>0.928821</td>\n",
              "      <td>0.928821</td>\n",
              "      <td>0.928821</td>\n",
              "      <td>0.917153</td>\n",
              "      <td>0.917153</td>\n",
              "      <td>0.917153</td>\n",
              "      <td>0.957325</td>\n",
              "      <td>0.883751</td>\n",
              "      <td>0.883664</td>\n",
              "      <td>0.885490</td>\n",
              "      <td>0.977321</td>\n",
              "      <td>0.977733</td>\n",
              "      <td>0.976909</td>\n",
              "      <td>0.841663</td>\n",
              "      <td>0.809244</td>\n",
              "      <td>0.876789</td>\n",
              "      <td>0.832268</td>\n",
              "      <td>0.864013</td>\n",
              "      <td>0.802773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.021900</td>\n",
              "      <td>0.207925</td>\n",
              "      <td>0.692110</td>\n",
              "      <td>0.675180</td>\n",
              "      <td>0.710707</td>\n",
              "      <td>0.696420</td>\n",
              "      <td>0.677828</td>\n",
              "      <td>0.716061</td>\n",
              "      <td>0.726073</td>\n",
              "      <td>0.692913</td>\n",
              "      <td>0.762565</td>\n",
              "      <td>0.658147</td>\n",
              "      <td>0.657447</td>\n",
              "      <td>0.658849</td>\n",
              "      <td>0.747403</td>\n",
              "      <td>0.767097</td>\n",
              "      <td>0.748277</td>\n",
              "      <td>0.796665</td>\n",
              "      <td>0.820658</td>\n",
              "      <td>0.797257</td>\n",
              "      <td>0.761345</td>\n",
              "      <td>0.785703</td>\n",
              "      <td>0.756223</td>\n",
              "      <td>0.733460</td>\n",
              "      <td>0.748491</td>\n",
              "      <td>0.740332</td>\n",
              "      <td>0.941657</td>\n",
              "      <td>0.941657</td>\n",
              "      <td>0.941657</td>\n",
              "      <td>0.935823</td>\n",
              "      <td>0.935823</td>\n",
              "      <td>0.935823</td>\n",
              "      <td>0.912485</td>\n",
              "      <td>0.912485</td>\n",
              "      <td>0.912485</td>\n",
              "      <td>0.956824</td>\n",
              "      <td>0.880424</td>\n",
              "      <td>0.892081</td>\n",
              "      <td>0.870046</td>\n",
              "      <td>0.976989</td>\n",
              "      <td>0.973591</td>\n",
              "      <td>0.980411</td>\n",
              "      <td>0.831354</td>\n",
              "      <td>0.828076</td>\n",
              "      <td>0.834658</td>\n",
              "      <td>0.832930</td>\n",
              "      <td>0.874576</td>\n",
              "      <td>0.795069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print('Training ...')\n",
        "train_hist = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNuAYQiZL54B"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZalCNRqTL54C"
      },
      "outputs": [],
      "source": [
        "# apply the best model loaded after finishing training to the test set\n",
        "print('Evaluating ...')\n",
        "test_res = trainer.evaluate(datasets['test'], metric_key_prefix='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFhB4MLWL54C"
      },
      "outputs": [],
      "source": [
        "# create a more nice-to-look-at output\n",
        "out = pd.DataFrame(test_res, index=['value']).T\n",
        "out = out.reset_index().rename(columns={'index': 'cat'})\n",
        "out[['set', 'scheme', 'metric', 'misc']] = out.cat.str.split('_', expand=True)\n",
        "out = out[out.misc.isnull()]\n",
        "out = out[out.metric.notnull()]\n",
        "out[['scheme', 'type']] = out.scheme.str.split('-', expand=True)\n",
        "out = out.drop(columns=['set', 'cat', 'misc'])\n",
        "out = out[['scheme', 'type', 'metric', 'value']]\n",
        "out = out.pivot(index=['type', 'scheme', ], columns='metric', values='value')\n",
        "keys = [\n",
        "    (typ, scheme)\n",
        "    for typ in types\n",
        "    for scheme in ['seqeval', 'softseqeval', 'wordlevel', 'doclevel']\n",
        "]\n",
        "out.loc[keys, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsNNw17yL54C"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8HChbGbL54C"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZipQOi_L54D"
      },
      "source": [
        "We'll use the transformer's `pipeline` for inference (i.e., predicting spans in unlabeled data).\n",
        "\n",
        "Specifically, we use the **NER** (named entity recognition) task and pass the fine-tuned model from the trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5ir8_TpL54D"
      },
      "outputs": [],
      "source": [
        "extractor = pipeline(task='ner', model=trainer.model, tokenizer=tokenizer, batch_size=32, aggregation_strategy='simple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35k99VdGL54D"
      },
      "outputs": [],
      "source": [
        "fields = ['id', 'text']\n",
        "df = pd.DataFrame([{f: doc[f] for f in fields} for doc in data_splits['test']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60PuoCAnL54D"
      },
      "outputs": [],
      "source": [
        "# apply the extractor to the dataset\n",
        "pred_ents = [p for p in extractor(df['text'].tolist())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_IU8uBnL54E"
      },
      "outputs": [],
      "source": [
        "pred_ents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4aUM5TlL54E"
      },
      "source": [
        "For each text in the list of texts taken from `docs`, we get a list of dictionaries, here called `pred_ents`.\n",
        "\n",
        "Each item in `pred_ents` is a dictionary with the following fields:\n",
        "\n",
        "- start: character start index of the entity in the text\n",
        "- end: character end index of the entity in the text\n",
        "- score: confidence score of the prediction\n",
        "- word: the text of the entity\n",
        "- entity_group: the entity type (e.g., 'social group')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPKyQXwoL54E"
      },
      "source": [
        "Let's use convert these annotations into one `Entities` instance and create a new `LabeledSequence` instance from this information for each text:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmMjVxCNL54F"
      },
      "outputs": [],
      "source": [
        "from soft_seqeval.classes import Entity, Entities\n",
        "from soft_seqeval.classes import LabeledSequence\n",
        "from copy import deepcopy\n",
        "\n",
        "def pipeline_output_to_entities(pred) -> Entities:\n",
        "    \"\"\"Take output from the NER pipeline and convert to Entities instance\"\"\"\n",
        "    ents = []\n",
        "    for ent in pred:\n",
        "        ent = deepcopy(ent)\n",
        "        if ent['word'][0] == ' ':\n",
        "            ent['start'] += 1\n",
        "        if ent['word'][-1] == ' ':\n",
        "            ent['end'] -= 1\n",
        "        ents.append(Entity(ent['start'], ent['end'], ent['entity_group']))\n",
        "    return Entities(ents)\n",
        "\n",
        "# iterate over the documents and predicted annotations to create a list of LabeledSequence instances\n",
        "preds = [\n",
        "    LabeledSequence(text=doc['text'], entities=pipeline_output_to_entities(pred), id=doc['id'], lang='english')\n",
        "    for (_, doc), pred in zip(df.iterrows(), pred_ents)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtm-FGjlL54F"
      },
      "outputs": [],
      "source": [
        "# look at first 10 examples\n",
        "preds[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntUYwYxTL54F"
      },
      "source": [
        "## Finally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7b-l-oiL54G"
      },
      "source": [
        "#### Delete intermediate checkpoints and log files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye_YMXNAL54G"
      },
      "outputs": [],
      "source": [
        "# finally: clean up\n",
        "if checkpoints_dir.exists():\n",
        "    shutil.rmtree(checkpoints_dir)\n",
        "if logs_dir.exists():\n",
        "    shutil.rmtree(logs_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fkhrn7OL54G"
      },
      "source": [
        "#### Save the best model (if desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gpMyhzKL54G"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(out_dir)\n",
        "tokenizer.save_pretrained(out_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bRw1RW9L54H"
      },
      "source": [
        "### Free the GPU and remove large objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-blpXKxL54H"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "trainer = trainer.model.to('cpu')\n",
        "del trainer, tokenizer, data_splits, datasets\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "advanced_text_analysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}